{"method": "ahgc", "num_chunks": 29, "avg_chunk_len": 1275.3103448275863, "std_chunk_len": 154.44519247678588, "max_chunk_len": 1616, "min_chunk_len": 970, "total_chars": 36984, "compression_ratio": 0.8630218472853126, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025 Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training Xiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng Liu4 and Junsong Wang4 1 College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China 2 College of Applied Technology, Shenzhen University, Shenzhen, China 3 Laboratory for Artificial Intelligence in Design, Hong Kong 4 College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China 5 College of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China xiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com, jinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.edu.cn, wangjunsong@sztu.edu.cn Abstract Multimodal protein features play a crucial role in protein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to protein attributes and interaction networks, making it challenging to decipher their complex interconnections. In this work, we propose a multimodal protein function prediction method (DSRPGO) by utilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information with low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM) to facilitate interactive learning among multimodal features. Additionally, to address the difficulty", "to mine more fine-grained information with low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM) to facilitate interactive learning among multimodal features. Additionally, to address the difficulty of hierarchical multi-label classification in this task, a Dynamic Selection Module (DSM) is designed to select the feature representation that is most conducive to current protein function prediction. Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets, thereby outperforming other benchmark models. 1 Introduction Protein function prediction has become a key challenge in biology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into three categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et al., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and Hoehndorf, 2020]. Many studies [Fan et al., 2020] have Co-corresponding authors: Xiaopeng Jin, Jie Wen. shown that using protein sequence information significantly improves the accuracy of MFO. Still, many proteins share functional similarities but have dissimilar sequences [Lin et al.,", "authors: Xiaopeng Jin, Jie Wen. shown that using protein sequence information significantly improves the accuracy of MFO. Still, many proteins share functional similarities but have dissimilar sequences [Lin et al., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the high complexity of protein structures and data acquisition costs limit their application [Paysan-Lafosse et al., 2023]. Furthermore, the noise introduced during the generation of protein-protein interaction (PPI) networks [Wang et al., 2022] through high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024]. Therefore, integrating these different types of protein data and taking advantage of their complementary advantages in functional prediction is an important way [Zhao et al., 2024] to improve the performance of protein function prediction. These methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023]. Graph2GO [Fan et al., 2020] integrates sequence similarity and PPI networks using GNNs, treating protein sequences and structures as node features. However, those using GNNs [Zhou et al., 2019] may amplify noise and face issues with over-smoothing.", "similarity and PPI networks using GNNs, treating protein sequences and structures as node features. However, those using GNNs [Zhou et al., 2019] may amplify noise and face issues with over-smoothing. To address these limitations, CFAGO [Wu et al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration. However, current multimodal approaches mainly fuse information without exploring the potential complementarity between different modalities. To address this issue, we propose a multimodal method for protein function prediction that efficiently mines the complex internal relationships among spatial structure features, such as PPI networks, subcellular locations, and protein domains, as well as sequence features, specifically the amino acid sequence. Furthermore, due to the complexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI local network topology, connection strength, amino acid frequency distribution, and key sequence fragments. We add a reconstruction pre-training step to obtain more low-semantic and fine-grained features from protein information of multi- ple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks. In addition, large language models play an important role in improving protein function prediction. Inspired by large language models, the protein sequence information", "a richer representational basis for downstream tasks. In addition, large language models play an important role in improving protein function prediction. Inspired by large language models, the protein sequence information in our method is extracted using the pre-trained ProtT5 [Elnaggar et al., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an interactive learning branch. In the shared learning branch, we concatenate features from different modalities and perform joint analysis in a unified representation space. Moreover, we introduce the Bidirectional Interaction Module (BInM), where each modality both influences and receives information from others, enhancing overall understanding. Besides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a challenging issue. Protein function prediction is essentially a complex hierarchical multi-label classification problem. In this situation, we propose the Dynamic Selection Module (DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and supplementary materials have been open-sourced1 . Our main contributions can be summarized as follows: We propose a multimodal feature-based approach for protein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics. A reconstructive pre-training phase", "be summarized as follows: We propose a multimodal feature-based approach for protein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics. A reconstructive pre-training phase is designed to make the model capable of learning more low-semantic finegrained features to assist the model in understanding protein function. Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features, enhancing the model s ability to capture strong protein information between different modes. We construct the DSM that enables the model to adaptively select channel features most relevant to specific functional labels, resulting in enhanced performance. 2 Methodology Our proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder model to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein domains, a Protein Spatial Structured Information (PSSI) encoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including protein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer blocks for pre-training. Then, during our DSRPGO model training phase, we integrate and learn", "including protein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer blocks for pre-training. Then, during our DSRPGO model training phase, we integrate and learn features from multimodal information. The proposed model is primarily divided 1 https://github.com/kioedru/DSRPGO into two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal interactive learning branch (MIL-Branch). Protein data are processed through these branches to generate several sets of features, which serve as inputs for DSM. Finally, the model dynamic selects the optimal features for the current protein, to enhance performance in protein function prediction. An illustration of our proposed method can be seen in Figure 1. 2.1 Reconstructive Pre-training In the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from multi-modal protein information, we utilize the PSSI and PSeI encoder-decoder model for feature reconstruction. PSSI Encoder-Decoder Learning The PPI network gets an N N adjacency matrix by matrix conversion as input to the encoder. Moreover, another input to the encoder is obtained by concatenating the bag-of-words encodings of subcellular location and Protein Domain. Mamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space", "input to the encoder is obtained by concatenating the bag-of-words encodings of subcellular location and Protein Domain. Mamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu et al., 2023] by enabling the transformation of a continuous 1D input xt R to yt R via a learnable hidden state ht RN with discrete parameters A RN N , B R1 N , and C R1 N as follows: ht = A ht 1 + B xt , yt = Cht + Dht , (1) A = e A , B = ( A) 1 (e A I) B, C = C. A and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process discrete-time sequences sampled at intervals of , SSMs can be calculated using the recurrence formula. C represents the projection parameters. In addition, the models compute output through a global convolution as follows: K = (C B , C A B , . . . , C A N 1 B ), y = x K , (2) where N is the length of x, and K is a convolutional kernel. BiMamba Block. Inspired", ". , C A N 1 B ), y = x K , (2) where N is the length of x, and K is a convolutional kernel. BiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block introduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and end of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba blocks, interleaved with linear layers and residual operations. As shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via positional transformations and reconstructions. Transformed tokens are scanned using Equation 1 to produce new features, with BiMamba s output x sp expressed as: x sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ Fσ + Fσ )), F = F SSCan(BSSCan(SSM (Conv1d α (BSSCan(F SSCan(xsp )))))), Fβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))), Fσ = SiLU (F SSCan(xsp )), Figure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the", ")))), Fσ = SiLU (F SSCan(xsp )), Figure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein Spatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge . The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional Interaction Module (BInM), and the Dynamic Selection Module (DSM). where the operation denotes the Hadamard product. PSSI Encoder. In this section, we propose a PSSI encoder architecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba block, Linear and Norm layers, which work in concert to extract features from the input data and generate a compact latent representation. Assume that the input feature k h(k) xi RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th input source. This feature is reconstructed using MLP to outd(k) put a low-dimensional representation xi RD , where D denotes the size of the MLP hidden layer. PSSI Decoder. The architecture of the PSSI decoder is", "using MLP to outd(k) put a low-dimensional representation xi RD , where D denotes the size of the MLP hidden layer. PSSI Decoder. The architecture of the PSSI decoder is a counterpart to that of the encoder. The PSSI decoder rebuilds the given protein spatial structure information based on the hidden representations output by the encoder. This process involves BiMamba computation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k) ing the output xi of the PSSI encoder and passing through the BiMamba block, alternating Linear and Norm layers, we h(k) k obtain the recovered high-dimensional features x i RHi . The overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy loss between the original and reconstructed source features, thereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI encoder-decoder is: k N K Hi 1 XXX h(k) h(k) xij log x ij Lsp = N i=1 k=1 j=1 h(k) h(k) , + 1 xij log 1 x ij (3) where N is the number of total proteins, K is the number h(k) h(k) of input sources, xij and x ij denotes", "+ 1 xij log 1 x ij (3) where N is the number of total proteins, K is the number h(k) h(k) of input sources, xij and x ij denotes the j-th dimension h(k) vector of xi h(k) and x i . PSeI Encoder-Decoder Learning In PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al., 2021] extracts long-distance features from protein sequences. Algorithm 1 Dynamic Selection Moudle Procedure Input: Protein vector Xdsm , Threshold t Output: Fusion feature after DSM 1: Initialize expert weights W 0N . 2: Compute expert confidence coefficients p Softmax(MLP(Xdsm )). 3: Select active experts S {Ei |p i t}. 4: for each experts Ei in S do 5: Normalize p to obtain weights Wi P p i p j . Ej S 6: end for 7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm )) Figure 2: Structure of the BiMamba block. Then, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of ProtT5 and connected it to the PSeI encoder for further pretaining. PSeI Encoder. The PSeI encoder consists of an MLP block and", "To achieve this, we froze the parameters of ProtT5 and connected it to the PSeI encoder for further pretaining. PSeI Encoder. The PSeI encoder consists of an MLP block and 6 self-attention blocks. The self-attention block includes an MSA computation layer, as well as alternating linear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di = M LP (shi ), the output feature is s di RD : s di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4) where shi RHi is the i-th input sequence feature of encoder, and Hi is the dimension of input feature. L(x) denotes the fuction of Linear layer, and N (x) denotes the Norm layer. PSeI Decoder. The PSeI decoder takes the hidden states from the encoder as input, which contains compressed information about the input sequence. To obtain the final protein sequence encoding, we designed the PSeI decoder using a combination of 6 self-attention blocks and one MLP block. Then, the output feature of the PSeI decoder is s hi RHi . Like the PSSI encoder-decoder, the loss function Lse for the", "6 self-attention blocks and one MLP block. Then, the output feature of the PSeI decoder is s hi RHi . Like the PSSI encoder-decoder, the loss function Lse for the PSeI encoder-decoder also adopts the form of cross-entropy: Lse = N1 PN PHi i=1 h h h h , (5) j=1 sij log s ij + 1 sij log 1 s ij where i denotes the sequence input of the i-th protein, j is the j-th dimension vector of the feature map. 2.2 Bidirectional Interaction and Dynamic Selection for Protein Function Prediction In this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein function prediction, BInM and DSM modules are proposed to capture deep interaction information between multimodal features and dynamically screen the features most suitable for the current task. Bidirectional Interaction Module The proposed BInM enhances the model s ability to learn complex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors with key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but focused on cross-branch", "compares query (Q) vectors with key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but focused on cross-branch connections. Therefore, we assume that the features transformed by PPI (1) are represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2) catenated to form xi , while the features extracted through the ProtT foundation model for protein sequences are denoted (3) (1) (2) as xi . Subsequently, xi and xi get features with the same dimension after the MLP reconstruction features, and their concatenated feature map x eB i is used as the input of the first branch of BInM. Similarly, xB i , the input to the second (3) branch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches Fa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb and Fb2 RLa Db Hb , where Hb is the number of multiple heads. As shown in Figure 1, Fb1 and Fb2 are converted into queries Q1 (Fb1 ) and Q2", "Fb2 RLa Db Hb , where Hb is the number of multiple heads. As shown in Figure 1, Fb1 and Fb2 are converted into queries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of Fb1 , and the key K2 and value V 2 of Fb2 are obtained using three generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as: Fc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6) where the operation T means matrix transpose, the operation represents matrix multiplication, and the goal of sof tmax function is to normalize the Fc1 . Finally, the cross-attention output feature Fd1 RLa Da of the first branch is obtained by feature mapping. Similarly, we can get the cross-attention output Fd2 RLa Da of the second branch. In this way, the model takes into account not only the meaning of each branch itself but also the relationships with other branch features, resulting in a more complete representation of multimodal data. Dynamic Selection Module In the final feature selection stage, we introduce DSM to enhance key features and mitigate the impact of conflicting ones. As", "more complete representation of multimodal data. Dynamic Selection Module In the final feature selection stage, we introduce DSM to enhance key features and mitigate the impact of conflicting ones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour, 2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep- resent PPI, sequence, and subcellular localization combined with domain features, respectively. All six-channel feature maps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm ) for the DSM. The function of DSM is: p i DSM(Xdsm ) = Concat( P Ei (Xdsm )), (7) Ej S p j where Ej is the experts belonging to the selected expert group S, p i denotes the confidence coefficient of expert Ei . Loss Functions In this work, protein function prediction is modeled as the multi-label classification task. The predictor, constructed from fully connected layers, takes the output features of the DSM as input and produces an M -dimensional score vector of GO terms: Pi = (p1i , p2i , , pM i )). In the context of protein", "of the DSM as input and produces an M -dimensional score vector of GO terms: Pi = (p1i , p2i , , pM i )). In the context of protein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et al., 2023] as the prediction loss L. L= N X M X 1 y+ y m (1 pm log (pm i ) i ) N M i=1 m=1 i (1 yim ) (pm i ) y log (1 pm i ), (8) where yim represents the ground truth label for the i-th protein, while pm i denotes the predicted score. The symbols {y+} and {y } refer to the positive and negative focusing parameters respectively. 3 Experiments In this section, we present the experimental setup, including the datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin scores to validate the effectiveness of the model. Further experiments on the model components, structures, and parameters can be seen in Appendix Sections 1, 2, and 5. 3.1 Experimental Setup Dataset Settings. We construct", "effectiveness of the model. Further experiments on the model components, structures, and parameters can be seen in Appendix Sections 1, 2, and 5. 3.1 Experimental Setup Dataset Settings. We construct our dataset based on CFAGO [Wu et al., 2023]. PPI data comes from the STRING [Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from the UniProt [Consortium, 2022] database (v3.5.175). A total of 19,385 proteins are used for pretraining. For fine-tuning, we collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The fine-tuning datasets for each GO branch, split by two-time points, including BPO: 3,197 training, 304 validation, 182 testing proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263 training, 577 validation, 119 testing proteins (35 GO terms). More details about sequence similarity and model performance are in Appendix Sections 3 and 6. Implementation Details. We conduct all experiments on NVIDIA GTX 4090. We set the dropout rate to 0.1 during pre-training, and the model trains for 5000 epochs, with a Figure 3: Davies Bouldin Score comparison of different protein features represents. o PPI, o Attribute, and o Sequence", "0.1 during pre-training, and the model trains for 5000 epochs, with a Figure 3: Davies Bouldin Score comparison of different protein features represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively. MSL embedding, MSI embedding, and DSM embedding represent the embedding from MSL-Branch, MIL-Branch, and DSM, respectively. learning rate of 1e-5 for the first 2500 epochs and 1e-6 for the remaining 2500 epochs. During fine-tuning, we use a dropout rate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for the first 50 epochs and reduced to 1e-4 for the remaining 50 epochs. Compared Methods. We compare DSRPGO with nine methods, which are categorized into two groups based on their data utilization strategies. Unimodal-based methods: Naive [Radivojac et al., 2013], BLAST[Altschul et al., 1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho et al., 2016], and deepNF[Gligorijevic et al., 2018]. Multimodal-based methods: Graph2GO[Fan et al., 2020], NetQuilt[Barot et al., 2021], DeepGraphGO[You et al., 2021], and CFAGO[Wu et al., 2023]. Evaluation Metrics. In this study, we evaluate predictive performance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng", "2021], DeepGraphGO[You et al., 2021], and CFAGO[Wu et al., 2023]. Evaluation Metrics. In this study, we evaluate predictive performance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al., 2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and F-max score (Fmax )[Lin et al., 2024], providing a comprehensive assessment of model accuracy and effectiveness. 3.2 Comparison with Unimodal-based and Multimodal-based Methods Comparision with Unimodal-based Methods. Most of the previous methods are based on unimodal protein features, so to verify the performance of our multimodal-based method, we compare our method with unimodal-based methods. The experimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various metrics, except for M-AUPR in MFO. Compared to unimodal methods, DSRPGO improves Fmax by at least 6.4% in BPO, 7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function prediction. Comparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with other state-of-the-art multimodal-based methods, including CFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out- Method Fmax m-AUPR M-AUPR F1 ACC Na ve BLAST GeneMANIA Mashup deepNF NetQuilt Graph2GO DeepGraphGO CFAGO", "Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out- Method Fmax m-AUPR M-AUPR F1 ACC Na ve BLAST GeneMANIA Mashup deepNF NetQuilt Graph2GO DeepGraphGO CFAGO DSRPGO (Ours) BPO 0.051 0 0.270 0 0.000 0 0.075 0 0.394 0.006 0.164 0.014 0.335 0.010 0.327 0.028 0.439 0.007 0.458 0.006 MFO 0.177 0 0.122 0 0.000 0 0.058 0 0.153 0.004 0.081 0.013 0.196 0.006 0.142 0.035 0.236 0.004 0.254 0.022 CCO 0.121 0 0.196 0 0.031 0 0.000 0 0.297 0.009 0.138 0.013 0.298 0.011 0.209 0.023 0.366 0.018 0.452 0.019 BPO 0.024 0 0.110 0 0.042 0 0.238 0 0.303 0.006 0.077 0.006 0.237 0.014 0.210 0.022 0.328 0.005 0.330 0.006 MFO 0.050 0 0.044 0 0.050 0 0.053 0 0.089 0.001 0.045 0.007 0.103 0.007 0.080 0.021 0.159 0.003 0.166 0.027 CCO 0.047 0 0.084 0 0.103 0 0.179 0 0.178 0.005 0.081 0.003 0.215 0.025 0.133 0.011 0.337 0.005 0.371 0.035 BPO 0.048 0 0.093 0 0.160 0 0.146 0 0.174 0.005 0.081 0.004 0.150 0.006 0.133 0.008 0.188 0.003 0.182 0.003 MFO 0.029 0 0.084 0 0.109 0 0.089 0 0.118 0.004 0.064 0.003 0.111 0.005 0.098 0.007 0.138 0.005 0.114 0.009", "0.004 0.150 0.006 0.133 0.008 0.188 0.003 0.182 0.003 MFO 0.029 0 0.084 0 0.109 0 0.089 0 0.118 0.004 0.064 0.003 0.111 0.005 0.098 0.007 0.138 0.005 0.114 0.009 CCO 0.060 0 0.082 0 0.150 0 0.104 0 0.155 0.009 0.063 0.004 0.159 0.021 0.133 0.006 0.210 0.007 0.239 0.025 BPO 0.035 0 0.159 0 0.054 0 0.248 0 0.228 0.005 0.114 0.017 0.222 0.010 0.238 0.012 0.283 0.006 0.272 0.008 MFO 0.004 0 0.064 0 0.008 0 0.106 0 0.117 0.004 0.070 0.016 0.167 0.009 0.165 0.056 0.234 0.005 0.241 0.019 CCO 0.070 0 0.107 0 0.123 0 0.202 0 0.205 0.009 0.108 0.013 0.261 0.015 0.210 0.016 0.314 0.007 0.357 0.033 BPO 0.000 0 0.071 0 0.000 0 0.044 0 0.158 0.011 0.048 0.007 0.257 0.007 0.153 0.034 0.338 0.013 0.346 0.016 MFO 0.000 0 0.015 0 0.000 0 0.038 0 0.034 0.002 0.017 0.002 0.114 0.015 0.048 0.007 0.100 0.003 0.124 0.037 CCO 0.000 0 0.034 0 0.000 0 0.000 0 0.080 0.012 0.037 0.005 0.180 0.024 0.066 0.011 0.210 0.008 0.262 0.017 Table 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based methods. The best results are", "0.024 0.066 0.011 0.210 0.008 0.262 0.017 Table 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based methods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the experimental results. Figure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO. Fmax Method m-AUPR M-AUPR F1 ACC BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO MSLB MILB MSLB+MILB w/o BInM w/o DSM w/o SP-F w/o SE-F w/o pretrain 0.437 0.310 0.458 0.435 0.397 0.216 0.251 0.297 0.179 0.179 0.254 0.193 0.190 0.173 0.238 0.167 0.371 0.420 0.452 0.333 0.378 0.263 0.363 0.356 0.315 0.180 0.330 0.313 0.275 0.106 0.119 0.196 0.108 0.091 0.166 0.116 0.105 0.059 0.117 0.093 0.304 0.330 0.371 0.266 0.302 0.164 0.219 0.284 0.173 0.138 0.182 0.174 0.163 0.105 0.115 0.129 0.102 0.113 0.114 0.106 0.113 0.039 0.099 0.095 0.197 0.220 0.239 0.186 0.205 0.115 0.181 0.200 0.261 0.236 0.272 0.265 0.265 0.174 0.179 0.205 0.172 0.162 0.241 0.180 0.173 0.004 0.224 0.162 0.311 0.342 0.357 0.305 0.328 0.226 0.322 0.286 0.292 0.216 0.346 0.301 0.315 0.151 0.170 0.200", "0.272 0.265 0.265 0.174 0.179 0.205 0.172 0.162 0.241 0.180 0.173 0.004 0.224 0.162 0.311 0.342 0.357 0.305 0.328 0.226 0.322 0.286 0.292 0.216 0.346 0.301 0.315 0.151 0.170 0.200 0.076 0.090 0.124 0.088 0.092 0.000 0.133 0.085 0.190 0.220 0.262 0.151 0.190 0.145 0.193 0.197 Table 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone components: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall model. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The best results are marked in bold. performs these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in BPO, 1.8% in MFO, and 8.6% in CCO. This indicates that DSRPGO s architecture is more effective in learning deep representations among multimodal features, thereby further enhancing overall performance. At the same time, we observe that DSRPGO does not perform optimally in M-AUPR. This is because M-AUPR evaluates each class equally, including those with fewer samples, which may not reflect the model s overall performance. In contrast, m-AUPR aggregates performance across", "in M-AUPR. This is because M-AUPR evaluates each class equally, including those with fewer samples, which may not reflect the model s overall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive measure of predictive capability. In addition, we discuss the Structure-based and PLM-based comparison methods, as detailed in Appendix Section 4. 3.3 Feature Effectiveness Analysis To further evaluate the distinguishing power of the multimodal features extracted by different components of DSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023] scores. In the calculation of DB scores, GO terms are set as the labels for protein clusters, meaning proteins sharing the same GO term set are grouped into the same cluster. A lower DB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively capture multimodal features. Among them, DSM embedding performs best, indicating that DSM successfully integrates inputs from the MIL and MSL branches. To further analyze the discriminative power of protein features, we visualize them using t-SNE [Chatzimparmpas et al., 2020], as shown in Figure 4. Raw input features (o PPI, o Attribute, o Sequence), which are not pre-trained, show distinct patterns but lack clear clustering boundaries. In", "et al., 2020], as shown in Figure 4. Raw input features (o PPI, o Attribute, o Sequence), which are not pre-trained, show distinct patterns but lack clear clustering boundaries. In contrast, the output of the feature by various modules of DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO (cf embedding), DSRPGO demonstrates significantly superior performance. 4 Ablation Studies In this section, the contributions of each component in DSRPGO are evaluated, as shown in Table 2. Analysis for Backbone Components. According to lines 1,2, and 3 of Table 2, the results of the backbone network only using MSL-Branch or MIL-Branch are not as good as those using combined branches. Effectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM block to facilitate bidirectional multimodal feature interaction before DSM. As shown in rows 3 and 4 of Table 2, we verify the validity of BInM for the overall model by removing it. Effectiveness of DSM. To enable effective feature selection and accurate prediction of protein functions, DSM is used to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference", "feature selection and accurate prediction of protein functions, DSM is used to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown in rows 3 and 5 of Table 2, DSM has a positive impact on protein function prediction. Impact of Sequence and Spatial Structure Features. To verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM module, it is removed as no interaction occurs with a single feature type. Rows 6 and 7 of Table 2 show that removing feature interaction significantly reduces model performance. Impact of Pre-training. To evaluate the contribution of pre-training, we conduct an ablation study by removing it. As shown in the last row of Table 2, the model s performance drops significantly across all metrics without pre-training. 5 Conclusion This paper proposes a dual-branched multimodal method for protein function prediction with reconstructive pre-training. The proposed method enhances the model s ability to integrate multimodal features through two key components: the BInM and the DSM, leading to significant performance gains. Experimental results show that the", "proposed method enhances the model s ability to integrate multimodal features through two key components: the BInM and the DSM, leading to significant performance gains. Experimental results show that the DSRPGO outperforms current state-of-the-art unimodal and multimodal methods across multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and the DSM in multimodal protein data integration. Acknowledgements This work was supported in part by the National Natural Science Foundation of China under Grant No. 62302317, the Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant JCYJ20240813141424032 and JCYJ20240813112420027, and the Foundation for Young innovative talents in ordinary universities of Guangdong under Grant 2024KQNCX042, the Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and 20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079. Contribution Statement Xiaoling Luo and Peng Chen contributed equally to this work."], "num_sections": 1, "num_graph_nodes": 31, "num_graph_edges": 58}
{"method": "fixed", "num_chunks": 40, "avg_chunk_len": 797.6, "std_chunk_len": 12.920526305069775, "max_chunk_len": 800, "min_chunk_len": 717, "total_chars": 31904, "compression_ratio": 1.000438816449348, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025\n\nEnhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.ed", "u.cn, wangjunsong@sztu.edu.cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to addre", "ss the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.\n\n1\n\nIntroduction\n\nProtein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zha", "ng et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have\n\nCo-corresponding authors: Xiaopeng Jin, Jie Wen.\n\nshown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-", "protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with", "over-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, conne", "ction strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-\n\nple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and per", "form\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\np", "rotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.\n\n2\n\nMethodology\n\nOur proposed method efficiently captu", "res multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1\n\nhttps://github.com/kioedru/DSRPGO\n\ninto two major branches: one is the multimodal shar", "ed learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.\n\n2.1\n\nReconstructive Pre-training\n\nIn the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input", "to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, t", "he models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,\n\n(2)\n\nwhere N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidi", "rectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:\n\nx sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ\n\n Fσ + Fσ )),\n\n F = F SSCan(BSSCan(SSM (Conv1d\nα\n\n(BSSCan(F SSCan(xsp )))))),\n\nFβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),\n\nFσ = SiLU (F SSCan(xsp )),\n\nFigure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInter", "action Module (BInM), and the Dynamic Selection Module (DSM).\n\nwhere the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI D", "ecoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we\n\nh(k)\n\nk\n\nobtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in repre", "senting protein data. The loss function of PSSI\nencoder-decoder is:\nk\n\nN K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1\n\nh(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij\n\n(3)\n\nwhere N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)\n\nvector of xi\n\nh(k)\n\nand x i\n\n.\n\nPSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.\n\nAlgorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4:", "for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S\n\n6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.\n\nThen, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N", "(s di + L(M SA(s di ))))), (4)\n\nwhere shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1\n\nPN PHi\ni=1\n\n h\n\nh\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij\n\nwhere i denotes the sequence input of the i-t", "h protein, j is\nthe j-th dimension vector of the feature map.\n\n2.2\n\nBidirectional Interaction and Dynamic\nSelection for Protein Function Prediction\n\nIn this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors\n\nwith key (K) vectors from the opposite branch, enabling bidirecti", "onal interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM,", "the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:\n\nFc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get t", "he cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-\n\nresent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-chan", "nel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the predic", "tion loss L.\nL=\n\nN X\nM\nX\n\n1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )\n\ny \n\nlog (1 pm\ni ),\n\n(8)\n\nwhere yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.\n\n3\n\nExperiments\n\nIn this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.\n\n3.1\n\nExperimental Setup\n\nDataset Settings. We construct", "our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are", "in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a\n\nFigure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.\n\nlearning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for th", "e first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], pro", "viding a comprehensive assessment of model accuracy and effectiveness.\n\n3.2\n\nComparison with Unimodal-based and\nMultimodal-based Methods\n\nComparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also co", "mpare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-\n\nMethod\nFmax\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nNa ve \n\nBLAST GeneMANIA Mashup deepNF \n\nNetQuilt\n\nGraph2GO\n\nDeepGraphGO CFAGO\n\nDSRPGO (Ours)\n\nBPO 0.051 0 0.270 0\n\n0.000 0\n\n0.075 0 0.394 0.006 0.164 0.014 0.335 0.010\n\n0.327 0.028\n\n0.439 0.007\n\n0.458 0.006\n\nMFO 0.177 0 0.122 0\n\n0.000 0\n\n0.058 0 0.153 0.004 0.081 0.013 0.196 0.006\n\n0.142 0.035\n\n0.236 0.004\n\n0.254 0.022\n\nCCO 0.121 0 0.196 0\n\n0.031 0\n\n0.000 0 0.297 0.009 0.138 0.013 0.298 0.011\n\n0.209 0.023\n\n0.366 0.018\n\n0.452 0.019\n\nBPO 0.024 0 0.110 0\n\n0.042 0\n\n0.238 0 0.303 0.006 0.077 0.006 0.237 0.014\n\n0.210 0.022\n\n0.328 0.005\n\n0.330 0.006\n\nMFO 0.050 0 0.044 0\n\n0.050", "0\n\n0.053 0 0.089 0.001 0.045 0.007 0.103 0.007\n\n0.080 0.021\n\n0.159 0.003\n\n0.166 0.027\n\nCCO 0.047 0 0.084 0\n\n0.103 0\n\n0.179 0 0.178 0.005 0.081 0.003 0.215 0.025\n\n0.133 0.011\n\n0.337 0.005\n\n0.371 0.035\n\nBPO 0.048 0 0.093 0\n\n0.160 0\n\n0.146 0 0.174 0.005 0.081 0.004 0.150 0.006\n\n0.133 0.008\n\n0.188 0.003\n\n0.182 0.003\n\nMFO 0.029 0 0.084 0\n\n0.109 0\n\n0.089 0 0.118 0.004 0.064 0.003 0.111 0.005\n\n0.098 0.007\n\n0.138 0.005\n\n0.114 0.009\n\nCCO 0.060 0 0.082 0\n\n0.150 0\n\n0.104 0 0.155 0.009 0.063 0.004 0.159 0.021\n\n0.133 0.006\n\n0.210 0.007\n\n0.239 0.025\n\nBPO 0.035 0 0.159 0\n\n0.054 0\n\n0.248 0 0.228 0.005 0.114 0.017 0.222 0.010\n\n0.238 0.012\n\n0.283 0.006\n\n0.272 0.008\n\nMFO 0.004 0 0.064 0\n\n0.008 0\n\n0.106 0 0.117 0.004 0.070 0.016 0.167 0.009\n\n0.165 0.056\n\n0.234 0.005\n\n0.241 0.019\n\nCCO 0.070 0 0.107 0\n\n0.123 0", "0.202 0 0.205 0.009 0.108 0.013 0.261 0.015\n\n0.210 0.016\n\n0.314 0.007\n\n0.357 0.033\n\nBPO 0.000 0 0.071 0\n\n0.000 0\n\n0.044 0 0.158 0.011 0.048 0.007 0.257 0.007\n\n0.153 0.034\n\n0.338 0.013\n\n0.346 0.016\n\nMFO 0.000 0 0.015 0\n\n0.000 0\n\n0.038 0 0.034 0.002 0.017 0.002 0.114 0.015\n\n0.048 0.007\n\n0.100 0.003\n\n0.124 0.037\n\nCCO 0.000 0 0.034 0\n\n0.000 0\n\n0.000 0 0.080 0.012 0.037 0.005 0.180 0.024\n\n0.066 0.011\n\n0.210 0.008\n\n0.262 0.017\n\nTable 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.\n\nFigure 4: Visualization of different feature representations for DSRPGO, and compari", "son with CFAGO.\n\nFmax\n\nMethod\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nBPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain\n\n0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297\n\n0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167\n\n0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356\n\n0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196\n\n0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093\n\n0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284\n\n0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129\n\n0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095\n\n0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200\n\n0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205\n\n0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162\n\n0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286\n\n0.292\n0.216\n0.346\n0.301\n0.3", "15\n0.151\n0.170\n0.200\n\n0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085\n\n0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197\n\nTable 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.\n\nperforms these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby", "further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.\n\n3.3\n\nFeature Effectiveness Analysis\n\nTo further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, mean", "ing proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature b", "y CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.\n\n4\n\nAblation Studies\n\nIn this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.\n\nAnalysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.\nEffectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functio", "ns, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing", "it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.\n\n5\n\nConclusion\n\nThis paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.\n\nAcknowledgements\nThis", "work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.\n\nContribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "sliding", "num_chunks": 79, "avg_chunk_len": 798.5822784810126, "std_chunk_len": 9.256687756765148, "max_chunk_len": 800, "min_chunk_len": 717, "total_chars": 63088, "compression_ratio": 0.5059282272381436, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025\n\nEnhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.ed", "iversity, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.edu.cn, wangjunsong@sztu.edu.cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein functio", "u.cn, wangjunsong@sztu.edu.cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to addre", "n prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.\n\n1\n\nIntroduction\n\nProtein function prediction", "ss the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.\n\n1\n\nIntroduction\n\nProtein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zha", "has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have\n\nCo-corresponding authors: Xiaopeng Jin, Jie Wen.\n\nshown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but hav", "ng et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have\n\nCo-corresponding authors: Xiaopeng Jin, Jie Wen.\n\nshown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-", "e dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese", "protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with", "methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction tha", "over-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, conne", "t\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-\n\nple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving", "ction strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-\n\nple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and per", "protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is ess", "form\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\np", "entially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to", "rotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.\n\n2\n\nMethodology\n\nOur proposed method efficiently captu", "promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.\n\n2\n\nMethodology\n\nOur proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine seq", "res multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1\n\nhttps://github.com/kioedru/DSRPGO\n\ninto two major branches: one is the multimodal shar", "uence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1\n\nhttps://github.com/kioedru/DSRPGO\n\ninto two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method", "ed learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.\n\n2.1\n\nReconstructive Pre-training\n\nIn the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input", "can be seen in Figure 1.\n\n2.1\n\nReconstructive Pre-training\n\nIn the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete param", "to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, t", "eters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,\n\n(2)\n\nwhere N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the sta", "he models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,\n\n(2)\n\nwhere N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidi", "rt and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:\n\nx sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ\n\n Fσ + Fσ )),\n\n F = F SSCan(BSSCan(SSM (Conv1d\nα\n\n(BSSCan(F SSCan(xsp )))))),\n\nFβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),\n\nFσ = SiLU (F SSCan(xsp )),\n\nFigure 1", "rectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:\n\nx sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ\n\n Fσ + Fσ )),\n\n F = F SSCan(BSSCan(SSM (Conv1d\nα\n\n(BSSCan(F SSCan(xsp )))))),\n\nFβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),\n\nFσ = SiLU (F SSCan(xsp )),\n\nFigure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInter", ": An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInteraction Module (BInM), and the Dynamic Selection Module (DSM).\n\nwhere the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to", "action Module (BInM), and the Dynamic Selection Module (DSM).\n\nwhere the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI D", "extract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI Decoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI", "ecoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we\n\nh(k)\n\nk\n\nobtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in repre", "encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we\n\nh(k)\n\nk\n\nobtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI\nencoder-decoder is:\nk\n\nN K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1\n\nh(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij\n\n(3)\n\nwhere N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)\n\nvector of xi\n\nh(k)\n\nand x i\n\n.\n\nPSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer", "senting protein data. The loss function of PSSI\nencoder-decoder is:\nk\n\nN K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1\n\nh(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij\n\n(3)\n\nwhere N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)\n\nvector of xi\n\nh(k)\n\nand x i\n\n.\n\nPSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.\n\nAlgorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4:", "block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.\n\nAlgorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4: for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S\n\n6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.\n\nThen, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder", "for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S\n\n6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.\n\nThen, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N", "for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)\n\nwhere shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encod", "(s di + L(M SA(s di ))))), (4)\n\nwhere shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1\n\nPN PHi\ni=1\n\n h\n\nh\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij\n\nwhere i denotes the sequence input of the i-t", "ing, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1\n\nPN PHi\ni=1\n\n h\n\nh\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij\n\nwhere i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.\n\n2.2\n\nBidirectional Interaction and Dynamic\nSelection for Protein Function Prediction\n\nIn this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture", "h protein, j is\nthe j-th dimension vector of the feature map.\n\n2.2\n\nBidirectional Interaction and Dynamic\nSelection for Protein Function Prediction\n\nIn this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors\n\nwith key (K) vectors from the opposite branch, enabling bidirecti", "deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors\n\nwith key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe P", "onal interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM,", "rotT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. The", "the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:\n\nFc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get t", "n, Fc1 RLa Db Hb obtained by cross-attention is defined as:\n\nFc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of confli", "he cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-\n\nresent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-chan", "cting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-\n\nresent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predict", "nel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the predic", "or, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the prediction loss L.\nL=\n\nN X\nM\nX\n\n1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )\n\ny \n\nlog (1 pm\ni ),\n\n(8)\n\nwhere yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.\n\n3\n\nExperiments\n\nIn this section, we present the experimental setup, including\nthe", "tion loss L.\nL=\n\nN X\nM\nX\n\n1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )\n\ny \n\nlog (1 pm\ni ),\n\n(8)\n\nwhere yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.\n\n3\n\nExperiments\n\nIn this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.\n\n3.1\n\nExperimental Setup\n\nDataset Settings. We construct", "datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.\n\n3.1\n\nExperimental Setup\n\nDataset Settings. We construct our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al.,", "our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are", "2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a\n\nFigure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with do", "in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a\n\nFigure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.\n\nlearning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for th", "main, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.\n\nlearning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal", "e first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], pro", "-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], providing a comprehensive assessment of model accuracy and effectiveness.\n\n3.2\n\nComparison with Unimodal-based and\nMultimodal-based Methods\n\nComparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Ta", "viding a comprehensive assessment of model accuracy and effectiveness.\n\n3.2\n\nComparison with Unimodal-based and\nMultimodal-based Methods\n\nComparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also co", "ble 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-\n\nMethod\nFmax\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nNa ve \n\nBLAST GeneMANIA Mashup deepNF \n\nNetQuilt\n\nGraph2GO\n\nDeepGraphGO CFAGO\n\nDSRPGO (Ours)\n\nBPO 0.051 0 0.270 0\n\n0.000 0\n\n0.075 0 0.394 0.006 0.164 0.014 0.335 0.010\n\n0.327 0", "mpare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-\n\nMethod\nFmax\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nNa ve \n\nBLAST GeneMANIA Mashup deepNF \n\nNetQuilt\n\nGraph2GO\n\nDeepGraphGO CFAGO\n\nDSRPGO (Ours)\n\nBPO 0.051 0 0.270 0\n\n0.000 0\n\n0.075 0 0.394 0.006 0.164 0.014 0.335 0.010\n\n0.327 0.028\n\n0.439 0.007\n\n0.458 0.006\n\nMFO 0.177 0 0.122 0\n\n0.000 0\n\n0.058 0 0.153 0.004 0.081 0.013 0.196 0.006\n\n0.142 0.035\n\n0.236 0.004\n\n0.254 0.022\n\nCCO 0.121 0 0.196 0\n\n0.031 0\n\n0.000 0 0.297 0.009 0.138 0.013 0.298 0.011\n\n0.209 0.023\n\n0.366 0.018\n\n0.452 0.019\n\nBPO 0.024 0 0.110 0\n\n0.042 0\n\n0.238 0 0.303 0.006 0.077 0.006 0.237 0.014\n\n0.210 0.022\n\n0.328 0.005\n\n0.330 0.006\n\nMFO 0.050 0 0.044 0\n\n0.050", ".028\n\n0.439 0.007\n\n0.458 0.006\n\nMFO 0.177 0 0.122 0\n\n0.000 0\n\n0.058 0 0.153 0.004 0.081 0.013 0.196 0.006\n\n0.142 0.035\n\n0.236 0.004\n\n0.254 0.022\n\nCCO 0.121 0 0.196 0\n\n0.031 0\n\n0.000 0 0.297 0.009 0.138 0.013 0.298 0.011\n\n0.209 0.023\n\n0.366 0.018\n\n0.452 0.019\n\nBPO 0.024 0 0.110 0\n\n0.042 0\n\n0.238 0 0.303 0.006 0.077 0.006 0.237 0.014\n\n0.210 0.022\n\n0.328 0.005\n\n0.330 0.006\n\nMFO 0.050 0 0.044 0\n\n0.050 0\n\n0.053 0 0.089 0.001 0.045 0.007 0.103 0.007\n\n0.080 0.021\n\n0.159 0.003\n\n0.166 0.027\n\nCCO 0.047 0 0.084 0\n\n0.103 0\n\n0.179 0 0.178 0.005 0.081 0.003 0.215 0.025\n\n0.133 0.011\n\n0.337 0.005\n\n0.371 0.035\n\nBPO 0.048 0 0.093 0\n\n0.160 0\n\n0.146 0 0.174 0.005 0.081 0.004 0.150 0.006\n\n0.133 0.008\n\n0.188 0.003\n\n0.182 0.003\n\nMFO 0.029 0 0.084 0\n\n0.109 0\n\n0.089 0 0.118 0.004 0.064 0.003 0.111 0.005\n\n0.098 0.0", "0\n\n0.053 0 0.089 0.001 0.045 0.007 0.103 0.007\n\n0.080 0.021\n\n0.159 0.003\n\n0.166 0.027\n\nCCO 0.047 0 0.084 0\n\n0.103 0\n\n0.179 0 0.178 0.005 0.081 0.003 0.215 0.025\n\n0.133 0.011\n\n0.337 0.005\n\n0.371 0.035\n\nBPO 0.048 0 0.093 0\n\n0.160 0\n\n0.146 0 0.174 0.005 0.081 0.004 0.150 0.006\n\n0.133 0.008\n\n0.188 0.003\n\n0.182 0.003\n\nMFO 0.029 0 0.084 0\n\n0.109 0\n\n0.089 0 0.118 0.004 0.064 0.003 0.111 0.005\n\n0.098 0.007\n\n0.138 0.005\n\n0.114 0.009\n\nCCO 0.060 0 0.082 0\n\n0.150 0\n\n0.104 0 0.155 0.009 0.063 0.004 0.159 0.021\n\n0.133 0.006\n\n0.210 0.007\n\n0.239 0.025\n\nBPO 0.035 0 0.159 0\n\n0.054 0\n\n0.248 0 0.228 0.005 0.114 0.017 0.222 0.010\n\n0.238 0.012\n\n0.283 0.006\n\n0.272 0.008\n\nMFO 0.004 0 0.064 0\n\n0.008 0\n\n0.106 0 0.117 0.004 0.070 0.016 0.167 0.009\n\n0.165 0.056\n\n0.234 0.005\n\n0.241 0.019\n\nCCO 0.070 0 0.107 0\n\n0.123 0", "07\n\n0.138 0.005\n\n0.114 0.009\n\nCCO 0.060 0 0.082 0\n\n0.150 0\n\n0.104 0 0.155 0.009 0.063 0.004 0.159 0.021\n\n0.133 0.006\n\n0.210 0.007\n\n0.239 0.025\n\nBPO 0.035 0 0.159 0\n\n0.054 0\n\n0.248 0 0.228 0.005 0.114 0.017 0.222 0.010\n\n0.238 0.012\n\n0.283 0.006\n\n0.272 0.008\n\nMFO 0.004 0 0.064 0\n\n0.008 0\n\n0.106 0 0.117 0.004 0.070 0.016 0.167 0.009\n\n0.165 0.056\n\n0.234 0.005\n\n0.241 0.019\n\nCCO 0.070 0 0.107 0\n\n0.123 0\n\n0.202 0 0.205 0.009 0.108 0.013 0.261 0.015\n\n0.210 0.016\n\n0.314 0.007\n\n0.357 0.033\n\nBPO 0.000 0 0.071 0\n\n0.000 0\n\n0.044 0 0.158 0.011 0.048 0.007 0.257 0.007\n\n0.153 0.034\n\n0.338 0.013\n\n0.346 0.016\n\nMFO 0.000 0 0.015 0\n\n0.000 0\n\n0.038 0 0.034 0.002 0.017 0.002 0.114 0.015\n\n0.048 0.007\n\n0.100 0.003\n\n0.124 0.037\n\nCCO 0.000 0 0.034 0\n\n0.000 0\n\n0.000 0 0.080 0.012 0.037 0.005 0.180 0.024\n\n0.066 0.011", "0.202 0 0.205 0.009 0.108 0.013 0.261 0.015\n\n0.210 0.016\n\n0.314 0.007\n\n0.357 0.033\n\nBPO 0.000 0 0.071 0\n\n0.000 0\n\n0.044 0 0.158 0.011 0.048 0.007 0.257 0.007\n\n0.153 0.034\n\n0.338 0.013\n\n0.346 0.016\n\nMFO 0.000 0 0.015 0\n\n0.000 0\n\n0.038 0 0.034 0.002 0.017 0.002 0.114 0.015\n\n0.048 0.007\n\n0.100 0.003\n\n0.124 0.037\n\nCCO 0.000 0 0.034 0\n\n0.000 0\n\n0.000 0 0.080 0.012 0.037 0.005 0.180 0.024\n\n0.066 0.011\n\n0.210 0.008\n\n0.262 0.017\n\nTable 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.\n\nFigure 4: Visualization of different feature representations for DSRPGO, and compari", "0.210 0.008\n\n0.262 0.017\n\nTable 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.\n\nFigure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO.\n\nFmax\n\nMethod\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nBPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain\n\n0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297\n\n0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167\n\n0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356\n\n0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196\n\n0.108\n0.091\n0.166\n0", "son with CFAGO.\n\nFmax\n\nMethod\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nBPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain\n\n0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297\n\n0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167\n\n0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356\n\n0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196\n\n0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093\n\n0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284\n\n0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129\n\n0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095\n\n0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200\n\n0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205\n\n0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162\n\n0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286\n\n0.292\n0.216\n0.346\n0.301\n0.3", ".116\n0.105\n0.059\n0.117\n0.093\n\n0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284\n\n0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129\n\n0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095\n\n0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200\n\n0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205\n\n0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162\n\n0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286\n\n0.292\n0.216\n0.346\n0.301\n0.315\n0.151\n0.170\n0.200\n\n0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085\n\n0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197\n\nTable 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spati", "15\n0.151\n0.170\n0.200\n\n0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085\n\n0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197\n\nTable 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.\n\nperforms these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby", "al structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.\n\nperforms these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition", "further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.\n\n3.3\n\nFeature Effectiveness Analysis\n\nTo further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, mean", ", we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.\n\n3.3\n\nFeature Effectiveness Analysis\n\nTo further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of", "ing proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature b", "protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.\n\n4\n\nAblation Studies\n\nIn this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.\n\nAnalysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.", "y CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.\n\n4\n\nAblation Studies\n\nIn this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.\n\nAnalysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.\nEffectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functio", "Effectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structu", "ns, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing", "re features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.\n\n5\n\nConclusion\n\nThis paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leadi", "it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.\n\n5\n\nConclusion\n\nThis paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.\n\nAcknowledgements\nThis", "ng to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.\n\nAcknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdon", "work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.\n\nContribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "sentence", "num_chunks": 213, "avg_chunk_len": 150.7511737089202, "std_chunk_len": 181.12447166440987, "max_chunk_len": 738, "min_chunk_len": 20, "total_chars": 32110, "compression_ratio": 0.994020554344441, "chunks": ["arXiv:2511. 04040v1 [cs. LG] 6 Nov 2025\n\nEnhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.", "com, 2300411008@emal. szu. edu.", "cn, liucl1996@163. com,\njinxiaopengit@gmail. com, wenjie@hit.", "edu. cn, liuyumeng@sztu. edu.", "cn, wangjunsong@sztu. edu. cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction.", "However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels.", "Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction. Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.", "1\n\nIntroduction\n\nProtein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al. , 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal.", ", 2023]. In recent decades, numerous deep learning methods [You et al. , 2021; Zhang et al.", ", 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al.", ", 2020] have\n\nCo-corresponding authors: Xiaopeng Jin, Jie Wen. shown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal.", ", 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al.", ", 2023]. Furthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al. , 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].", "Therefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al. , 2024]\nto improve the performance of protein function prediction. These methods mainly adopt two strategies: graph neural networks (GNNs) [You et al.", ", 2021] and autoencoders [Gligorijevic et al. , 2018; Fan et al. , 2020; Pan et al.", ", 2023]. Graph2GO [Fan et al. , 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features.", "However, those using GNNs\n[Zhou et al. , 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al.", ", 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration. However, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence.", "Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-\n\nple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.", "In addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal. , 2021].", "In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.", "Besides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions.", "The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics. A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.", "Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes. We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance. 2\n\nMethodology\n\nOur proposed method efficiently captures multimodal information about proteins through a strategy for two-step training.", "In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training.", "Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1\n\nhttps://github. com/kioedru/DSRPGO\n\ninto two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch).", "Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.", "2. 1\n\nReconstructive Pre-training\n\nIn the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction. PSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder.", "Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain. Mamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al.", ", 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C. A and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula.", "C represents the\nprojection parameters. In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . .", ". , C A N 1 B ), y = x K ,\n\n(2)\n\nwhere N is the length of x, and K is a convolutional kernel. BiMamba Block.", "Inspired by the selective scan mechanism in Vision Mamba [Zhu et al. , 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors.", "Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations. As shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:\n\nx sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ\n\n Fσ + Fσ )),\n\n F = F SSCan(BSSCan(SSM (Conv1d\nα\n\n(BSSCan(F SSCan(xsp )))))),\n\nFβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),\n\nFσ = SiLU (F SSCan(xsp )),\n\nFigure 1: An illustration of our proposed method.", "This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInteraction Module (BInM), and the Dynamic Selection Module (DSM).", "where the operation denotes the Hadamard product. PSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space.", "The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer.", "PSSI Decoder. The architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder.", "This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we\n\nh(k)\n\nk\n\nobtain the recovered high-dimensional features x i\n RHi . The overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data.", "The loss function of PSSI\nencoder-decoder is:\nk\n\nN K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1\n\nh(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij\n\n(3)\n\nwhere N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)\n\nvector of xi\n\nh(k)\n\nand x i\n\n. PSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al. ,\n2021] extracts long-distance features from protein sequences.", "Algorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N . 2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )). 3: Select active experts S {Ei |p i t}.", "4: for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j . Ej S\n\n6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block. Then, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al.", ", 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining. PSeI Encoder.", "The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)\n\nwhere shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature.", "L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer. PSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence.", "To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block. Then, the output feature of the PSeI decoder is s hi RHi . Like the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1\n\nPN PHi\ni=1\n\n h\n\nh\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij\n\nwhere i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.", "2. 2\n\nBidirectional Interaction and Dynamic\nSelection for Protein Function Prediction\n\nIn this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.", "Bidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors\n\nwith key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.", "Therefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP.", "In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads. As shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V.", "Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:\n\nFc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch.", "In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data. Dynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014].", "The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-\n\nresent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .", "Loss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set.", "Consequently, we employ an asymmetric loss [Wu et\nal. , 2023] as the prediction loss L. L=\n\nN X\nM\nX\n\n1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )\n\ny \n\nlog (1 pm\ni ),\n\n(8)\n\nwhere yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score.", "The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively. 3\n\nExperiments\n\nIn this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.", "Further experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5. 3. 1\n\nExperimental Setup\n\nDataset Settings.", "We construct our dataset based on CFAGO\n[Wu et al. , 2023]. PPI data comes from the STRING\n[Szklarczyk et al.", ", 2023] database (v11. 5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3. 5.", "175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al.", ", 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms). More details about sequence similarity and model performance are in Appendix Sections 3 and 6.", "Implementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.", "1 during\npre-training, and the model trains for 5000 epochs, with a\n\nFigure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively. MSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.", "learning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0. 3 and train for 100 epochs with the AdamW optimizer.", "The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs. Compared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies.", "Unimodal-based methods:\nNaive [Radivojac et al. , 2013], BLAST[Altschul et al. ,\n1990], GeneMANIA[Mostafavi et al.", ", 2008], Mashup[Cho\net al. , 2016], and deepNF[Gligorijevic et al. , 2018].", "Multimodal-based methods: Graph2GO[Fan et al. , 2020],\nNetQuilt[Barot et al. , 2021], DeepGraphGO[You et al.", ",\n2021], and CFAGO[Wu et al. , 2023]. Evaluation Metrics.", "In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al. ,\n2021], F1-score (F1) [Wu et al. , 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al.", ", 2024], providing a comprehensive assessment of model accuracy and effectiveness. 3. 2\n\nComparison with Unimodal-based and\nMultimodal-based Methods\n\nComparision with Unimodal-based Methods.", "Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO.", "Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6. 4% in BPO,\n7. 7% in MFO, and 15.", "5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction. Comparision with Multimodal-based Methods.", "To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-\n\nMethod\nFmax\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nNa ve \n\nBLAST GeneMANIA Mashup deepNF \n\nNetQuilt\n\nGraph2GO\n\nDeepGraphGO CFAGO\n\nDSRPGO (Ours)\n\nBPO 0. 051 0 0.", "270 0\n\n0. 000 0\n\n0. 075 0 0.", "394 0. 006 0. 164 0.", "014 0. 335 0. 010\n\n0.", "327 0. 028\n\n0. 439 0.", "007\n\n0. 458 0. 006\n\nMFO 0.", "177 0 0. 122 0\n\n0. 000 0\n\n0.", "058 0 0. 153 0. 004 0.", "081 0. 013 0. 196 0.", "006\n\n0. 142 0. 035\n\n0.", "236 0. 004\n\n0. 254 0.", "022\n\nCCO 0. 121 0 0. 196 0\n\n0.", "031 0\n\n0. 000 0 0. 297 0.", "009 0. 138 0. 013 0.", "298 0. 011\n\n0. 209 0.", "023\n\n0. 366 0. 018\n\n0.", "452 0. 019\n\nBPO 0. 024 0 0.", "110 0\n\n0. 042 0\n\n0. 238 0 0.", "303 0. 006 0. 077 0.", "006 0. 237 0. 014\n\n0.", "210 0. 022\n\n0. 328 0.", "005\n\n0. 330 0. 006\n\nMFO 0.", "050 0 0. 044 0\n\n0. 050 0\n\n0.", "053 0 0. 089 0. 001 0.", "045 0. 007 0. 103 0.", "007\n\n0. 080 0. 021\n\n0.", "159 0. 003\n\n0. 166 0.", "027\n\nCCO 0. 047 0 0. 084 0\n\n0.", "103 0\n\n0. 179 0 0. 178 0.", "005 0. 081 0. 003 0.", "215 0. 025\n\n0. 133 0.", "011\n\n0. 337 0. 005\n\n0.", "371 0. 035\n\nBPO 0. 048 0 0.", "093 0\n\n0. 160 0\n\n0. 146 0 0.", "174 0. 005 0. 081 0.", "004 0. 150 0. 006\n\n0.", "133 0. 008\n\n0. 188 0.", "003\n\n0. 182 0. 003\n\nMFO 0.", "029 0 0. 084 0\n\n0. 109 0\n\n0.", "089 0 0. 118 0. 004 0.", "064 0. 003 0. 111 0.", "005\n\n0. 098 0. 007\n\n0.", "138 0. 005\n\n0. 114 0.", "009\n\nCCO 0. 060 0 0. 082 0\n\n0.", "150 0\n\n0. 104 0 0. 155 0.", "009 0. 063 0. 004 0.", "159 0. 021\n\n0. 133 0.", "006\n\n0. 210 0. 007\n\n0.", "239 0. 025\n\nBPO 0. 035 0 0.", "159 0\n\n0. 054 0\n\n0. 248 0 0.", "228 0. 005 0. 114 0.", "017 0. 222 0. 010\n\n0.", "238 0. 012\n\n0. 283 0.", "006\n\n0. 272 0. 008\n\nMFO 0.", "004 0 0. 064 0\n\n0. 008 0\n\n0.", "106 0 0. 117 0. 004 0.", "070 0. 016 0. 167 0.", "009\n\n0. 165 0. 056\n\n0.", "234 0. 005\n\n0. 241 0.", "019\n\nCCO 0. 070 0 0. 107 0\n\n0.", "123 0\n\n0. 202 0 0. 205 0.", "009 0. 108 0. 013 0.", "261 0. 015\n\n0. 210 0.", "016\n\n0. 314 0. 007\n\n0.", "357 0. 033\n\nBPO 0. 000 0 0.", "071 0\n\n0. 000 0\n\n0. 044 0 0.", "158 0. 011 0. 048 0.", "007 0. 257 0. 007\n\n0.", "153 0. 034\n\n0. 338 0.", "013\n\n0. 346 0. 016\n\nMFO 0.", "000 0 0. 015 0\n\n0. 000 0\n\n0.", "038 0 0. 034 0. 002 0.", "017 0. 002 0. 114 0.", "015\n\n0. 048 0. 007\n\n0.", "100 0. 003\n\n0. 124 0.", "037\n\nCCO 0. 000 0 0. 034 0\n\n0.", "000 0\n\n0. 000 0 0. 080 0.", "012 0. 037 0. 005 0.", "180 0. 024\n\n0. 066 0.", "011\n\n0. 210 0. 008\n\n0.", "262 0. 017\n\nTable 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods.", "The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results. Figure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO.", "Fmax\n\nMethod\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nBPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain\n\n0. 437\n0. 310\n0.", "458\n0. 435\n0. 397\n0.", "216\n0. 251\n0. 297\n\n0.", "179\n0. 179\n0. 254\n0.", "193\n0. 190\n0. 173\n0.", "238\n0. 167\n\n0. 371\n0.", "420\n0. 452\n0. 333\n0.", "378\n0. 263\n0. 363\n0.", "356\n\n0. 315\n0. 180\n0.", "330\n0. 313\n0. 275\n0.", "106\n0. 119\n0. 196\n\n0.", "108\n0. 091\n0. 166\n0.", "116\n0. 105\n0. 059\n0.", "117\n0. 093\n\n0. 304\n0.", "330\n0. 371\n0. 266\n0.", "302\n0. 164\n0. 219\n0.", "284\n\n0. 173\n0. 138\n0.", "182\n0. 174\n0. 163\n0.", "105\n0. 115\n0. 129\n\n0.", "102\n0. 113\n0. 114\n0.", "106\n0. 113\n0. 039\n0.", "099\n0. 095\n\n0. 197\n0.", "220\n0. 239\n0. 186\n0.", "205\n0. 115\n0. 181\n0.", "200\n\n0. 261\n0. 236\n0.", "272\n0. 265\n0. 265\n0.", "174\n0. 179\n0. 205\n\n0.", "172\n0. 162\n0. 241\n0.", "180\n0. 173\n0. 004\n0.", "224\n0. 162\n\n0. 311\n0.", "342\n0. 357\n0. 305\n0.", "328\n0. 226\n0. 322\n0.", "286\n\n0. 292\n0. 216\n0.", "346\n0. 301\n0. 315\n0.", "151\n0. 170\n0. 200\n\n0.", "076\n0. 090\n0. 124\n0.", "088\n0. 092\n0. 000\n0.", "133\n0. 085\n\n0. 190\n0.", "220\n0. 262\n0. 151\n0.", "190\n0. 145\n0. 193\n0.", "197\n\nTable 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel.", "w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold. performs these methods.", "Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1. 9% in\nBPO, 1. 8% in MFO, and 8.", "6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR.", "This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.", "3. 3\n\nFeature Effectiveness Analysis\n\nTo further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al. , 2023]\nscores.", "In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features.", "Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches. To further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al. , 2020], as shown in Figure 4.", "Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries. In contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.", "4\n\nAblation Studies\n\nIn this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2. Analysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.", "Effectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.", "Effectiveness of DSM. To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features.", "As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction. Impact of Sequence and Spatial Structure Features. To verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features.", "For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance. Impact of Pre-training.", "To evaluate the contribution of\npre-training, we conduct an ablation study by removing it. As shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training. 5\n\nConclusion\n\nThis paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.", "The proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains. Experimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.", "Acknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079. Contribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "paragraph", "num_chunks": 208, "avg_chunk_len": 151.4278846153846, "std_chunk_len": 368.1159341202408, "max_chunk_len": 2357, "min_chunk_len": 1, "total_chars": 31497, "compression_ratio": 1.0133663523510175, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025", "Enhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.edu.cn, wangjunsong@sztu.edu.cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.", "1", "Introduction", "Protein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have", "Co-corresponding authors: Xiaopeng Jin, Jie Wen.", "shown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-", "ple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.", "2", "Methodology", "Our proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1", "https://github.com/kioedru/DSRPGO", "into two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.", "2.1", "Reconstructive Pre-training", "In the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,", "(2)", "where N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:", "x sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ", "Fσ + Fσ )),", "F = F SSCan(BSSCan(SSM (Conv1d\nα", "(BSSCan(F SSCan(xsp )))))),", "Fβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),", "Fσ = SiLU (F SSCan(xsp )),", "Figure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInteraction Module (BInM), and the Dynamic Selection Module (DSM).", "where the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI Decoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we", "h(k)", "k", "obtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI\nencoder-decoder is:\nk", "N K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1", "h(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij", "(3)", "where N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)", "vector of xi", "h(k)", "and x i", ".", "PSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.", "Algorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4: for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S", "6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.", "Then, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)", "where shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1", "PN PHi\ni=1", "h", "h\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij", "where i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.", "2.2", "Bidirectional Interaction and Dynamic\nSelection for Protein Function Prediction", "In this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors", "with key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:", "Fc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-", "resent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the prediction loss L.\nL=", "N X\nM\nX", "1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )", "y", "log (1 pm\ni ),", "(8)", "where yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.", "3", "Experiments", "In this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.", "3.1", "Experimental Setup", "Dataset Settings. We construct our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a", "Figure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.", "learning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], providing a comprehensive assessment of model accuracy and effectiveness.", "3.2", "Comparison with Unimodal-based and\nMultimodal-based Methods", "Comparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-", "Method\nFmax", "m-AUPR", "M-AUPR", "F1", "ACC", "Na ve", "BLAST GeneMANIA Mashup deepNF", "NetQuilt", "Graph2GO", "DeepGraphGO CFAGO", "DSRPGO (Ours)", "BPO 0.051 0 0.270 0", "0.000 0", "0.075 0 0.394 0.006 0.164 0.014 0.335 0.010", "0.327 0.028", "0.439 0.007", "0.458 0.006", "MFO 0.177 0 0.122 0", "0.000 0", "0.058 0 0.153 0.004 0.081 0.013 0.196 0.006", "0.142 0.035", "0.236 0.004", "0.254 0.022", "CCO 0.121 0 0.196 0", "0.031 0", "0.000 0 0.297 0.009 0.138 0.013 0.298 0.011", "0.209 0.023", "0.366 0.018", "0.452 0.019", "BPO 0.024 0 0.110 0", "0.042 0", "0.238 0 0.303 0.006 0.077 0.006 0.237 0.014", "0.210 0.022", "0.328 0.005", "0.330 0.006", "MFO 0.050 0 0.044 0", "0.050 0", "0.053 0 0.089 0.001 0.045 0.007 0.103 0.007", "0.080 0.021", "0.159 0.003", "0.166 0.027", "CCO 0.047 0 0.084 0", "0.103 0", "0.179 0 0.178 0.005 0.081 0.003 0.215 0.025", "0.133 0.011", "0.337 0.005", "0.371 0.035", "BPO 0.048 0 0.093 0", "0.160 0", "0.146 0 0.174 0.005 0.081 0.004 0.150 0.006", "0.133 0.008", "0.188 0.003", "0.182 0.003", "MFO 0.029 0 0.084 0", "0.109 0", "0.089 0 0.118 0.004 0.064 0.003 0.111 0.005", "0.098 0.007", "0.138 0.005", "0.114 0.009", "CCO 0.060 0 0.082 0", "0.150 0", "0.104 0 0.155 0.009 0.063 0.004 0.159 0.021", "0.133 0.006", "0.210 0.007", "0.239 0.025", "BPO 0.035 0 0.159 0", "0.054 0", "0.248 0 0.228 0.005 0.114 0.017 0.222 0.010", "0.238 0.012", "0.283 0.006", "0.272 0.008", "MFO 0.004 0 0.064 0", "0.008 0", "0.106 0 0.117 0.004 0.070 0.016 0.167 0.009", "0.165 0.056", "0.234 0.005", "0.241 0.019", "CCO 0.070 0 0.107 0", "0.123 0", "0.202 0 0.205 0.009 0.108 0.013 0.261 0.015", "0.210 0.016", "0.314 0.007", "0.357 0.033", "BPO 0.000 0 0.071 0", "0.000 0", "0.044 0 0.158 0.011 0.048 0.007 0.257 0.007", "0.153 0.034", "0.338 0.013", "0.346 0.016", "MFO 0.000 0 0.015 0", "0.000 0", "0.038 0 0.034 0.002 0.017 0.002 0.114 0.015", "0.048 0.007", "0.100 0.003", "0.124 0.037", "CCO 0.000 0 0.034 0", "0.000 0", "0.000 0 0.080 0.012 0.037 0.005 0.180 0.024", "0.066 0.011", "0.210 0.008", "0.262 0.017", "Table 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.", "Figure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO.", "Fmax", "Method", "m-AUPR", "M-AUPR", "F1", "ACC", "BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain", "0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297", "0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167", "0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356", "0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196", "0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093", "0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284", "0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129", "0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095", "0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200", "0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205", "0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162", "0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286", "0.292\n0.216\n0.346\n0.301\n0.315\n0.151\n0.170\n0.200", "0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085", "0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197", "Table 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.", "performs these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.", "3.3", "Feature Effectiveness Analysis", "To further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.", "4", "Ablation Studies", "In this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.", "Analysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.\nEffectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.", "5", "Conclusion", "This paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.", "Acknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.", "Contribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "recursive", "num_chunks": 265, "avg_chunk_len": 118.82641509433962, "std_chunk_len": 175.94280819366554, "max_chunk_len": 772, "min_chunk_len": 1, "total_chars": 31489, "compression_ratio": 1.0136238051383022, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025", "Enhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook. com, 2300411008@emal. szu.", "edu. cn, liucl1996@163. com,\njinxiaopengit@gmail.", "com, wenjie@hit. edu. cn, liuyumeng@sztu.", "edu. cn, wangjunsong@sztu. edu.", "cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms.", "To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.", "Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.", "1", "Introduction", "Protein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have", "Co-corresponding authors: Xiaopeng Jin, Jie Wen.", "shown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal. , 2024].", "As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al. , 2023].", "Furthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al. , 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024]. Therefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al.", ", 2024]\nto improve the performance of protein function prediction. These methods mainly adopt two strategies: graph neural networks (GNNs) [You et al. , 2021] and autoencoders [Gligorijevic et al.", ", 2018; Fan et al. , 2020; Pan et al. , 2023].", "Graph2GO [Fan et al. , 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al.", ", 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al. , 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.", "However, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments.", "We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-", "ple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks. In addition, large language models play an important role\nin improving protein function prediction.", "Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal. , 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch.", "In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding. Besides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue.", "Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 .", "Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics. A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function. Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.", "We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.", "2", "Methodology", "Our proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1", "https://github.com/kioedru/DSRPGO", "into two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.", "2.1", "Reconstructive Pre-training", "In the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction. PSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.", "Mamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al. , 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.", "A and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters.", "In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . . .", ", C A N 1 B ), y = x K ,", "(2)", "where N is the length of x, and K is a convolutional kernel. BiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al.", ", 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.", "As shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:", "x sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ", "Fσ + Fσ )),", "F = F SSCan(BSSCan(SSM (Conv1d\nα", "(BSSCan(F SSCan(xsp )))))),", "Fβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),", "Fσ = SiLU (F SSCan(xsp )),", "Figure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInteraction Module (BInM), and the Dynamic Selection Module (DSM).", "where the operation denotes the Hadamard product. PSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space.", "The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer.", "PSSI Decoder. The architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder.", "This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we", "h(k)", "k", "obtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI\nencoder-decoder is:\nk", "N K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1", "h(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij", "(3)", "where N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)", "vector of xi", "h(k)", "and x i", ".", "PSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.", "Algorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4: for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S", "6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.", "Then, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)", "where shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1", "PN PHi\ni=1", "h", "h\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij", "where i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.", "2.2", "Bidirectional Interaction and Dynamic\nSelection for Protein Function Prediction", "In this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors", "with key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections. Therefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi .", "Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.", "As shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:", "Fc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch.", "In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data. Dynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014].", "The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-", "resent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .", "Loss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set.", "Consequently, we employ an asymmetric loss [Wu et\nal. , 2023] as the prediction loss L. L=", "N X\nM\nX", "1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )", "y", "log (1 pm\ni ),", "(8)", "where yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.", "3", "Experiments", "In this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.", "3.1", "Experimental Setup", "Dataset Settings. We construct our dataset based on CFAGO\n[Wu et al. , 2023].", "PPI data comes from the STRING\n[Szklarczyk et al. , 2023] database (v11. 5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.", "5. 175). A total\nof 19,385 proteins are used for pretraining.", "For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al. , 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).", "More details about sequence similarity and model performance are in Appendix Sections 3 and 6. Implementation Details. We conduct all experiments on\nNVIDIA GTX 4090.", "We set the dropout rate to 0. 1 during\npre-training, and the model trains for 5000 epochs, with a", "Figure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.", "learning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0. 3 and train for 100 epochs with the AdamW optimizer.", "The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs. Compared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies.", "Unimodal-based methods:\nNaive [Radivojac et al. , 2013], BLAST[Altschul et al. ,\n1990], GeneMANIA[Mostafavi et al.", ", 2008], Mashup[Cho\net al. , 2016], and deepNF[Gligorijevic et al. , 2018].", "Multimodal-based methods: Graph2GO[Fan et al. , 2020],\nNetQuilt[Barot et al. , 2021], DeepGraphGO[You et al.", ",\n2021], and CFAGO[Wu et al. , 2023]. Evaluation Metrics.", "In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al. ,\n2021], F1-score (F1) [Wu et al. , 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al.", ", 2024], providing a comprehensive assessment of model accuracy and effectiveness.", "3.2", "Comparison with Unimodal-based and\nMultimodal-based Methods", "Comparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1.", "DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6. 4% in BPO,\n7.", "7% in MFO, and 15. 5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.", "Comparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-", "Method\nFmax", "m-AUPR", "M-AUPR", "F1", "ACC", "Na ve", "BLAST GeneMANIA Mashup deepNF", "NetQuilt", "Graph2GO", "DeepGraphGO CFAGO", "DSRPGO (Ours)", "BPO 0.051 0 0.270 0", "0.000 0", "0.075 0 0.394 0.006 0.164 0.014 0.335 0.010", "0.327 0.028", "0.439 0.007", "0.458 0.006", "MFO 0.177 0 0.122 0", "0.000 0", "0.058 0 0.153 0.004 0.081 0.013 0.196 0.006", "0.142 0.035", "0.236 0.004", "0.254 0.022", "CCO 0.121 0 0.196 0", "0.031 0", "0.000 0 0.297 0.009 0.138 0.013 0.298 0.011", "0.209 0.023", "0.366 0.018", "0.452 0.019", "BPO 0.024 0 0.110 0", "0.042 0", "0.238 0 0.303 0.006 0.077 0.006 0.237 0.014", "0.210 0.022", "0.328 0.005", "0.330 0.006", "MFO 0.050 0 0.044 0", "0.050 0", "0.053 0 0.089 0.001 0.045 0.007 0.103 0.007", "0.080 0.021", "0.159 0.003", "0.166 0.027", "CCO 0.047 0 0.084 0", "0.103 0", "0.179 0 0.178 0.005 0.081 0.003 0.215 0.025", "0.133 0.011", "0.337 0.005", "0.371 0.035", "BPO 0.048 0 0.093 0", "0.160 0", "0.146 0 0.174 0.005 0.081 0.004 0.150 0.006", "0.133 0.008", "0.188 0.003", "0.182 0.003", "MFO 0.029 0 0.084 0", "0.109 0", "0.089 0 0.118 0.004 0.064 0.003 0.111 0.005", "0.098 0.007", "0.138 0.005", "0.114 0.009", "CCO 0.060 0 0.082 0", "0.150 0", "0.104 0 0.155 0.009 0.063 0.004 0.159 0.021", "0.133 0.006", "0.210 0.007", "0.239 0.025", "BPO 0.035 0 0.159 0", "0.054 0", "0.248 0 0.228 0.005 0.114 0.017 0.222 0.010", "0.238 0.012", "0.283 0.006", "0.272 0.008", "MFO 0.004 0 0.064 0", "0.008 0", "0.106 0 0.117 0.004 0.070 0.016 0.167 0.009", "0.165 0.056", "0.234 0.005", "0.241 0.019", "CCO 0.070 0 0.107 0", "0.123 0", "0.202 0 0.205 0.009 0.108 0.013 0.261 0.015", "0.210 0.016", "0.314 0.007", "0.357 0.033", "BPO 0.000 0 0.071 0", "0.000 0", "0.044 0 0.158 0.011 0.048 0.007 0.257 0.007", "0.153 0.034", "0.338 0.013", "0.346 0.016", "MFO 0.000 0 0.015 0", "0.000 0", "0.038 0 0.034 0.002 0.017 0.002 0.114 0.015", "0.048 0.007", "0.100 0.003", "0.124 0.037", "CCO 0.000 0 0.034 0", "0.000 0", "0.000 0 0.080 0.012 0.037 0.005 0.180 0.024", "0.066 0.011", "0.210 0.008", "0.262 0.017", "Table 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.", "Figure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO.", "Fmax", "Method", "m-AUPR", "M-AUPR", "F1", "ACC", "BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain", "0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297", "0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167", "0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356", "0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196", "0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093", "0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284", "0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129", "0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095", "0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200", "0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205", "0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162", "0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286", "0.292\n0.216\n0.346\n0.301\n0.315\n0.151\n0.170\n0.200", "0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085", "0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197", "Table 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.", "performs these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.", "3.3", "Feature Effectiveness Analysis", "To further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al. , 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster.", "A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.", "To further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al. , 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.", "In contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.", "4", "Ablation Studies", "In this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.", "Analysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches. Effectiveness of BInM.", "Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it. Effectiveness of DSM.", "To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.", "Impact of Sequence and Spatial Structure Features. To verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type.", "Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance. Impact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing it.", "As shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.", "5", "Conclusion", "This paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.", "Acknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.", "Contribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "semantic", "num_chunks": 9, "avg_chunk_len": 3005.0, "std_chunk_len": 3514.3039140063, "max_chunk_len": 11406, "min_chunk_len": 83, "total_chars": 27045, "compression_ratio": 1.180181179515622, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025\n\nEnhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.edu.cn, wangjunsong@sztu.edu.cn", "Abstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.", "1 Introduction\n\nProtein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have\n\nCo-corresponding authors: Xiaopeng Jin, Jie Wen.\n\nshown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multiple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.\n1\nhttps://github.com/kioedru/DSRPGO", "2 Methodology\n\nOur proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\ninto two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.\n\n2.1 Reconstructive Pre-training\n\nIn the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,\n\n(2)\n\nwhere N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:\n\nx sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ\n\n Fσ + Fσ )),\n\n F = F SSCan(BSSCan(SSM (Conv1d\nα\n\n(BSSCan(F SSCan(xsp )))))),\n\nFβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),\n\nFσ = SiLU (F SSCan(xsp )),\n\nwhere the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI Decoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we\n\nh(k)\n\nk\nobtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI\nencoder-decoder is:\nk\n\nN K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp = N i=1\nk=1 j=1\n\nh(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij\n\n(3)\n\nwhere N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)\n\nvector of xi\n\nh(k)\n\nand x i\n.\n\nPSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.\nThen, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)\n\nwhere shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1\n\nPN PHi\ni=1\n\n h\n\nh\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij\n\nwhere i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.\n\n2.2 Bidirectional Interaction and Dynamic\nSelection for Protein Function Prediction\n\nIn this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors\nwith key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:\n\nFc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels represent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the prediction loss L.\nL=\n\nN X\nM\nX\n\n1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )\n\ny \n\nlog (1 pm\ni ),\n\n(8)\n\nwhere yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.", "3 Experiments\n\nIn this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.\n\n3.1 Experimental Setup\n\nDataset Settings. We construct our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a\nlearning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], providing a comprehensive assessment of model accuracy and effectiveness.\n\n3.2 Comparison with Unimodal-based and\nMultimodal-based Methods\n\nComparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out\nperforms these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.\n\n3.3 Feature Effectiveness Analysis\n\nTo further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.", "4 Ablation Studies\n\nIn this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.\n\nAnalysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.\nEffectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.", "5 Conclusion\n\nThis paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.", "Acknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.", "Contribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "delimiter", "num_chunks": 208, "avg_chunk_len": 151.4278846153846, "std_chunk_len": 368.1159341202408, "max_chunk_len": 2357, "min_chunk_len": 1, "total_chars": 31497, "compression_ratio": 1.0133663523510175, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025", "Enhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.edu.cn, wangjunsong@sztu.edu.cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.", "1", "Introduction", "Protein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have", "Co-corresponding authors: Xiaopeng Jin, Jie Wen.", "shown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-", "ple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.", "2", "Methodology", "Our proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1", "https://github.com/kioedru/DSRPGO", "into two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.", "2.1", "Reconstructive Pre-training", "In the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,", "(2)", "where N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:", "x sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ", "Fσ + Fσ )),", "F = F SSCan(BSSCan(SSM (Conv1d\nα", "(BSSCan(F SSCan(xsp )))))),", "Fβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),", "Fσ = SiLU (F SSCan(xsp )),", "Figure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInteraction Module (BInM), and the Dynamic Selection Module (DSM).", "where the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI Decoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we", "h(k)", "k", "obtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI\nencoder-decoder is:\nk", "N K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1", "h(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij", "(3)", "where N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)", "vector of xi", "h(k)", "and x i", ".", "PSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.", "Algorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4: for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S", "6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.", "Then, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)", "where shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1", "PN PHi\ni=1", "h", "h\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij", "where i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.", "2.2", "Bidirectional Interaction and Dynamic\nSelection for Protein Function Prediction", "In this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors", "with key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:", "Fc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-", "resent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the prediction loss L.\nL=", "N X\nM\nX", "1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )", "y", "log (1 pm\ni ),", "(8)", "where yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.", "3", "Experiments", "In this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.", "3.1", "Experimental Setup", "Dataset Settings. We construct our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a", "Figure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.", "learning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], providing a comprehensive assessment of model accuracy and effectiveness.", "3.2", "Comparison with Unimodal-based and\nMultimodal-based Methods", "Comparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-", "Method\nFmax", "m-AUPR", "M-AUPR", "F1", "ACC", "Na ve", "BLAST GeneMANIA Mashup deepNF", "NetQuilt", "Graph2GO", "DeepGraphGO CFAGO", "DSRPGO (Ours)", "BPO 0.051 0 0.270 0", "0.000 0", "0.075 0 0.394 0.006 0.164 0.014 0.335 0.010", "0.327 0.028", "0.439 0.007", "0.458 0.006", "MFO 0.177 0 0.122 0", "0.000 0", "0.058 0 0.153 0.004 0.081 0.013 0.196 0.006", "0.142 0.035", "0.236 0.004", "0.254 0.022", "CCO 0.121 0 0.196 0", "0.031 0", "0.000 0 0.297 0.009 0.138 0.013 0.298 0.011", "0.209 0.023", "0.366 0.018", "0.452 0.019", "BPO 0.024 0 0.110 0", "0.042 0", "0.238 0 0.303 0.006 0.077 0.006 0.237 0.014", "0.210 0.022", "0.328 0.005", "0.330 0.006", "MFO 0.050 0 0.044 0", "0.050 0", "0.053 0 0.089 0.001 0.045 0.007 0.103 0.007", "0.080 0.021", "0.159 0.003", "0.166 0.027", "CCO 0.047 0 0.084 0", "0.103 0", "0.179 0 0.178 0.005 0.081 0.003 0.215 0.025", "0.133 0.011", "0.337 0.005", "0.371 0.035", "BPO 0.048 0 0.093 0", "0.160 0", "0.146 0 0.174 0.005 0.081 0.004 0.150 0.006", "0.133 0.008", "0.188 0.003", "0.182 0.003", "MFO 0.029 0 0.084 0", "0.109 0", "0.089 0 0.118 0.004 0.064 0.003 0.111 0.005", "0.098 0.007", "0.138 0.005", "0.114 0.009", "CCO 0.060 0 0.082 0", "0.150 0", "0.104 0 0.155 0.009 0.063 0.004 0.159 0.021", "0.133 0.006", "0.210 0.007", "0.239 0.025", "BPO 0.035 0 0.159 0", "0.054 0", "0.248 0 0.228 0.005 0.114 0.017 0.222 0.010", "0.238 0.012", "0.283 0.006", "0.272 0.008", "MFO 0.004 0 0.064 0", "0.008 0", "0.106 0 0.117 0.004 0.070 0.016 0.167 0.009", "0.165 0.056", "0.234 0.005", "0.241 0.019", "CCO 0.070 0 0.107 0", "0.123 0", "0.202 0 0.205 0.009 0.108 0.013 0.261 0.015", "0.210 0.016", "0.314 0.007", "0.357 0.033", "BPO 0.000 0 0.071 0", "0.000 0", "0.044 0 0.158 0.011 0.048 0.007 0.257 0.007", "0.153 0.034", "0.338 0.013", "0.346 0.016", "MFO 0.000 0 0.015 0", "0.000 0", "0.038 0 0.034 0.002 0.017 0.002 0.114 0.015", "0.048 0.007", "0.100 0.003", "0.124 0.037", "CCO 0.000 0 0.034 0", "0.000 0", "0.000 0 0.080 0.012 0.037 0.005 0.180 0.024", "0.066 0.011", "0.210 0.008", "0.262 0.017", "Table 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.", "Figure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO.", "Fmax", "Method", "m-AUPR", "M-AUPR", "F1", "ACC", "BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain", "0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297", "0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167", "0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356", "0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196", "0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093", "0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284", "0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129", "0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095", "0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200", "0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205", "0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162", "0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286", "0.292\n0.216\n0.346\n0.301\n0.315\n0.151\n0.170\n0.200", "0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085", "0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197", "Table 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.", "performs these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.", "3.3", "Feature Effectiveness Analysis", "To further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.", "4", "Ablation Studies", "In this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.", "Analysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.\nEffectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.", "5", "Conclusion", "This paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.", "Acknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.", "Contribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "token_limit", "num_chunks": 40, "avg_chunk_len": 797.6, "std_chunk_len": 12.920526305069775, "max_chunk_len": 800, "min_chunk_len": 717, "total_chars": 31904, "compression_ratio": 1.000438816449348, "avg_chunk_tokens": 199.225, "max_chunk_tokens": 200, "min_chunk_tokens": 179, "tokenizer": "", "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025\n\nEnhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.ed", "u.cn, wangjunsong@sztu.edu.cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to addre", "ss the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.\n\n1\n\nIntroduction\n\nProtein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zha", "ng et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have\n\nCo-corresponding authors: Xiaopeng Jin, Jie Wen.\n\nshown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-", "protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with", "over-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, conne", "ction strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-\n\nple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and per", "form\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\np", "rotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.\n\n2\n\nMethodology\n\nOur proposed method efficiently captu", "res multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1\n\nhttps://github.com/kioedru/DSRPGO\n\ninto two major branches: one is the multimodal shar", "ed learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.\n\n2.1\n\nReconstructive Pre-training\n\nIn the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input", "to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, t", "he models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,\n\n(2)\n\nwhere N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidi", "rectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:\n\nx sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ\n\n Fσ + Fσ )),\n\n F = F SSCan(BSSCan(SSM (Conv1d\nα\n\n(BSSCan(F SSCan(xsp )))))),\n\nFβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),\n\nFσ = SiLU (F SSCan(xsp )),\n\nFigure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInter", "action Module (BInM), and the Dynamic Selection Module (DSM).\n\nwhere the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI D", "ecoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we\n\nh(k)\n\nk\n\nobtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in repre", "senting protein data. The loss function of PSSI\nencoder-decoder is:\nk\n\nN K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1\n\nh(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij\n\n(3)\n\nwhere N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)\n\nvector of xi\n\nh(k)\n\nand x i\n\n.\n\nPSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.\n\nAlgorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4:", "for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S\n\n6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.\n\nThen, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N", "(s di + L(M SA(s di ))))), (4)\n\nwhere shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1\n\nPN PHi\ni=1\n\n h\n\nh\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij\n\nwhere i denotes the sequence input of the i-t", "h protein, j is\nthe j-th dimension vector of the feature map.\n\n2.2\n\nBidirectional Interaction and Dynamic\nSelection for Protein Function Prediction\n\nIn this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors\n\nwith key (K) vectors from the opposite branch, enabling bidirecti", "onal interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM,", "the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:\n\nFc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get t", "he cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-\n\nresent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-chan", "nel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the predic", "tion loss L.\nL=\n\nN X\nM\nX\n\n1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )\n\ny \n\nlog (1 pm\ni ),\n\n(8)\n\nwhere yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.\n\n3\n\nExperiments\n\nIn this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.\n\n3.1\n\nExperimental Setup\n\nDataset Settings. We construct", "our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are", "in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a\n\nFigure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.\n\nlearning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for th", "e first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], pro", "viding a comprehensive assessment of model accuracy and effectiveness.\n\n3.2\n\nComparison with Unimodal-based and\nMultimodal-based Methods\n\nComparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also co", "mpare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-\n\nMethod\nFmax\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nNa ve \n\nBLAST GeneMANIA Mashup deepNF \n\nNetQuilt\n\nGraph2GO\n\nDeepGraphGO CFAGO\n\nDSRPGO (Ours)\n\nBPO 0.051 0 0.270 0\n\n0.000 0\n\n0.075 0 0.394 0.006 0.164 0.014 0.335 0.010\n\n0.327 0.028\n\n0.439 0.007\n\n0.458 0.006\n\nMFO 0.177 0 0.122 0\n\n0.000 0\n\n0.058 0 0.153 0.004 0.081 0.013 0.196 0.006\n\n0.142 0.035\n\n0.236 0.004\n\n0.254 0.022\n\nCCO 0.121 0 0.196 0\n\n0.031 0\n\n0.000 0 0.297 0.009 0.138 0.013 0.298 0.011\n\n0.209 0.023\n\n0.366 0.018\n\n0.452 0.019\n\nBPO 0.024 0 0.110 0\n\n0.042 0\n\n0.238 0 0.303 0.006 0.077 0.006 0.237 0.014\n\n0.210 0.022\n\n0.328 0.005\n\n0.330 0.006\n\nMFO 0.050 0 0.044 0\n\n0.050", "0\n\n0.053 0 0.089 0.001 0.045 0.007 0.103 0.007\n\n0.080 0.021\n\n0.159 0.003\n\n0.166 0.027\n\nCCO 0.047 0 0.084 0\n\n0.103 0\n\n0.179 0 0.178 0.005 0.081 0.003 0.215 0.025\n\n0.133 0.011\n\n0.337 0.005\n\n0.371 0.035\n\nBPO 0.048 0 0.093 0\n\n0.160 0\n\n0.146 0 0.174 0.005 0.081 0.004 0.150 0.006\n\n0.133 0.008\n\n0.188 0.003\n\n0.182 0.003\n\nMFO 0.029 0 0.084 0\n\n0.109 0\n\n0.089 0 0.118 0.004 0.064 0.003 0.111 0.005\n\n0.098 0.007\n\n0.138 0.005\n\n0.114 0.009\n\nCCO 0.060 0 0.082 0\n\n0.150 0\n\n0.104 0 0.155 0.009 0.063 0.004 0.159 0.021\n\n0.133 0.006\n\n0.210 0.007\n\n0.239 0.025\n\nBPO 0.035 0 0.159 0\n\n0.054 0\n\n0.248 0 0.228 0.005 0.114 0.017 0.222 0.010\n\n0.238 0.012\n\n0.283 0.006\n\n0.272 0.008\n\nMFO 0.004 0 0.064 0\n\n0.008 0\n\n0.106 0 0.117 0.004 0.070 0.016 0.167 0.009\n\n0.165 0.056\n\n0.234 0.005\n\n0.241 0.019\n\nCCO 0.070 0 0.107 0\n\n0.123 0", "0.202 0 0.205 0.009 0.108 0.013 0.261 0.015\n\n0.210 0.016\n\n0.314 0.007\n\n0.357 0.033\n\nBPO 0.000 0 0.071 0\n\n0.000 0\n\n0.044 0 0.158 0.011 0.048 0.007 0.257 0.007\n\n0.153 0.034\n\n0.338 0.013\n\n0.346 0.016\n\nMFO 0.000 0 0.015 0\n\n0.000 0\n\n0.038 0 0.034 0.002 0.017 0.002 0.114 0.015\n\n0.048 0.007\n\n0.100 0.003\n\n0.124 0.037\n\nCCO 0.000 0 0.034 0\n\n0.000 0\n\n0.000 0 0.080 0.012 0.037 0.005 0.180 0.024\n\n0.066 0.011\n\n0.210 0.008\n\n0.262 0.017\n\nTable 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.\n\nFigure 4: Visualization of different feature representations for DSRPGO, and compari", "son with CFAGO.\n\nFmax\n\nMethod\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nBPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain\n\n0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297\n\n0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167\n\n0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356\n\n0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196\n\n0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093\n\n0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284\n\n0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129\n\n0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095\n\n0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200\n\n0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205\n\n0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162\n\n0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286\n\n0.292\n0.216\n0.346\n0.301\n0.3", "15\n0.151\n0.170\n0.200\n\n0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085\n\n0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197\n\nTable 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.\n\nperforms these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby", "further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.\n\n3.3\n\nFeature Effectiveness Analysis\n\nTo further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, mean", "ing proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature b", "y CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.\n\n4\n\nAblation Studies\n\nIn this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.\n\nAnalysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.\nEffectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functio", "ns, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing", "it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.\n\n5\n\nConclusion\n\nThis paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.\n\nAcknowledgements\nThis", "work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.\n\nContribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "format_aware", "num_chunks": 1, "avg_chunk_len": 31917.0, "std_chunk_len": 0.0, "max_chunk_len": 31917, "min_chunk_len": 31917, "total_chars": 31917, "compression_ratio": 1.0000313312654698, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025\n\nEnhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook.com, 2300411008@emal.szu.edu.cn, liucl1996@163.com,\njinxiaopengit@gmail.com, wenjie@hit.edu.cn, liuyumeng@sztu.edu.cn, wangjunsong@sztu.edu.cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms. To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.\nOur proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.\n\n1\n\nIntroduction\n\nProtein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have\n\nCo-corresponding authors: Xiaopeng Jin, Jie Wen.\n\nshown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal., 2024]. As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al., 2023].\nFurthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al., 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024].\nTherefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al., 2024]\nto improve the performance of protein function prediction.\nThese methods mainly adopt two strategies: graph neural networks (GNNs) [You et al., 2021] and autoencoders [Gligorijevic et al., 2018; Fan et al., 2020; Pan et al., 2023].\nGraph2GO [Fan et al., 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al., 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al., 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.\nHowever, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments. We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-\n\nple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks.\nIn addition, large language models play an important role\nin improving protein function prediction. Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal., 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch. In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding.\nBesides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue. Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 . Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics.\n A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function.\n Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.\n We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.\n\n2\n\nMethodology\n\nOur proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1\n\nhttps://github.com/kioedru/DSRPGO\n\ninto two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.\n\n2.1\n\nReconstructive Pre-training\n\nIn the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction.\nPSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.\nMamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al., 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.\nA and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters. In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . . . , C A N 1 B ), y = x K ,\n\n(2)\n\nwhere N is the length of x, and K is a convolutional kernel.\nBiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al., 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.\nAs shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:\n\nx sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ\n\n Fσ + Fσ )),\n\n F = F SSCan(BSSCan(SSM (Conv1d\nα\n\n(BSSCan(F SSCan(xsp )))))),\n\nFβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),\n\nFσ = SiLU (F SSCan(xsp )),\n\nFigure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInteraction Module (BInM), and the Dynamic Selection Module (DSM).\n\nwhere the operation denotes the Hadamard product.\nPSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space. The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer. PSSI Decoder.\nThe architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder. This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we\n\nh(k)\n\nk\n\nobtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI\nencoder-decoder is:\nk\n\nN K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1\n\nh(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij\n\n(3)\n\nwhere N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)\n\nvector of xi\n\nh(k)\n\nand x i\n\n.\n\nPSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.\n\nAlgorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4: for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S\n\n6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.\n\nThen, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)\n\nwhere shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1\n\nPN PHi\ni=1\n\n h\n\nh\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij\n\nwhere i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.\n\n2.2\n\nBidirectional Interaction and Dynamic\nSelection for Protein Function Prediction\n\nIn this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors\n\nwith key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections.\nTherefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi . Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.\nAs shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:\n\nFc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch. In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data.\nDynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014]. The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-\n\nresent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .\nLoss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set. Consequently, we employ an asymmetric loss [Wu et\nal., 2023] as the prediction loss L.\nL=\n\nN X\nM\nX\n\n1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )\n\ny \n\nlog (1 pm\ni ),\n\n(8)\n\nwhere yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.\n\n3\n\nExperiments\n\nIn this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.\n\n3.1\n\nExperimental Setup\n\nDataset Settings. We construct our dataset based on CFAGO\n[Wu et al., 2023]. PPI data comes from the STRING\n[Szklarczyk et al., 2023] database (v11.5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.5.175). A total\nof 19,385 proteins are used for pretraining. For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al., 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).\nMore details about sequence similarity and model performance are in Appendix Sections 3 and 6.\nImplementation Details. We conduct all experiments on\nNVIDIA GTX 4090. We set the dropout rate to 0.1 during\npre-training, and the model trains for 5000 epochs, with a\n\nFigure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.\n\nlearning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0.3 and train for 100 epochs with the AdamW optimizer. The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs.\nCompared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies. Unimodal-based methods:\nNaive [Radivojac et al., 2013], BLAST[Altschul et al.,\n1990], GeneMANIA[Mostafavi et al., 2008], Mashup[Cho\net al., 2016], and deepNF[Gligorijevic et al., 2018].\nMultimodal-based methods: Graph2GO[Fan et al., 2020],\nNetQuilt[Barot et al., 2021], DeepGraphGO[You et al.,\n2021], and CFAGO[Wu et al., 2023].\nEvaluation Metrics. In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al.,\n2021], F1-score (F1) [Wu et al., 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al., 2024], providing a comprehensive assessment of model accuracy and effectiveness.\n\n3.2\n\nComparison with Unimodal-based and\nMultimodal-based Methods\n\nComparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1. DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6.4% in BPO,\n7.7% in MFO, and 15.5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.\nComparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-\n\nMethod\nFmax\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nNa ve \n\nBLAST GeneMANIA Mashup deepNF \n\nNetQuilt\n\nGraph2GO\n\nDeepGraphGO CFAGO\n\nDSRPGO (Ours)\n\nBPO 0.051 0 0.270 0\n\n0.000 0\n\n0.075 0 0.394 0.006 0.164 0.014 0.335 0.010\n\n0.327 0.028\n\n0.439 0.007\n\n0.458 0.006\n\nMFO 0.177 0 0.122 0\n\n0.000 0\n\n0.058 0 0.153 0.004 0.081 0.013 0.196 0.006\n\n0.142 0.035\n\n0.236 0.004\n\n0.254 0.022\n\nCCO 0.121 0 0.196 0\n\n0.031 0\n\n0.000 0 0.297 0.009 0.138 0.013 0.298 0.011\n\n0.209 0.023\n\n0.366 0.018\n\n0.452 0.019\n\nBPO 0.024 0 0.110 0\n\n0.042 0\n\n0.238 0 0.303 0.006 0.077 0.006 0.237 0.014\n\n0.210 0.022\n\n0.328 0.005\n\n0.330 0.006\n\nMFO 0.050 0 0.044 0\n\n0.050 0\n\n0.053 0 0.089 0.001 0.045 0.007 0.103 0.007\n\n0.080 0.021\n\n0.159 0.003\n\n0.166 0.027\n\nCCO 0.047 0 0.084 0\n\n0.103 0\n\n0.179 0 0.178 0.005 0.081 0.003 0.215 0.025\n\n0.133 0.011\n\n0.337 0.005\n\n0.371 0.035\n\nBPO 0.048 0 0.093 0\n\n0.160 0\n\n0.146 0 0.174 0.005 0.081 0.004 0.150 0.006\n\n0.133 0.008\n\n0.188 0.003\n\n0.182 0.003\n\nMFO 0.029 0 0.084 0\n\n0.109 0\n\n0.089 0 0.118 0.004 0.064 0.003 0.111 0.005\n\n0.098 0.007\n\n0.138 0.005\n\n0.114 0.009\n\nCCO 0.060 0 0.082 0\n\n0.150 0\n\n0.104 0 0.155 0.009 0.063 0.004 0.159 0.021\n\n0.133 0.006\n\n0.210 0.007\n\n0.239 0.025\n\nBPO 0.035 0 0.159 0\n\n0.054 0\n\n0.248 0 0.228 0.005 0.114 0.017 0.222 0.010\n\n0.238 0.012\n\n0.283 0.006\n\n0.272 0.008\n\nMFO 0.004 0 0.064 0\n\n0.008 0\n\n0.106 0 0.117 0.004 0.070 0.016 0.167 0.009\n\n0.165 0.056\n\n0.234 0.005\n\n0.241 0.019\n\nCCO 0.070 0 0.107 0\n\n0.123 0\n\n0.202 0 0.205 0.009 0.108 0.013 0.261 0.015\n\n0.210 0.016\n\n0.314 0.007\n\n0.357 0.033\n\nBPO 0.000 0 0.071 0\n\n0.000 0\n\n0.044 0 0.158 0.011 0.048 0.007 0.257 0.007\n\n0.153 0.034\n\n0.338 0.013\n\n0.346 0.016\n\nMFO 0.000 0 0.015 0\n\n0.000 0\n\n0.038 0 0.034 0.002 0.017 0.002 0.114 0.015\n\n0.048 0.007\n\n0.100 0.003\n\n0.124 0.037\n\nCCO 0.000 0 0.034 0\n\n0.000 0\n\n0.000 0 0.080 0.012 0.037 0.005 0.180 0.024\n\n0.066 0.011\n\n0.210 0.008\n\n0.262 0.017\n\nTable 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.\n\nFigure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO.\n\nFmax\n\nMethod\n\nm-AUPR\n\nM-AUPR\n\nF1\n\nACC\n\nBPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain\n\n0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297\n\n0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167\n\n0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356\n\n0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196\n\n0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093\n\n0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284\n\n0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129\n\n0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095\n\n0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200\n\n0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205\n\n0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162\n\n0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286\n\n0.292\n0.216\n0.346\n0.301\n0.315\n0.151\n0.170\n0.200\n\n0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085\n\n0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197\n\nTable 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.\n\nperforms these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.\n\n3.3\n\nFeature Effectiveness Analysis\n\nTo further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al., 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster. A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.\nTo further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al., 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.\nIn contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.\n\n4\n\nAblation Studies\n\nIn this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.\n\nAnalysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches.\nEffectiveness of BInM. Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it.\nEffectiveness of DSM. To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.\nImpact of Sequence and Spatial Structure Features.\nTo verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type. Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance.\nImpact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing it.\nAs shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.\n\n5\n\nConclusion\n\nThis paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.\n\nAcknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.\n\nContribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
{"method": "hybrid", "num_chunks": 265, "avg_chunk_len": 118.82641509433962, "std_chunk_len": 175.94280819366554, "max_chunk_len": 772, "min_chunk_len": 1, "total_chars": 31489, "compression_ratio": 1.0136238051383022, "chunks": ["arXiv:2511.04040v1 [cs.LG] 6 Nov 2025", "Enhancing Multimodal Protein Function Prediction Through Dual-Branch\nDynamic Selection with Reconstructive Pre-Training\nXiaoling Luo1 , Peng Chen2 , Chengliang Liu3 , Xiaopeng Jin4 , Jie Wen5 , Yumeng\nLiu4 and Junsong Wang4\n1\nCollege of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n2\nCollege of Applied Technology, Shenzhen University, Shenzhen, China\n3\nLaboratory for Artificial Intelligence in Design, Hong Kong\n4\nCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China\n5\nCollege of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\nxiaolingluoo@outlook. com, 2300411008@emal. szu.", "edu. cn, liucl1996@163. com,\njinxiaopengit@gmail.", "com, wenjie@hit. edu. cn, liuyumeng@sztu.", "edu. cn, wangjunsong@sztu. edu.", "cn\nAbstract\nMultimodal protein features play a crucial role in\nprotein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to\nprotein attributes and interaction networks, making\nit challenging to decipher their complex interconnections. In this work, we propose a multimodal\nprotein function prediction method (DSRPGO) by\nutilizing dynamic selection and reconstructive pretraining mechanisms.", "To acquire complex protein information, we introduce reconstructive pretraining to mine more fine-grained information\nwith low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM)\nto facilitate interactive learning among multimodal\nfeatures. Additionally, to address the difficulty of\nhierarchical multi-label classification in this task,\na Dynamic Selection Module (DSM) is designed\nto select the feature representation that is most\nconducive to current protein function prediction.", "Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets,\nthereby outperforming other benchmark models.", "1", "Introduction", "Protein function prediction has become a key challenge in\nbiology, with the rapid development of bioinformatics [Hasselgren and Oprea, 2024]. The Gene Ontology (GO) framework [Ma et al., 2025] standardizes protein functions into\nthree categories: biological process (BPO), molecular function (MFO), and cellular component (CCO) [Aleksander et\nal., 2023]. In recent decades, numerous deep learning methods [You et al., 2021; Zhang et al., 2023] have been developed to predict protein functions. However, using singlemodal features often faces data limitations [Kulmanov and\nHoehndorf, 2020]. Many studies [Fan et al., 2020] have", "Co-corresponding authors: Xiaopeng Jin, Jie Wen.", "shown that using protein sequence information significantly\nimproves the accuracy of MFO. Still, many proteins share\nfunctional similarities but have dissimilar sequences [Lin et\nal. , 2024].", "As a result, for proteins with low sequence similarity, the accuracy of predictions may be compromised. Moreover, structure-based methods usually perform better, but the\nhigh complexity of protein structures and data acquisition\ncosts limit their application [Paysan-Lafosse et al. , 2023].", "Furthermore, the noise introduced during the generation of\nprotein-protein interaction (PPI) networks [Wang et al. , 2022]\nthrough high-throughput techniques poses risks to the accuracy of predictions [Chen and Luo, 2024]. Therefore, integrating these different types of protein data\nand taking advantage of their complementary advantages in\nfunctional prediction is an important way [Zhao et al.", ", 2024]\nto improve the performance of protein function prediction. These methods mainly adopt two strategies: graph neural networks (GNNs) [You et al. , 2021] and autoencoders [Gligorijevic et al.", ", 2018; Fan et al. , 2020; Pan et al. , 2023].", "Graph2GO [Fan et al. , 2020] integrates sequence similarity\nand PPI networks using GNNs, treating protein sequences\nand structures as node features. However, those using GNNs\n[Zhou et al.", ", 2019] may amplify noise and face issues with\nover-smoothing. To address these limitations, CFAGO [Wu\net al. , 2023] introduces Transformer-based fusion within autoencoders to enhance multimodal feature integration.", "However, current multimodal approaches mainly fuse information without exploring the potential complementarity\nbetween different modalities. To address this issue, we propose a multimodal method for protein function prediction that\nefficiently mines the complex internal relationships among\nspatial structure features, such as PPI networks, subcellular\nlocations, and protein domains, as well as sequence features,\nspecifically the amino acid sequence. Furthermore, due to the\ncomplexity of protein information, existing models tend to ignore the detailed features inside the information, such as PPI\nlocal network topology, connection strength, amino acid frequency distribution, and key sequence fragments.", "We add a\nreconstruction pre-training step to obtain more low-semantic\nand fine-grained features from protein information of multi-", "ple modes. By learning these basic features, the model provides a richer representational basis for downstream tasks. In addition, large language models play an important role\nin improving protein function prediction.", "Inspired by large\nlanguage models, the protein sequence information in our\nmethod is extracted using the pre-trained ProtT5 [Elnaggar et\nal. , 2021]. In this work, to better learn multimodal information, our proposed DSRPGO model includes a shared and an\ninteractive learning branch.", "In the shared learning branch, we\nconcatenate features from different modalities and perform\njoint analysis in a unified representation space. Moreover,\nwe introduce the Bidirectional Interaction Module (BInM),\nwhere each modality both influences and receives information from others, enhancing overall understanding. Besides, faced with thousands of protein functions, accurately predicting the protein function of a sample remains a\nchallenging issue.", "Protein function prediction is essentially a\ncomplex hierarchical multi-label classification problem. In\nthis situation, we propose the Dynamic Selection Module\n(DSM) to dynamically select the optimal feature combination for fitting more diverse protein functions. The code and\nsupplementary materials have been open-sourced1 .", "Our main\ncontributions can be summarized as follows:\n We propose a multimodal feature-based approach for\nprotein function prediction that overcomes the limitations of single-modality methods, effectively representing protein functional characteristics. A reconstructive pre-training phase is designed to make\nthe model capable of learning more low-semantic finegrained features to assist the model in understanding\nprotein function. Our proposed BInM incorporates a bidirectional interaction mechanism to promote efficient fusion and information exchange between sequence and spatial features,\nenhancing the model s ability to capture strong protein\ninformation between different modes.", "We construct the DSM that enables the model to adaptively select channel features most relevant to specific\nfunctional labels, resulting in enhanced performance.", "2", "Methodology", "Our proposed method efficiently captures multimodal information about proteins through a strategy for two-step training. In the pre-training stage, we use the encoder-decoder\nmodel to learn and inject multimodal knowledge. For spatial features including PPI, subcellular location, and protein\ndomains, a Protein Spatial Structured Information (PSSI)\nencoder-decoder model using the BiMamba blocks is introduced in this stage. To mine sequence features including\nprotein sequences, we design a Protein Sequence Information (PSeI) encoder-decoder model based on the Transformer\nblocks for pre-training. Then, during our DSRPGO model\ntraining phase, we integrate and learn features from multimodal information. The proposed model is primarily divided\n1", "https://github.com/kioedru/DSRPGO", "into two major branches: one is the multimodal shared learning branch (MSL-Branch), and the other is the multimodal\ninteractive learning branch (MIL-Branch). Protein data are\nprocessed through these branches to generate several sets of\nfeatures, which serve as inputs for DSM. Finally, the model\ndynamic selects the optimal features for the current protein,\nto enhance performance in protein function prediction. An\nillustration of our proposed method can be seen in Figure 1.", "2.1", "Reconstructive Pre-training", "In the reconstructive pre-training stage, to obtain feature extractors that are good at mining fine-grained features from\nmulti-modal protein information, we utilize the PSSI and\nPSeI encoder-decoder model for feature reconstruction. PSSI Encoder-Decoder Learning\nThe PPI network gets an N N adjacency matrix by matrix\nconversion as input to the encoder. Moreover, another input\nto the encoder is obtained by concatenating the bag-of-words\nencodings of subcellular location and Protein Domain.", "Mamba Preliminaries. Mamba [Gu and Dao, 2023] extends the capabilities of the State-Space Models (SSMs) [Gu\net al. , 2023] by enabling the transformation of a continuous\n1D input xt R to yt R via a learnable hidden state\nht RN with discrete parameters A RN N , B R1 N ,\nand C R1 N as follows:\nht = A ht 1 + B xt , yt = Cht + Dht ,\n(1)\nA = e A , B = ( A) 1 (e A I) B, C = C.", "A and B are continuous A and B converted to discrete evolution parameters using a timescale parameter . To process\ndiscrete-time sequences sampled at intervals of , SSMs can\nbe calculated using the recurrence formula. C represents the\nprojection parameters.", "In addition, the models compute output through a global convolution as follows:\nK = (C B , C A B , . . .", ", C A N 1 B ), y = x K ,", "(2)", "where N is the length of x, and K is a convolutional kernel. BiMamba Block. Inspired by the selective scan mechanism in Vision Mamba [Zhu et al.", ", 2024], BiMamba Block\nintroduces a novel bidirectional selective scanning mechanism designed for protein data, capturing both the start and\nend of spatial structure features for enhanced detail and context. Multi-dimensional features are first converted into onedimensional vectors. Features xsp from PPI, subcellular location, and protein domains are then passed through BiMamba\nblocks, interleaved with linear layers and residual operations.", "As shown in Figure 2, forward (FSScan) and backward selective scans (BSScan) extract bidirectional matrix features via\npositional transformations and reconstructions. Transformed\ntokens are scanned using Equation 1 to produce new features,\nwith BiMamba s output x sp expressed as:", "x sp =F SSCan(xsp ) + F SSCan(Linear(Fα Fσ + Fβ", "Fσ + Fσ )),", "F = F SSCan(BSSCan(SSM (Conv1d\nα", "(BSSCan(F SSCan(xsp )))))),", "Fβ = F SSCan(SSM (Conv1d(F SSCan(xsp )))),", "Fσ = SiLU (F SSCan(xsp )),", "Figure 1: An illustration of our proposed method. This method is mainly divided into two stages. The first stage is to pre-train the Protein\nSpatial Structure Information (PSSI) encoder and Protein Sequence Information (PSeI) encoder for the injection of multimodal knowledge\n. The second stage is training our proposed DSRPGO model, which consists of an MSL-Branch, a MIL-Branch with the Bidirectional\nInteraction Module (BInM), and the Dynamic Selection Module (DSM).", "where the operation denotes the Hadamard product. PSSI Encoder. In this section, we propose a PSSI encoder\narchitecture designed to effectively map high-dimensional input data into a low-dimensional latent space.", "The PSSI encoder consists of multilayer perceptrons (MLPs), BiMamba\nblock, Linear and Norm layers, which work in concert to\nextract features from the input data and generate a compact latent representation. Assume that the input feature\nk\nh(k)\nxi\n RHi is a high-dimensional vector of the i-th protein, where Hik represents the feature dimension of the k-th\ninput source. This feature is reconstructed using MLP to outd(k)\nput a low-dimensional representation xi\n RD , where D\ndenotes the size of the MLP hidden layer.", "PSSI Decoder. The architecture of the PSSI decoder is a counterpart to that\nof the encoder. The PSSI decoder rebuilds the given protein\nspatial structure information based on the hidden representations output by the encoder.", "This process involves BiMamba\ncomputation and residual operations, optimizing the crossentropy loss function to enhance the performance. After takd(k)\ning the output xi of the PSSI encoder and passing through\nthe BiMamba block, alternating Linear and Norm layers, we", "h(k)", "k", "obtain the recovered high-dimensional features x i\n RHi .\nThe overarching objective of the encoder-decoder architecture is to minimize the sample wise binary cross-entropy\nloss between the original and reconstructed source features,\nthereby enhancing the model s predictive accuracy and fidelity in representing protein data. The loss function of PSSI\nencoder-decoder is:\nk", "N K Hi\n1 XXX\nh(k)\nh(k)\n xij log x ij\nLsp =\nN i=1\nk=1 j=1", "h(k)\nh(k)\n,\n+ 1 xij\nlog 1 x ij", "(3)", "where N is the number of total proteins, K is the number\nh(k)\nh(k)\nof input sources, xij and x ij denotes the j-th dimension\nh(k)", "vector of xi", "h(k)", "and x i", ".", "PSeI Encoder-Decoder Learning\nIn PSeI encoder-decoder, the transformer block with multihead self-attention (MSA) mechanism [Dosovitskiy et al.,\n2021] extracts long-distance features from protein sequences.", "Algorithm 1 Dynamic Selection Moudle Procedure\nInput: Protein vector Xdsm , Threshold t\nOutput: Fusion feature after DSM\n1: Initialize expert weights W 0N .\n2: Compute expert confidence coefficients\np Softmax(MLP(Xdsm )).\n3: Select active experts S {Ei |p i t}.\n4: for each experts Ei in S do\n5:\nNormalize p to obtain weights Wi P p i p j .\nEj S", "6: end for\n7: return DSM(Xdsm ) Concat(Wi Ei (Xdsm ))\nFigure 2: Structure of the BiMamba block.", "Then, to further leverage these features, we use the pretrained ProtT5 [Elnaggar et al., 2021] model to parse the protein sequences. To achieve this, we froze the parameters of\nProtT5 and connected it to the PSeI encoder for further pretaining.\nPSeI Encoder. The PSeI encoder consists of an MLP\nblock and 6 self-attention blocks. The self-attention block\nincludes an MSA computation layer, as well as alternating\nlinear and norm layers, connected through a residual structure. Assuming the input of the self-attention block is s di =\nM LP (shi ), the output feature is s di RD :\ns di = N (N (s di + L(M SA(s di ))) + L(N (s di + L(M SA(s di ))))), (4)", "where shi RHi is the i-th input sequence feature of encoder,\nand Hi is the dimension of input feature. L(x) denotes the\nfuction of Linear layer, and N (x) denotes the Norm layer.\nPSeI Decoder. The PSeI decoder takes the hidden states\nfrom the encoder as input, which contains compressed information about the input sequence. To obtain the final protein\nsequence encoding, we designed the PSeI decoder using a\ncombination of 6 self-attention blocks and one MLP block.\nThen, the output feature of the PSeI decoder is s hi RHi .\nLike the PSSI encoder-decoder, the loss function Lse for the\nPSeI encoder-decoder also adopts the form of cross-entropy:\nLse = N1", "PN PHi\ni=1", "h", "h\nh\nh\n, (5)\nj=1 sij log s ij + 1 sij log 1 s ij", "where i denotes the sequence input of the i-th protein, j is\nthe j-th dimension vector of the feature map.", "2.2", "Bidirectional Interaction and Dynamic\nSelection for Protein Function Prediction", "In this section, we apply the encoders sensitive to low semantic features obtained in the pre-training stage to high semantic tasks. Specifically, to improve the performance of protein\nfunction prediction, BInM and DSM modules are proposed\nto capture deep interaction information between multimodal\nfeatures and dynamically screen the features most suitable for\nthe current task.\nBidirectional Interaction Module\nThe proposed BInM enhances the model s ability to learn\ncomplex patterns by integrating information across modalities. Using cross-attention, it compares query (Q) vectors", "with key (K) vectors from the opposite branch, enabling bidirectional interaction. This approach captures interdependencies between branches more effectively, similar to MSA but\nfocused on cross-branch connections. Therefore, we assume that the features transformed by PPI\n(1)\nare represented as xi , and the features obtained from the encoding of subcellular location and protein domains are con(2)\ncatenated to form xi , while the features extracted through\nthe ProtT foundation model for protein sequences are denoted\n(3)\n(1)\n(2)\nas xi .", "Subsequently, xi and xi get features with the\nsame dimension after the MLP reconstruction features, and\ntheir concatenated feature map x\neB\ni is used as the input of the\nfirst branch of BInM. Similarly, xB\ni , the input to the second\n(3)\nbranch of BInM, is derived from xi after its transformation through the MLP. In BInM, the input embedded patches\nFa1 RLa Da and Fa2 RLa Da are initially and randomly divided into multiple heads vectors Fb1 RLa Db Hb\nand Fb2 RLa Db Hb , where Hb is the number of multiple\nheads.", "As shown in Figure 1, Fb1 and Fb2 are converted into\nqueries Q1 (Fb1 ) and Q2 (Fb2 ). The key K1 and value V 1 of\nFb1 , and the key K2 and value V 2 of Fb2 are obtained using\nthree generators Q, K, and V. Then, Fc1 RLa Db Hb obtained by cross-attention is defined as:", "Fc1 = sof tmax(Q1 (Fb1 ) K2 (Fb2 )T ) V 2 (Fb2 ), (6)\nwhere the operation T means matrix transpose, the operation\n represents matrix multiplication, and the goal of sof tmax\nfunction is to normalize the Fc1 . Finally, the cross-attention\noutput feature Fd1 RLa Da of the first branch is obtained\nby feature mapping. Similarly, we can get the cross-attention\noutput Fd2 RLa Da of the second branch.", "In this way, the\nmodel takes into account not only the meaning of each branch\nitself but also the relationships with other branch features, resulting in a more complete representation of multimodal data. Dynamic Selection Module\nIn the final feature selection stage, we introduce DSM to\nenhance key features and mitigate the impact of conflicting\nones. As illustrated in Algorithm 1 and Figure 1, this module employs an improved Mixture-of-Experts (MoE) strategy based on Masoudnia et al [Masoudnia and Ebrahimpour,\n2014].", "The MSL-Branch and MIL-Branch each output a single vector with three channels, where the three channels rep-", "resent PPI, sequence, and subcellular localization combined\nwith domain features, respectively. All six-channel feature\nmaps serve as the input Xdsm = (x1dsm , x2dsm , , xVdsm )\nfor the DSM. The function of DSM is:\np i\nDSM(Xdsm ) = Concat( P\n Ei (Xdsm )),\n(7)\nEj S p j\nwhere Ej is the experts belonging to the selected expert\ngroup S, p i denotes the confidence coefficient of expert Ei .", "Loss Functions\nIn this work, protein function prediction is modeled as the\nmulti-label classification task. The predictor, constructed\nfrom fully connected layers, takes the output features of the\nDSM as input and produces an M -dimensional score vector\nof GO terms: Pi = (p1i , p2i , , pM\ni )). In the context of\nprotein function prediction using GO terms, there are significantly more negative proteins than positive ones in the training set.", "Consequently, we employ an asymmetric loss [Wu et\nal. , 2023] as the prediction loss L. L=", "N X\nM\nX", "1\ny+\n y m (1 pm\nlog (pm\ni )\ni )\nN M i=1 m=1 i\n (1 yim ) (pm\ni )", "y", "log (1 pm\ni ),", "(8)", "where yim represents the ground truth label for the i-th protein, while pm\ni denotes the predicted score. The symbols\n{y+} and {y } refer to the positive and negative focusing\nparameters respectively.", "3", "Experiments", "In this section, we present the experimental setup, including\nthe datasets, baseline models, training details, and evaluation metrics. Then we provide an analysis of the experimental results, supported by ablation studies and Davies-Bouldin\nscores to validate the effectiveness of the model.\nFurther experiments on the model components, structures,\nand parameters can be seen in Appendix Sections 1, 2, and 5.", "3.1", "Experimental Setup", "Dataset Settings. We construct our dataset based on CFAGO\n[Wu et al. , 2023].", "PPI data comes from the STRING\n[Szklarczyk et al. , 2023] database (v11. 5), and protein sequences, subcellular localization, and domain data are from\nthe UniProt [Consortium, 2022] database (v3.", "5. 175). A total\nof 19,385 proteins are used for pretraining.", "For fine-tuning,\nwe collect protein function annotations from the Gene Ontology [Aleksander et al. , 2023] database (v2022-01-13). The\nfine-tuning datasets for each GO branch, split by two-time\npoints, including BPO: 3,197 training, 304 validation, 182\ntesting proteins (45 GO terms), MFO: 2,747 training, 503 validation, 719 testing proteins (38 GO terms), and CCO: 5,263\ntraining, 577 validation, 119 testing proteins (35 GO terms).", "More details about sequence similarity and model performance are in Appendix Sections 3 and 6. Implementation Details. We conduct all experiments on\nNVIDIA GTX 4090.", "We set the dropout rate to 0. 1 during\npre-training, and the model trains for 5000 epochs, with a", "Figure 3: Davies Bouldin Score comparison of different protein\nfeatures represents. o PPI, o Attribute, and o Sequence represent the original embedding of PPI, subcellular localization combined with domain, and protein language model, respectively.\nMSL embedding, MSI embedding, and DSM embedding represent\nthe embedding from MSL-Branch, MIL-Branch, and DSM, respectively.", "learning rate of 1e-5 for the first 2500 epochs and 1e-6 for the\nremaining 2500 epochs. During fine-tuning, we use a dropout\nrate of 0. 3 and train for 100 epochs with the AdamW optimizer.", "The learning rate is set to 1e-3 for the first 50 epochs\nand reduced to 1e-4 for the remaining 50 epochs. Compared Methods. We compare DSRPGO with nine\nmethods, which are categorized into two groups based on\ntheir data utilization strategies.", "Unimodal-based methods:\nNaive [Radivojac et al. , 2013], BLAST[Altschul et al. ,\n1990], GeneMANIA[Mostafavi et al.", ", 2008], Mashup[Cho\net al. , 2016], and deepNF[Gligorijevic et al. , 2018].", "Multimodal-based methods: Graph2GO[Fan et al. , 2020],\nNetQuilt[Barot et al. , 2021], DeepGraphGO[You et al.", ",\n2021], and CFAGO[Wu et al. , 2023]. Evaluation Metrics.", "In this study, we evaluate predictive\nperformance using five metrics: micro-averaged AUPR (mAUPR) and macro-averaged AUPR (M-AUPR) [Peng et al. ,\n2021], F1-score (F1) [Wu et al. , 2023], accuracy (ACC), and\nF-max score (Fmax )[Lin et al.", ", 2024], providing a comprehensive assessment of model accuracy and effectiveness.", "3.2", "Comparison with Unimodal-based and\nMultimodal-based Methods", "Comparision with Unimodal-based Methods. Most of the\nprevious methods are based on unimodal protein features, so\nto verify the performance of our multimodal-based method,\nwe compare our method with unimodal-based methods. The\nexperimental results are shown in Table 1.", "DSRPGO significantly outperforms unimodal-based methods across various\nmetrics, except for M-AUPR in MFO. Compared to unimodal\nmethods, DSRPGO improves Fmax by at least 6. 4% in BPO,\n7.", "7% in MFO, and 15. 5% in CCO. This demonstrates the advantage of integrating multimodal data for protein function\nprediction.", "Comparision with Multimodal-based Methods. To better evaluate our method, we also compare DSRPGO with\nother state-of-the-art multimodal-based methods, including\nCFAGO, DeepGraphGO, Graph2GO, and NetQuilt. The detailed results in Table 1 show that DSRPGO generally out-", "Method\nFmax", "m-AUPR", "M-AUPR", "F1", "ACC", "Na ve", "BLAST GeneMANIA Mashup deepNF", "NetQuilt", "Graph2GO", "DeepGraphGO CFAGO", "DSRPGO (Ours)", "BPO 0.051 0 0.270 0", "0.000 0", "0.075 0 0.394 0.006 0.164 0.014 0.335 0.010", "0.327 0.028", "0.439 0.007", "0.458 0.006", "MFO 0.177 0 0.122 0", "0.000 0", "0.058 0 0.153 0.004 0.081 0.013 0.196 0.006", "0.142 0.035", "0.236 0.004", "0.254 0.022", "CCO 0.121 0 0.196 0", "0.031 0", "0.000 0 0.297 0.009 0.138 0.013 0.298 0.011", "0.209 0.023", "0.366 0.018", "0.452 0.019", "BPO 0.024 0 0.110 0", "0.042 0", "0.238 0 0.303 0.006 0.077 0.006 0.237 0.014", "0.210 0.022", "0.328 0.005", "0.330 0.006", "MFO 0.050 0 0.044 0", "0.050 0", "0.053 0 0.089 0.001 0.045 0.007 0.103 0.007", "0.080 0.021", "0.159 0.003", "0.166 0.027", "CCO 0.047 0 0.084 0", "0.103 0", "0.179 0 0.178 0.005 0.081 0.003 0.215 0.025", "0.133 0.011", "0.337 0.005", "0.371 0.035", "BPO 0.048 0 0.093 0", "0.160 0", "0.146 0 0.174 0.005 0.081 0.004 0.150 0.006", "0.133 0.008", "0.188 0.003", "0.182 0.003", "MFO 0.029 0 0.084 0", "0.109 0", "0.089 0 0.118 0.004 0.064 0.003 0.111 0.005", "0.098 0.007", "0.138 0.005", "0.114 0.009", "CCO 0.060 0 0.082 0", "0.150 0", "0.104 0 0.155 0.009 0.063 0.004 0.159 0.021", "0.133 0.006", "0.210 0.007", "0.239 0.025", "BPO 0.035 0 0.159 0", "0.054 0", "0.248 0 0.228 0.005 0.114 0.017 0.222 0.010", "0.238 0.012", "0.283 0.006", "0.272 0.008", "MFO 0.004 0 0.064 0", "0.008 0", "0.106 0 0.117 0.004 0.070 0.016 0.167 0.009", "0.165 0.056", "0.234 0.005", "0.241 0.019", "CCO 0.070 0 0.107 0", "0.123 0", "0.202 0 0.205 0.009 0.108 0.013 0.261 0.015", "0.210 0.016", "0.314 0.007", "0.357 0.033", "BPO 0.000 0 0.071 0", "0.000 0", "0.044 0 0.158 0.011 0.048 0.007 0.257 0.007", "0.153 0.034", "0.338 0.013", "0.346 0.016", "MFO 0.000 0 0.015 0", "0.000 0", "0.038 0 0.034 0.002 0.017 0.002 0.114 0.015", "0.048 0.007", "0.100 0.003", "0.124 0.037", "CCO 0.000 0 0.034 0", "0.000 0", "0.000 0 0.080 0.012 0.037 0.005 0.180 0.024", "0.066 0.011", "0.210 0.008", "0.262 0.017", "Table 1: Comparison results of different methods. Unimodal-based methods are marked with , while the rest are multimodal-based\nmethods. The best results are highlighted in bold, and the sub-optimal results are underlined. After the is the standard deviation of the\nexperimental results.", "Figure 4: Visualization of different feature representations for DSRPGO, and comparison with CFAGO.", "Fmax", "Method", "m-AUPR", "M-AUPR", "F1", "ACC", "BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO BPO MFO CCO\nMSLB\nMILB\nMSLB+MILB\nw/o BInM\nw/o DSM\nw/o SP-F\nw/o SE-F\nw/o pretrain", "0.437\n0.310\n0.458\n0.435\n0.397\n0.216\n0.251\n0.297", "0.179\n0.179\n0.254\n0.193\n0.190\n0.173\n0.238\n0.167", "0.371\n0.420\n0.452\n0.333\n0.378\n0.263\n0.363\n0.356", "0.315\n0.180\n0.330\n0.313\n0.275\n0.106\n0.119\n0.196", "0.108\n0.091\n0.166\n0.116\n0.105\n0.059\n0.117\n0.093", "0.304\n0.330\n0.371\n0.266\n0.302\n0.164\n0.219\n0.284", "0.173\n0.138\n0.182\n0.174\n0.163\n0.105\n0.115\n0.129", "0.102\n0.113\n0.114\n0.106\n0.113\n0.039\n0.099\n0.095", "0.197\n0.220\n0.239\n0.186\n0.205\n0.115\n0.181\n0.200", "0.261\n0.236\n0.272\n0.265\n0.265\n0.174\n0.179\n0.205", "0.172\n0.162\n0.241\n0.180\n0.173\n0.004\n0.224\n0.162", "0.311\n0.342\n0.357\n0.305\n0.328\n0.226\n0.322\n0.286", "0.292\n0.216\n0.346\n0.301\n0.315\n0.151\n0.170\n0.200", "0.076\n0.090\n0.124\n0.088\n0.092\n0.000\n0.133\n0.085", "0.190\n0.220\n0.262\n0.151\n0.190\n0.145\n0.193\n0.197", "Table 2: Results of Ablation Studies. The overall model is denoted as MSLB+MILB , where MSLB and MILB are the backbone\ncomponents: MSL-Branch and MIL-Branch. w/o BInM and w/o DSM represent removing the BInM and DSM modules from the overall\nmodel. w/o SP-F refers to removing spatial structure features from the input, while w/o SE-F indicates removing sequence features. The\nbest results are marked in bold.", "performs these methods. Compared to multimodal methods, DSRPGO improves the Fmax metric by at least 1.9% in\nBPO, 1.8% in MFO, and 8.6% in CCO. This indicates that\nDSRPGO s architecture is more effective in learning deep\nrepresentations among multimodal features, thereby further\nenhancing overall performance. At the same time, we observe\nthat DSRPGO does not perform optimally in M-AUPR. This\nis because M-AUPR evaluates each class equally, including\nthose with fewer samples, which may not reflect the model s\noverall performance. In contrast, m-AUPR aggregates performance across all classes, offering a more comprehensive\nmeasure of predictive capability. In addition, we discuss the\nStructure-based and PLM-based comparison methods, as detailed in Appendix Section 4.", "3.3", "Feature Effectiveness Analysis", "To further evaluate the distinguishing power of the multimodal features extracted by different components of\nDSRPGO, we use Davies-Bouldin (DB) [Wu et al. , 2023]\nscores. In the calculation of DB scores, GO terms are set as\nthe labels for protein clusters, meaning proteins sharing the\nsame GO term set are grouped into the same cluster.", "A lower\nDB score indicates more compact clusters and clearer separation. As shown in Figure 3, DSRPGO components effectively\ncapture multimodal features. Among them, DSM embedding\nperforms best, indicating that DSM successfully integrates inputs from the MIL and MSL branches.", "To further analyze the discriminative power of protein\nfeatures, we visualize them using t-SNE [Chatzimparmpas\net al. , 2020], as shown in Figure 4. Raw input features\n(o PPI, o Attribute, o Sequence), which are not pre-trained,\nshow distinct patterns but lack clear clustering boundaries.", "In contrast, the output of the feature by various modules\nof DSRPGO achieves better clustering results. Additionally, compared to the output of the feature by CFAGO\n(cf embedding), DSRPGO demonstrates significantly superior performance.", "4", "Ablation Studies", "In this section, the contributions of each component in\nDSRPGO are evaluated, as shown in Table 2.", "Analysis for Backbone Components. According to lines\n1,2, and 3 of Table 2, the results of the backbone network only\nusing MSL-Branch or MIL-Branch are not as good as those\nusing combined branches. Effectiveness of BInM.", "Considering the correlation of features among space and sequence, this method uses the BInM\nblock to facilitate bidirectional multimodal feature interaction\nbefore DSM. As shown in rows 3 and 4 of Table 2, we verify\nthe validity of BInM for the overall model by removing it. Effectiveness of DSM.", "To enable effective feature selection and accurate prediction of protein functions, DSM is\nused to select channel features most relevant to specific functional labels adaptively. At the same time, it reduces the interference and conflict caused by redundant features. As shown\nin rows 3 and 5 of Table 2, DSM has a positive impact on\nprotein function prediction.", "Impact of Sequence and Spatial Structure Features. To verify the complementarity between sequence and spatial structure features, we perform an ablation study, retaining only spatial structure or sequence features. For the BInM\nmodule, it is removed as no interaction occurs with a single\nfeature type.", "Rows 6 and 7 of Table 2 show that removing\nfeature interaction significantly reduces model performance. Impact of Pre-training. To evaluate the contribution of\npre-training, we conduct an ablation study by removing it.", "As shown in the last row of Table 2, the model s performance\ndrops significantly across all metrics without pre-training.", "5", "Conclusion", "This paper proposes a dual-branched multimodal method for\nprotein function prediction with reconstructive pre-training.\nThe proposed method enhances the model s ability to integrate multimodal features through two key components: the\nBInM and the DSM, leading to significant performance gains.\nExperimental results show that the DSRPGO outperforms\ncurrent state-of-the-art unimodal and multimodal methods\nacross multiple metrics. These results underscore the importance of integrating multimodal data to enhance protein function prediction, and validate the superiority of the BInM and\nthe DSM in multimodal protein data integration.", "Acknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China under Grant No. 62302317,\nthe Natural Science Foundation of Guangdong Province under Grant 2025A1515010184, the project of Shenzhen Science and Technology Innovation Committee under Grant\nJCYJ20240813141424032 and JCYJ20240813112420027,\nand the Foundation for Young innovative talents in ordinary\nuniversities of Guangdong under Grant 2024KQNCX042,\nthe Stable Support Projects for Shenzhen Higher Education Institutions under grant 20231122005530001 and\n20220715183602001, and Guangdong Basic and Applied Basic Research Foundation grant 2024A1515220079.", "Contribution Statement\nXiaoling Luo and Peng Chen contributed equally to this\nwork."]}
