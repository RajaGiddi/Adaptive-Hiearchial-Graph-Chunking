{"method": "ahgc", "num_chunks": 81, "avg_chunk_len": 952.1604938271605, "std_chunk_len": 151.31395346542362, "max_chunk_len": 1428, "min_chunk_len": 321, "total_chars": 77125, "compression_ratio": 0.8517990275526742, "chunks": ["Asymptotics of constrained M -estimation under convexity arXiv:2511.04612v1 [math.ST] 6 Nov 2025 Victor-Emmanuel Brunel Abstract: M -estimation, aka empirical risk minimization, is at the heart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the loss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can be too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an asymptotic theory for M -estimation with convex loss (which needs not be differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an interplay between the loss function and the boundary structure of the set of constraints. We extend our results to U -estimators, building on the asymptotic theory of U -statistics. Applications of our work include, among other, robust location/scatter estimation, estimation of deepest points relative to depth functions such as Oja s depth, etc. Key words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives.. 1. INTRODUCTION 1.1 Preliminaries We consider a", "Key words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives.. 1. INTRODUCTION 1.1 Preliminaries We consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables taking values in some measurable space (E, E) and we denote by P their distribution. Let Θ0 Rd be a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer representing the parameter dimension. Let ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 . Set Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is to estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and 1 n θ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while n i=1 Φn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is", "population risk evaluated at θ, while n i=1 Φn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random function Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn . When minimization is performed over the whole parameter space Θ0 , we talk about unconstrained M -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk about constrained M -estimation with Θ as the set of constraints. In this work, we are concerned with the latter. CREST-ENSAE, victor.emmanuel.brunel@ensae.fr 1 2 V.-E. BRUNEL Let Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a minimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below). Standard asymptotic theory questions (weak or strong) consistency and aims at determining the asymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge (in probability or almost surely) to zero", "aims at determining the asymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge (in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n to the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in distribution for some rescaling factor ρn and if so, what is the asymptotic distribution? n It may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable θ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term. For simplicity, here, we only study the properties of exact empirical risk minimizers. Our main working assumption is that the loss function is convex in its second argument. That is, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples include: 1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex", "is convex on Θ0 for P -almost all x E. Relevant examples include: 1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R. For instance, if is the squared Euclidean norm, we recover mean estimation. If is the Euclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where α (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover geometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles of P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0 is a given, tuning parameter. 2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric, positive semi-definite matrices. There are several ways of averaging positive definite matrices, beyond simply taking their arithmetic mean (i.e., their standard linear average). A simple", "space of d d symmetric, positive semi-definite matrices. There are several ways of averaging positive definite matrices, beyond simply taking their arithmetic mean (i.e., their standard linear average). A simple example is that of the harmonic mean, which is simply the inverse of the linear average of the inverses (if the matrices are positive definite). More involved ways include (again for positive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to their geometric mean [7]. In the context of optimal transport, a large body of literature has been interested in the Bures-Wasserstein mean of positive definite matrices, which is related to Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown in [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization problem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive (semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss, and our results also allows to consider the constrained case, as well as robust alternatives to Bures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]). 3. Linear regression (here, data are rather denoted as", "allows to consider the constrained case, as well as robust alternatives to Bures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]). 3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R, Θ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume to be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median regression, etc. In all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest example is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In that case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ and Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this", "n ) are the unique minimizers of Φ and Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this example can be studied with elementary tools, but it is worth keeping it in mind as an illustration of our results, in order to fix ideas. ASYMPTOTICS OF CONVEX M -ESTIMATION 3 Typically, proving consistency and finding the asymptotic distribution of M -estimators require some tools from the theory of empirical processes and imposes some smoothness of the loss function ϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect to its second argument, are locally dominated, allowing the use of dominated convergence to swap derivatives and expectations in the analysis. In our context, the full power of convexity comes in through fairly elementary convex analysis and allows to completely avoid such common technical assumptions. 1.2 Related works M -estimation is a quintessential problem in statistical inference (maximum likelihood estimation being a particular instance in general) and, as a particular case, constrained M -estimation. Asymptotic theory of statistical estimation has been overlooked in the era of high-dimensional data and models. Yet, it", "particular instance in general) and, as a particular case, constrained M -estimation. Asymptotic theory of statistical estimation has been overlooked in the era of high-dimensional data and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant when the data set contains a lot of samples and their dimension is not too large. Asymptotic theory of M -estimators is well understood when the loss function is smooth and satisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is a regular closed set and the population minimizer is a local minimum of the population risk in the ambient space. See also [34] for inference on constrained statistical problems and [26,47] for special cases. Recently, [35] drew connections between the statistical error of constrained M -estimation and the statistical dimension of the constrained set, building on [11, 46] in linear regression and Gaussian sequence models. Even though these connections belong to the non-asymptotic world, we also discuss such connections at infinitesimal scales in the remarks following Theorem 7 below. When the loss function is", "models. Even though these connections belong to the non-asymptotic world, we also discuss such connections at infinitesimal scales in the remarks following Theorem 7 below. When the loss function is convex, [19] proved asymptotic normality, only requiring the population risk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive definite Hessian at that point - convexity allowing to avoid any local domination assumption. [40] proved further asymptotic expansions of the statistical error under stronger smoothness assumptions of convex the loss. Asymptotics of penalized M -estimators have also been established [24], in particular for penalized regression (such as Lasso) [27]. In the context of high dimensional linear regression and classification, some recent work has also tackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing dimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to this line of work are the high-dimensional central limit theorems of [12, 15] which correspond to the squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar high-dimensional central limit theorems have not been tackled for general M -estimators. This work is not concerned", "in the context of M -estimation. To the best of our knowledge, similar high-dimensional central limit theorems have not been tackled for general M -estimators. This work is not concerned with penalized M -estimation. Indeed, even though penalized and constrained optimization problems are related through Lagrangian functions, in penalized statistical problems, it is standard to let the penalty depend on the sample size in order to enforce some regularization and achieve optimal performance, although here, we only consider fixed constraint sets, independently of the sample size. 1.3 Outline In Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some results about convex functions and sequences of convex functions, which we chose to highlight in the first part of this work because they are essential to build the intuition behind the theory. In Section 2.2, which is much more theoretical and could be skipped at first, we deal with the 4 V.-E. BRUNEL existence of a measurable empirical minimizer, based on results that guarantee the existence of measurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals with asymptotic distributions of M -estimators. We propose an extension to", "guarantee the existence of measurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals with asymptotic distributions of M -estimators. We propose an extension to U -estimators with convex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric projections, which are only used for some technical parts of the main proofs, but not essential to build the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional differentiability of metric projections onto convex sets, may be of independent interest to the reader. 1.4 Notation and standard definitions/assumptions Here, we gather all the notation that we use in this work, as well as several simple definitions. 1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables that we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with values in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a fixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 .", "we let P = X1 #P be their distribution. The set Θ0 is a fixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function ϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second, and to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred to as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ) (referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we simply write Φn (θ) and skip the dependence on ω . 2. The power set of a non-empty set A is denoted by P(A). 3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) int(G) its boundary. 4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for 1/2 x, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x", "matrix S Rd d yields a scalar product by setting, for 1/2 x, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all x Rd . The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by BS (x, r). 5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S is denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd , we denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S . S = {x C 6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t S S u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t . 7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm associated with S is", "simply write Cu = Cu,t . 7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm associated with S is denoted by dS (x, C) = miny C x y S . 8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is S S : For all u Rd , πC (u) is the unique minimizer of the map t C t u 2S . In denoted by πC S particular, dS (u, C) = u πC (u) S . d 9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is the set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone, not necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be symmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all t Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex", "to G at x0 with respect to S is the set of all t Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no mention of a matrix S, it is implicitly assumed to be the identity matrix. 10. The support function of a non-empty convex set C Rd is the map hC Rd R { } defined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a hyperplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear function (that is, positively homogeneous and convex). If C is bounded, then hC only takes finite values. See, e.g., [49, Section 1.7.1]. ASYMPTOTICS OF CONVEX M -ESTIMATION 5 11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S and simply write, for instance, x , B(x, r), u , Cu , πC , etc. 12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f on C is denoted by Argminy C", "πC , etc. 12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f on C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When this set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case a , the unique element of that set. 13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us, in practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say that f has a directional derivative at x in the direction t Rd if and only if the quantity ε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by d+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous at x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken", "has directional derivatives at x Rd , then it must be continuous at x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken along the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on all compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense. This is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and for all seuqences (εn )n 1 of positive numbers converging to 0, ε 1 n (f (x + εn tn ) f (x)) has a (finite) limit as n (see, e.g., [17, Chapter III]). 14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) = f (x) t for all t Rd . 15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean that it takes finite values only, i.e., we only consider convex functions f G0 R, which may be", "we talk about a convex function on G0 , we always mean that it takes finite values only, i.e., we only consider convex functions f G0 R, which may be the restriction to G of some lower-semicontinuous convex function f Rd R { } whose domain contains G0 . 16. We call random convex function any map f G R, where G Rd is some convex set, such that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only assume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more generality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t) instead of f (ω, t) for simplicity. 17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) = E[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd , u var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote by varS (X) = Svar(X)S = var(SX) so that for all vectors u,", "u var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote by varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity u varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S. 18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by Nd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V . 2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS 2.1 On the behavior of convex functions and sequences of convex functions First, we state a minimum principle for convex functions, which we will use a few times in the next sections. Lemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let f G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 ) for some t0 K G, then Argmin f (t) K and it", "and K G0 be any compact, convex set. If mint K G f (t) > f (t0 ) for some t0 K G, then Argmin f (t) K and it is not empty. t G Remark 1. Recall that a convex function defined on an open convex set is automatically 6 V.-E. BRUNEL continuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any compact set. The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says that if f has one value inside K that is smaller than all values taken on K, then, it has at least one minimizer, and they all lie in K. We need this slightly more technical statement in order to deal with constrained M -estimation later. Proof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ [0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in K G so f (t0 ) min K G f , which would contradict the assumption). Hence,", "convex function. First, note that t0 K (or else, t0 would be in K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some λ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G. Therefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing on [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0). That is, f (t) > f (t0 ). Therefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at least one such minimizer since f is continuous on the compact set K G. In the main statistical results presented in the next sections, Lemma 1 will be used to localize empirical minimizers of Φn . The second key result is due to Rockafellar and shows that, for sequences of convex functions, uniform convergence can be deduced from pointwise convergence on a dense subset. From this lemma, we will derive two probabilistic corollaries. Lemma 2. [48, Theorem 10.8] Let", "of convex functions, uniform convergence can be deduced from pointwise convergence on a dense subset. From this lemma, we will derive two probabilistic corollaries. Lemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex functions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t). Then, fn converges uniformly to f on all compact subsets of G0 . An important consequence that we will use extensively is the following corollary. Corollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for n all compact sets K G0 , supK fn f 0 almost surely (resp. in probability). n Proof. Let us prove the statement for the almost sure convergence and the convergence in probability separately. Almost sure convergence. Let C be a dense and countable subset of G0 . By assumption, for each t C, it holds with probability one that fn", "in probability separately. Almost sure convergence. Let C be a dense and countable subset of G0 . By assumption, for each t C, it holds with probability one that fn (t) f (t). Since C is countable, this implies that with probability 1, n fn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges n uniformly to f on all compact subsets of G0 . Convergence in probability. Again, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our goal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient n to show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost surely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the terms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost surely. Denote by t1 , t2 , . . . the elements of C. ASYMPTOTICS OF CONVEX M -ESTIMATION 7 By assumption, fn (t1 ) f (t1", "to 0 almost surely. Denote by t1 , t2 , . . . the elements of C. ASYMPTOTICS OF CONVEX M -ESTIMATION 7 By assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost n surely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost n surely. Similarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 ) and thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By induction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all integers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all n 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove n the lemma. First, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1. Second, for", "which will prove n the lemma. First, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1. Second, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the first p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest n follows from the first part of the proof (the case of almost sure convergence). In fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to the appendix (Section E), because we only use it to formulate an open question, see the end of Section 4.2). 2.2 On the existence of measurable minimizers and measurable subgradients The existence of minimizers of a random convex function can often be established quite easily (for instance, if the function is coercive). Same for subgradients since any convex function defined on an open convex set has at least one subgradient at any point of that set. However, the existence of a measurable minimizer or subgradient is much less trivial and relies on", "open convex set has at least one subgradient at any point of that set. However, the existence of a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable selections. 2.2.1 Measurable selections Definition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω to some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd such that for all ω , γ(ω) Γ(ω). There are numerous theorems that guarantee the existence of measurable selections in various setups, see [21,38]. The one that we will need is the following, that follows from combining Theorems 3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd . Lemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the set {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a measurable selection. A multifunction satisfying this property above is called C-measurable (C as in compact , the test sets K used in Lemma 3 being compact). 2.2.2 Measurable empirical risk minimizers", "selection. A multifunction satisfying this property above is called C-measurable (C as in compact , the test sets K used in Lemma 3 being compact). 2.2.2 Measurable empirical risk minimizers From Lemma 3, we obtain the following result, which will guarantee the existence of a measurable empirical risk minimizer for large enough n, and which will, at the same time, yield its strong consistency. Theorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd such that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume n 8 V.-E. BRUNEL that G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of random variables with values in G such that with probability 1, tn is a minimizer of fn on G for all large enough n. Moreover, d(tn , G ) 0 almost surely. n Proof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First, we prove that with probability 1, Mn is non-empty for all large enough n. Second,", "let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First, we prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the measurable selection to obtain such a sequence (tn )n 1 . Step 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous, on the open set G0 . First, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost surely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is compact, so (1) sup fn (t) f (t) 0. n t G ε G Let f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can be written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f on the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0. Then, the following holds with probability 1: For all sufficiently large", "the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0. Then, the following holds with probability 1: For all sufficiently large integers n and for all t G ε G, fn (t) f (t) η/3 f + η η/3 by (1) by definition of η fn (t ) η/3 + η η/3 again by (1) = fn (t ) + η/3 > fn (t ). Therefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1, (2) Mn G ε . Mn if Mn Step 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = {t0 } otherwise. Let us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by continuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that for each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence of a measurable selection. Fix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω)", "order to guarantee the existence of a measurable selection. Fix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K } is a measurable set. First, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}. Since fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω inf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite inf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1 and G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t) and inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω Mn (ω) K } F. 1 recall that above, we only wrote fn (t) instead of fn (ω, t) for", "from to R { }) and we obtain that {ω Mn (ω) K } F. 1 recall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity. 9 ASYMPTOTICS OF CONVEX M -ESTIMATION Now, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to the set {ω Mn (ω) = }, which can be decomposed as {ω Mn (ω) = } = {ω p N q p+1 min t G B(t0 ,q) fn (ω, t) < min t G B(t0 ,q) fn (ω, t)} which, therefore, is also measurable. Finally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for all n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1, tn Mn for all large enough n. Step 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with probability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely. n 2.2.3 Measurable subgradients Now, we apply Lemma 3", "1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely. n 2.2.3 Measurable subgradients Now, we apply Lemma 3 to show the existence of measurable subgradients for random convex functions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f at a point t0 G0 is any vector u Rd such that f (t) f (t0 ) + u (t t0 ), t G0 . We denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at every point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient (i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem. Theorem 2. Let f be a random convex function defined on a convex set G0 Rd and let t0 int(G0 ). Then, f has a measurable subgradient at t0 . Proof. Let Γ = f", "a random convex function defined on a convex set G0 Rd and let t0 int(G0 ). Then, f has a measurable subgradient at t0 . Proof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) = (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it always takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it is C-measurable in order to apply Lemma 3. Let K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there exists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any small enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous on int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then, Γ K if and only if there exists u K for which supt C (u (t t0 ) f (t) +", "fixed dense, countable subset of B(t0 , ε). Then, Γ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let h(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize the dependence on ω for clarity, even though it was omitted above). First, note that for all u Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions. Second, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it only takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε). Hence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if minu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of K. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable, u K since", "0, where K is a fixed, countable, dense subset of K. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable, u K since inf u K h( , u) is a measurable map. 10 V.-E. BRUNEL Finally, let us state an incredibly simple yet powerful result that shows that for convex functions, there is no need to apply any dominated convergence theorem in order to swap expectations and (sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set G0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that this fact still holds for generalized sums of convex functions. Theorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all t int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume that for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and for", "real number and assume that for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and for all t G0 , g(t) Lp (P) and E[g(t)] F (t). Proof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of which is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient to check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is integrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an ε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 )) and g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is, g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))). Since the right hand side is in Lp (P) by assumption, so", "v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))). Since the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so we conclude that g(t0 ) Lp (P). Now, for the rest of the proof, simply note that, again, by definition of subgradients, f (t) f (t0 ) + g(t0 ) (t t0 ) holds for all t G0 . Taking the expectation, which is linear, yields that F (t) F (t0 ) + E[g(t0 )] (t t0 ) which concludes the proof. Remark 2. In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P) for all t B(t0 , ε), for any arbitrary, small enough ε > 0. As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not depend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 ) (since F (t0 ) is the only subgradient of F at t0 , in that case). In fact, Lemma 12 shows that if", "is automatically equal to F (t0 ) (since F (t0 ) is the only subgradient of F at t0 , in that case). In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely differentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 ) almost surely. To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all subgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )? 2 The other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial argument. ASYMPTOTICS OF CONVEX M -ESTIMATION 11 3. CONSISTENCY Consistency of empirical risk minimizers with a convex loss function is automatically granted in a strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in an arbitrarily small neighborhood of the set of population minimizers with probability 1. In what follows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all large enough n, θ n is", "with probability 1. In what follows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all large enough n, θ n is a minimizer of Φn on Θ. Existence of such a sequence is granted by Theorem 1. Theorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost n surely, as n . The proof of this theorem can be found in [19] (the only difference here being that we do not assume that Θ = Rd ), and it is a direct consequence of Theorem 1 above. Remark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true, that is, there can be elements of Θ that may never be approached by any empirical minimizer. For instance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard normal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so with", "X1 has the standard normal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so with probability 1, the empirical minimizer is unique, given by θ n = X n / X n . 4. ASYMPTOTIC DISTRIBUTION In this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ). 4.1 Non-differentiable case We first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That is, Φ(θ ) may not be not a singleton. The following useful property is fundamental in that case. Recall that for a non-empty convex subset C Rd , we denote by hC Rd R { } its support function. Proposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd , ρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t) n in probability. Proof. Fix θ Θ0 . For all t Rd , define 1 n t g(Xi", ", ρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t) n in probability. Proof. Fix θ Θ0 . For all t Rd , define 1 n t g(Xi , θ)) nρn i=1 1 ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) . ρn Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) Write Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)), for all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for all i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square and the expectation in the last display, 2 E[Zi,n ] 1 E[Yn2 ] n2 12 V.-E. BRUNEL where Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that 2 the sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n ] n12 E[Y12 ] and,", ") g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that 2 the sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n ] n12 E[Y12 ] and, by independence of X1 , X2 , . . ., n n n E[Y12 ] 2 var ( Zi,n ) = var(Zi,n ) E[Zi,n ] 0. n n i=1 i=1 i=1 We conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as n Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ)) (3) 1 n t ( g(Xi , θ) E[g(X1 , θ)]) n i=1 (4) ρn (Φ(θ + t/ρn ) Φ(θ)) . The law of large numbers yields that the term (3) converges to 0 in probability, and the term in (4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9. As a consequence, we obtain the following theorem. Theorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ with probability going to 1 as n . Note that the assumption that 0 int( Φ(θ )) readily implies that θ", "0 int( Φ(θ )). Then, θ n = θ with probability going to 1 as n . Note that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique minimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ . Proof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as n . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by Tn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all t Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for all large enough n, with probability 1. Now, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get sup Gn (t) h Φ(θ ) (t) 0 n t B(0,ε) in probability (note that B(0, ε) ρn (Θ0 θ ) for all", "Corollary 1 and Lemma 9, we get sup Gn (t) h Φ(θ ) (t) 0 n t B(0,ε) in probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 int( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive. Assume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least 1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε, Gn (t) h Φ(θ ) (t) εη/2 = εh Φ(θ ) (t/ε) εη/2 εη εη/2 by positive homogeneity of h Φ(θ ) by definition of η > εη/2 > 0 = Gn (0) yielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that for all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is, ρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence n (ρn )n 1 diverging to as n , Lemma 25 implies the desired statement. ASYMPTOTICS OF CONVEX", "0 in probability. Since this must hold for any positive, non-decreasing sequence n (ρn )n 1 diverging to as n , Lemma 25 implies the desired statement. ASYMPTOTICS OF CONVEX M -ESTIMATION 13 Let C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields that C h 1 Φ(θ ) ([0, )). The next result extends Theorem 5. Theorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}. Then, with probability going to 1 as n , θ n = θ . The assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0} have a trivial intersection. Note that, by the first order condition at θ , this intersection must always be included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ. Proof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ", "directions of strict, linear increase of the population risk Φ. Proof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) ε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t C t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all t C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0. Finally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )). Now, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as n and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof n of Theorem 5. Let n be", "(t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof n of Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability at least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ ) with h Φ(θ ) (t) = ε, that Gn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0) so, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain, for all large enough n, that with probability at least 1 ε, 0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε where the first inequality follows from the first order condition for Φ at θ (Lemma 10). That is ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that n h Φ(θ ) (θ n θ ) = 0 with probability", ") (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that n h Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that θ n θ = 0 with probability going to 1 as n , which is the desired statement. Remark 4. Results of this section rely on Proposition 1, which imposes square-integrability of the loss function. We do not know whether the same results could be proved under weaker assumptions. Now, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this could be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we will assume the existence of second order derivatives for Φ at θ . This is the object of the next section. 4.2 Differentiable case Let us first state the main result of this section. Theorem 7. following: Let g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the 14 V.-E. BRUNEL (i) Φ is twice differentiable at θ and S =", "7. following: Let g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the 14 V.-E. BRUNEL (i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite; (ii) g( , θ ) L2 (P ); S 1 (iii) πΘ θ Φ(θ ). has directional derivatives at S Then, S 1 n(θ n θ ) d+ πΘ θ Φ(θ ); Z) ( S n 1 1 in distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )). Remark 5 (on the assumptions of the theorem). (i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are twice differentiable almost eveywhere in the interior of their domains [1]. The assumption that 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This assumption could be relaxed, yielding slower rates under further, technical assumptions on higher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it only requires minimal, easy to check, non-restrictive smoothness assumptions. (ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first", "on the n 1/2 rate because it only requires minimal, easy to check, non-restrictive smoothness assumptions. (ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ implies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable at θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures that it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In fact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume that ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does not require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of θ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ). S S (iii-a) Directional differentiability of πΘ θ is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by Rademacher s theorem [16, Section", "πΘ θ is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by Rademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present S for a several sufficient conditions that guarantee the existence of directional derivatives of πK convex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.). By an obvious linear change of variables, it is clear that the existence of a directional derivative S 1 of πΘ θ Φ(θ ) in a direction z Rd is equivalent to the existence of a directional at S derivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields that S 1 d+ πΘ θ Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z). ( S Recall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order", ") ( S 1/2 Φ(θ ); S 1/2 z). ( S Recall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition at θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently, S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0. Remark 6 (on the conclusion of the theorem). 1 S Lemma 20 yields that for any z Rd , d+ πΘ θ Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where ( S C is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is S 1 in the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ Φ(θ ); ) = 0 so ( S Theorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak) n consequence of Theorem 6. If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and, S d+", "This was already a (rather weak) n consequence of Theorem 6. If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and, S d+ πΘ θ n(θ n θ ) Z (0; ) is simply the identity map. Therefore, Theorem 7 says that n ASYMPTOTICS OF CONVEX M -ESTIMATION 15 in distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough n, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and we recover Haberman s theorem [19, Theorem 6.1]. In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0 is a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the open set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ on the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g., take Θ = B(θ , ε)", "unique minimizer of Φ on the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g., take Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields that θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n n eventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion S d of Theorem 7, with d+ πΘ θ (0; ) being the identity map (note that in the case Θ = Θ0 = R , we necessarily have that Φ(θ ) = 0). If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented as the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that n(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear hyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ . Lemmas 23 and 24 imply that for all t, t 0 with t >", "the linear hyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ . Lemmas 23 and 24 imply that for all t, t 0 with t > t, (5) 1 1 S S Φ(θ ); Z) S Φ(θ ); Z) S d+ πΘ θ d+ πΘ θ ( tS ( t S almost surely. This can be interpreted as follows. First, note that the set Θ can represent some constraints that are imposed by a specific application, or it can represent a model (e.g., if it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is misspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0. In other words, the vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount of model misspecification. In that regard, (5) suggests that more misspecification yields better asymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be thought of as corresponding to the well-specified case. This will be illustrated in the examples below. As a consequence of Theorem 7, the", "In (5), t = 0 can be thought of as corresponding to the well-specified case. This will be illustrated in the examples below. As a consequence of Theorem 7, the mean squared error of θ n satisfies (6) 1 S Φ(θ ); Z) 2S ] lim inf nE[ θ n θ 2S ] E[ d+ πΘ θ ( S n (we do not know, in general, whether this is in fact an equality, with the lim inf being a simple limit, see the open question below). The right hand side can be interpreted as a local measure of the statistical complexity of Θ around θ , relative to the (population) loss function Φ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd is measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account for a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a projection, but the directional derivative of a projection. The right hand side of (6) can rather be seen as a statistical dimension at an infinitesimal scale. We can", "have a projection, but the directional derivative of a projection. The right hand side of (6) can rather be seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11] who studied least squares under convex constraint, and proved that the statistical dimension at a fixed scale drives the statistical error. A similar phenomenon has also been studied for constrained M -estimators in a more general setup [35]. Recall, however, that except in specific S 1 cases (see Section C in the appendix), d+ πΘ θ Φ(θ ); ) is not the projection onto a ( S convex set. S 1 It is worth mentioning some further important properties of Π = d+ πΘ θ Φ(θ ); ). ( S As we have noted above, in general, it is not the projection onto a convex cone. Nevertheless, 16 V.-E. BRUNEL it shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it satisfies the following properties: Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity); Π(z ) Π(z) S z z 2S (non-expansiveness); Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z", "λ 0 and z Rd (positive homogeneity); Π(z ) Π(z) S z z 2S (non-expansiveness); Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity). Note that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last two properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] implies that Π is the gradient of a convex function. Now, let us look at some applications of Theorem 7. Example 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two moments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) = (1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n ) where X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4, also follows directly from the strong law of large numbers, together with continuity of πΘ (since it is non-expansive). For asymptotic normality, we obtain, from Theorem 7, that n(θ", "4, also follows directly from the strong law of large numbers, together with continuity of πΘ (since it is non-expansive). For asymptotic normality, we obtain, from Theorem 7, that n(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z) n in distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result can also be obtained using the central limit theorem, combined with the delta method4 . Here, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ is a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular, θ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n . Example 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R. Assume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is independent of X1 and has the centered Gaussian distribution with variance", "has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is independent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd and σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd , 1 Φ(θ) = θ θ0 2S + σ 2 . 2 Let Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) = S {πΘ (θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n , defined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies S + S n(θ n θ ) d+ πΘ θ (θ0 θ ; Z) = d πΘ (θ0 ; Z) n in distribution, where Z Nd (0, S 1 BS 1 ) and B = var((Y1 X1 θ )X1 ) = var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 ) = E[(X1 (θ0 θ ))2 X1", "S 1 BS 1 ) and B = var((Y1 X1 θ )X1 ) = var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 ) = E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S. 3 In fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd Delta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by the existence of directional derivatives together with non-expansiveness of πΘ θ 4 17 ASYMPTOTICS OF CONVEX M -ESTIMATION Example 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 . Consider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is any empirical geometric median. Here, in the unconstrained case, we recover standard results for geometric median M -estimation, provided that the distribution of X1 is not supported on an affine line (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is twice differentiable at θ with positive definite Hessian),", "on an affine line (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is twice differentiable at θ with positive definite Hessian), see, e.g., [28]. Proof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive definite matrix, by assumption. First, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose value will be determined later, and let n 1 be any integer that is large enough so R/ n r. For all such integers n, let Fn be the random function defined on B(0, R) by t n 1 Fn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t) 2 n i=1 for all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges pointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability. From this, we will then obtain that any minimizer of the first term (one of which is", "1, uniformly on the compact set BS (0, R)) to zero in probability. From this, we will then obtain that any minimizer of the first term (one of which is given by n(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second, quadratic term. Fix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ). By definition of subgradients, 0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )). Squaring and taking the expectation yields that 2 2 ] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ] E[Zi,n (7) (we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ + n 1/2 t) g(X 1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and v = θ + t/ n + 1, Yn Yn+1 = t", ")). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and v = θ + t/ n + 1, Yn Yn+1 = t (g(X1 , u) g(X1 , v)) = (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v)) 0 by Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to some non-negative random variable Y . By monotone convergence (noting that Y1 is integrable), this implies that E[Yn ] E[Y ]. (8) n However, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7 yielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0 n 5 n Similarly to the first example, one need not assume the existence of one moment if the loss function is replaced with ϕ(x, θ) = x θ x , x, θ Rd . 18 V.-E. BRUNEL and, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence (noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y", ". 18 V.-E. BRUNEL and, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence (noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0. n Combined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that (9) n n n i=1 i=1 i=1 2 var ( Zi,n ) = var(Zi,n ) E[Zi,n ] E[Yn2 ] 0. n Therefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is, n n n(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0 n i=1 in probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain that Fn (t) 0 (10) n in probability, for all t BS (0, R), as desired. For all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd θ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn", "n1/2 (Θ θ ) T and Sn = {t Rd θ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and Θ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number, whose value is still to be determined). Define the maps G n t Sn n(Φn (θ + n 1/2 t) Φn (θ )) and n 1 Gn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t. 2 i=1 As per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that (11) sup t BS (0,R) G n (t) Gn (t) 0 n in probability. Moreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk minimizer θ n . Now, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as n 1 Gn (t) = n 1/2 t g(Xi , θ ) + t", "g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as n 1 Gn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t 2 i=1 n 1 = n 1/2 S 1 g(Xi , θ ), t S + t 2S 2 i=1 1 = Zn + nS 1 Φ(θ ), t S + t 2S 2 1 1 2 = t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S . 2 It is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given by t n = πTSn ( Zn 1 nS Φ(θ )). 19 ASYMPTOTICS OF CONVEX M -ESTIMATION Now, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges in distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is, t n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence in distribution of t n . Asymptotic behavior of t n .", "n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence in distribution of t n . Asymptotic behavior of t n . First, by the central limit theorem, we have that Zn Z in distribution, where Z is is a n centered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 . By Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume S that Zn converges almost surely to Z. Since πC is non-expansive by Lemma 13, it holds that S 1 tn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover, 1 S πTSn ( Z nS 1 Φ(θ )) = π n(Θ θ ) ( Z nS Φ(θ )) S 1/2 = nπΘ θ Z S 1 Φ(θ )) ( n S 1 d+ πΘ θ Φ(θ ); Z) ( S n almost surely, using the third assumption of the theorem. Therefore, we conclude that t n n 1 S Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows, d+ πΘ θ ( S since Z and Z are identically distributed. Convergence in probability", "1 S Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows, d+ πΘ θ ( S since Z and Z are identically distributed. Convergence in probability of t n t n to 0. Fix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it is tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let K = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1, supt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all large enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with t t n S = ε, G n (t) Gn (t) η ε2 η 2 ε2 G n (t n ) η + η. 2 Gn (t n ) + Hence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least 1 2ε,", "η + η. 2 Gn (t n ) + Hence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least 1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all large enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in probability to 0. S 1 Conclusion. We have proved that t n converges in distribution to d+ πΘ θ Φ(θ ); Z) for ( S some Gaussian random variable Z and that t n tn converges to zero in probability, as n . Hence, Slutsky s theorem implies the desired result. In the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2 sense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2 sense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving n this convergence would yield an exact asymptotic quantification of the", "not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving n this convergence would yield an exact asymptotic quantification of the mean squared error of θ n , since, it would yield that S 1 nE[ θ n θ 2 ] E[ d+ πΘ θ Φ(θ ); Z) 2 ] ( S n 20 V.-E. BRUNEL where Z is a Gaussian vector as in the theorem. We leave the following question open: Open question. Is it true that under the assumptions of Theorem 7, for all large enough n, θ n has two moments, and that S 1 nE[ θ n θ 2 ] E[ d+ πΘ θ Φ(θ ); Z) 2 ]? ( S n 5. EXTENSION: CONVEX U -ESTIMATION The previous theory can be easily extended to more general convex empirical risks, e.g., when Φn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer k and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its last. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k", "Θ0 R be symmetric and measurable in its first k arguments and convex in its last. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set Φ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k, Φn (θ) = 1 ϕ(Xi1 , . . . , Xik , θ). (nk) 1 i1 <...<ik n Estimators obtained by minimizing such empirical risks are called U -estimators. Some relevant examples include: 1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ) be the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd . The minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ) is the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency and asymptotic normality of Oja", ". . , xd , θ) is the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency and asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our asymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other definitions of medians that are U -estimators associated with depth functions. 2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator defined as a U -estimator obtained by computing an empirical median of all empirical averages k of the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x θ , for k all x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that in [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks, such as geometric medians of means in multivariate settings [36] can be considered as well. Note that in [37], the order k", "size k, with overlaps, are considered. Other frameworks, such as geometric medians of means in multivariate settings [36] can be considered as well. Note that in [37], the order k of the U -process is allowed to grow with the sample size n - we do not consider this setup here and leave it for future work. 3. More generally, aggregation of estimators that are based on overlapping subsamples, e.g., random forests [9] or bagging [8], which have attracted lots of interest in modern machine learning. 4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ) where p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply twice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of hc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = ,", "a robust version of the empirical variance. If now p = 1 and (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy to a median absolute deviation (and intermediate robust versions if = hc for some c > 0). In higher dimensions, one recovers the empirical covariance matrix of X1 , . . . , Xn by setting 2 ϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust versions can be defined by taking the square root of the above, or applying Huber s loss hc for some c > 0. ASYMPTOTICS OF CONVEX M -ESTIMATION 21 5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g., for data driven procedures), see, e.g., [53]. Note that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as U -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central limit theorem for U -statistics. Theorem 8. Law of large numbers for U -statistics", "e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central limit theorem for U -statistics. Theorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a symmetric, measurable map satisfying h L1 (P k ). Then, 1 h(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )] n (nk) 1 i1 < <ik n almost surely. Theorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the 1 covariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n h(Xi1 , . . . , Xik ). (k ) 1 i1 < <ik n Then, n(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ) n in distribution. Theorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1, Theorems 5 and 6 require more care but also remain true in this context. Proofs are deferred", "remains true in the context of U -estimation with convex loss. Proposition 1, Theorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to Section D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k appears in the limit, accounting for the dependence of the terms in the new definition of Φn . Theorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the following: (i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive definite; (ii) g( , θ ) L2 (P k ); S 1 (iii) πΘ θ Φ(θ ). has directional derivatives at S Then, S 1 n(θ n θ ) k d+ πΘ θ Φ(θ ); Z) (S n 1 1 in distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]). Note the extra k factor in the limit in distribution. 6 Σ can also be written as E[h(X1 , X2 , . . .", "Xk , θ ) X1 ]). Note the extra k factor in the limit in distribution. 6 Σ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is, the covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 , . . . , Xk are such that X1 , X2 , . . . , Xk , X2 , . . . , Xk are iid. 22 V.-E. BRUNEL 6. CONCLUSION AND FUTURE DIRECTIONS We have established the asymptotic properties of constrained M -estimators with a convex loss and a convex set of constraints, under minimal assumptions. In this work, asymptotics are only relative to the sample size n, while the dimension d is kept fixed. In large dimensional problems, asymptotic theory can be approached from different angles. First, one may look at asymptotic distributions of low-dimensional projections of the M -estimator. For instance, in the context of", "dimensional problems, asymptotic theory can be approached from different angles. First, one may look at asymptotic distributions of low-dimensional projections of the M -estimator. For instance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates of penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle consists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and prove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability metric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When θ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high dimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do not exist for other M -estimators, even with convex loss. In the context of U -estimators, we have also let the order k of the U -process be fixed. However, it may be relevant to also let k grow with the sample size (e.g., for median-of-means", "we have also let the order k of the U -process be fixed. However, it may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures). While the asymptotics of U -statistics with increasing order have been studied only recently [14], we leave this direction for future work on U -estimation."], "num_sections": 1, "num_graph_nodes": 83, "num_graph_edges": 162}
{"method": "fixed", "num_chunks": 83, "avg_chunk_len": 791.1927710843373, "std_chunk_len": 76.99389082811189, "max_chunk_len": 800, "min_chunk_len": 94, "total_chars": 65669, "compression_ratio": 1.0003959250178929, "chunks": ["Asymptotics of constrained\nM -estimation under convexity\n\narXiv:2511.04612v1 [math.ST] 6 Nov 2025\n\nVictor-Emmanuel Brunel \n\nAbstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the correspondin", "g M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.\nKey words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..\n1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribu", "tion. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter s", "pace Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.\n\nCREST-ENSAE, victor.emmanuel.brunel@ensae.fr\n\n1\n\n2\n\nV.-E. BRUNEL\n\nLet Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of", "θ n\n\nto the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn \n\nIt may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared E", "uclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A si", "mple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed unde", "r the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\ntha", "t case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n3\n\nTypically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectation", "s in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.\n1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well", "understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in", "the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is,", "when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.\n1.3 Outlin", "e\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the\n\n4\n\nV.-E. BRUNEL\n\nexistence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojectio", "ns, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.\n1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of", "Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd .", "The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .\nS\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is th", "e unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhy", "perplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5\n\n11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with v", "alues in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqen", "ces (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e., we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does no", "t bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with", "mean u and covariance matrix V .\n2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G\n\nRemark 1.\n\n Recall that a convex function defined on an open convex set is automatically\n\n6\n\nV.-E. BRUNEL\n\ncontinuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version,", "when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1)", "> ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all co", "mpact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn \n\nall compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn \n\nProof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn \n\nfn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nunif", "ormly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn \n\nto show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n7\n\nBy assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn \n\nsurely. That is, there is an increasing map ψ1 N N such that", "fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Henc", "e, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn \n\nfollows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).\n2.2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the th", "eory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is calle", "d C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn \n\n8\n\nV.-E. BRUNEL\n\nthat G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn \n\nProof", ". For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)\n\nsup fn (t) f (t) 0.\nn \n\nt G ε G\n\nLet f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open se", "t G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3\n\n f + η η/3\n\nby (1)\nby definition of η\n\n fn (t ) η/3 + η η/3\n\nagain by (1)\n\n= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)\n\n Mn G ε .\n\n Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = \n\n {t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a me", "asurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1\n\nrecall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.\n\n9\n\nASYMPTOTICS OF C", "ONVEX M -ESTIMATION\n\nNow, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } = \n\n {ω \n\np N q p+1\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t) <\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t)}\n\nwhich, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn \n\n2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable sub", "gradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),\n\n t G0 .\n\nWe denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f", "at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for al", "l ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K \n\nsince inf u K h( , u) is a measurable map.\n\n10\n\nV.-E. BRUNEL\n\nFinally, let us state an incredibly simple yet pow", "erful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient", "of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (", "t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.\n\nRemark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of", "our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2\n\nThe other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n11\n\n3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a s", "equence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the em", "pirical minimizer is unique, given by θ n = X n / X n .\n4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).\n4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that case. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn \n\nin probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn", "(Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn\n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) \n\nWrite Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n] \n\n1\nE[Yn2 ]\nn2\n\n12\n\nV.-E. BRUNEL\n\nwhere Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (", "t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn \n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)\n\n1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1\n\n(4)\n\n ρn (Φ(θ + t/ρn ) Φ(θ)) .\n\nThe law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is o", "pen, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn \n\nt B(0,ε)\n\nin probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n ε", "η εη/2\n\nby positive homogeneity of h Φ(θ )\n\nby definition of η\n\n> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn \n\n(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n13\n\nLet C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theo", "rem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, s", "ince it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn \n\nof Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(", "θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn \n\nh Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivat", "ives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:\n\nLet g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the\n\n14\n\nV.-E. BRUNEL\n\n(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is ma", "de in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P )", "for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear con", "vex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . Whe", "n Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S\n\nTheorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,\n\nS\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn \n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n15\n\nin distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.", "1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the bou", "ndary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that\n\nn(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)\n\n 1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S\n\nalmost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not", "in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrated in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)\n\n 1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn \n\n(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around", "θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS", "1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,\n\n16\n\nV.-E. BRUNEL\n\nit shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] impl", "ies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.\nExample 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that\n\nn(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn \n\nin distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this sim", "ple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .", "2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies\n\nS\n\n+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn \n\nin distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )\n\n= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.\n\n3\n\nIn fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4\n\n17\n\nASYMPTOTICS OF CON", "VEX M -ESTIMATION\n\nExample 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose\n\nvalue", "will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by\n\nt n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by\n\nn(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/", "2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2\n\n2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n\n\n(7)\n\n(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +\n\nn 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and\n\nv = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))\n\n= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].\n\n(8)\n\nn \n\nHowever, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Y", "n ] 0. Together with (8), this shows that E[Y ] = 0\nn \n\n5\n\nn \n\nSimilarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .\n\n18\n\nV.-E. BRUNEL\n\nand, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn \n\nCombined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)\n\nn\n\nn\n\nn\n\ni=1\n\ni=1\n\ni=1\n\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn \n\nTherefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn \n\nn\n\nn(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn \n\ni=1\n\nin probability. Now, since we have assumed that Φ is twice d", "ifferentiable at θ , we finally obtain\nthat\nFn (t) 0\n\n(10)\n\nn \n\nin probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand\n\nn\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1\n\nAs per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)\n\nsup\nt BS (0,R)\n\n G n (t) Gn (t) 0\nn \n\nin probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote b", "y Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1\n\n1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1\n\n1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2\n\nIt is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby\n\nt n = πTSn ( Zn \n\n 1\nnS Φ(θ )).\n\n19\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by", "the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn \n\ncentered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that\n\nS\n 1\n\ntn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,\n\n 1\nS\n\nπTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\nalmost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn \n\n 1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in", "probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,\n\nG n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2\n\n Gn (t n ) +\n\nHence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlar", "ge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S\n\nsome Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn \n\nthis convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d", "+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn \n\n20\n\nV.-E. BRUNEL\n\nwhere Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn \n\n5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ", ") =\n\n1\nϕ(Xi1 , . . . , Xik , θ).\n\n(nk) 1 i1 <...<ik n\n\nEstimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -est", "imators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally", ", aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matr", "ix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n21\n\n5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 ,", ". . . , Xik ) E[h(X1 , . . . , Xk )]\n\nn \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).\n\n(k ) 1 i1 < <ik n\nThen,\n\nn(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn \n\nin distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the ter", "ms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6\n\nΣ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 ,", ". . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.\n\n22\n\nV.-E. BRUNEL\n\n6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of look", "ing at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order h", "ave been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "sliding", "num_chunks": 164, "avg_chunk_len": 797.689024390244, "std_chunk_len": 23.795769792629013, "max_chunk_len": 800, "min_chunk_len": 494, "total_chars": 130821, "compression_ratio": 0.5021747272991339, "chunks": ["Asymptotics of constrained\nM -estimation under convexity\n\narXiv:2511.04612v1 [math.ST] 6 Nov 2025\n\nVictor-Emmanuel Brunel \n\nAbstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the correspondin", "dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.\nKey words and phrases:", "g M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.\nKey words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..\n1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribu", "Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..\n1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many sampl", "tion. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter s", "es from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.\n\nCREST-ENSAE, victor.emmanuel.brunel@ensae.fr\n\n1\n\n2\n\nV.-E. BRUNEL\n\nLet Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, le", "pace Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.\n\nCREST-ENSAE, victor.emmanuel.brunel@ensae.fr\n\n1\n\n2\n\nV.-E. BRUNEL\n\nLet Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of", "t θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n\n\nto the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn \n\nIt may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor", "θ n\n\nto the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn \n\nIt may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared E", "simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss", "uclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A si", "(x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the", "mple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed unde", "Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume", "r the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\ntha", "to be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n3\n\nTypically, proving cons", "t case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n3\n\nTypically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectation", "istency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.\n1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic", "s in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.\n1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well", "theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constra", "understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in", "ined statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under", "the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is,", "stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with pe", "when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.\n1.3 Outlin", "nalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.\n1.3 Outline\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the\n\n4\n\nV.-E. BRU", "e\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the\n\n4\n\nV.-E. BRUNEL\n\nexistence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojectio", "NEL\n\nexistence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.\n1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this", "ns, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.\n1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of", "work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply writ", "Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd .", "e Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd . The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .\nS\n= {x C \n6. For a set C Rd , a", "The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .\nS\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is th", "vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The", "e unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhy", "normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5\n\n11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance,", "perplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5\n\n11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with v", "x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuo", "alues in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqen", "us\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e.,", "ces (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e., we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does no", "we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we", "t bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with", "denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V .\n2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be", "mean u and covariance matrix V .\n2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G\n\nRemark 1.\n\n Recall that a convex function defined on an open convex set is automatically\n\n6\n\nV.-E. BRUNEL\n\ncontinuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version,", "any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G\n\nRemark 1.\n\n Recall that a convex function defined on an open convex set is automatically\n\n6\n\nV.-E. BRUNEL\n\ncontinuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex f", "when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1)", "unction. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences", "> ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all co", "of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all compact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn \n\nall compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn \n\nProof. Let us prove the st", "mpact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn \n\nall compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn \n\nProof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn \n\nfn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nunif", "atement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn \n\nfn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn \n\nto show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no lo", "ormly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn \n\nto show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n7\n\nBy assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn \n\nsurely. That is, there is an increasing map ψ1 N N such that", "ss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n7\n\nBy assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn \n\nsurely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) alm", "fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Henc", "ost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn \n\nfollows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).\n2.2 On the existence of measurable minimizers and m", "e, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn \n\nfollows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).\n2.2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the th", "easurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need", "eory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is calle", "is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined", "d C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn \n\n8\n\nV.-E. BRUNEL\n\nthat G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn \n\nProof", "on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn \n\n8\n\nV.-E. BRUNEL\n\nthat G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn \n\nProof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges unif", ". For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)\n\nsup fn (t) f (t) 0.\nn \n\nt G ε G\n\nLet f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open se", "ormly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)\n\nsup fn (t) f (t) 0.\nn \n\nt G ε G\n\nLet f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3\n\n f + η η/3\n\nby (1)\nby definition of η\n\n fn (t ) η/3 + η η/3\n\nagain by (1)\n\n= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)\n\n Mn G ε .", "t G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3\n\n f + η η/3\n\nby (1)\nby definition of η\n\n fn (t ) η/3 + η η/3\n\nagain by (1)\n\n= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)\n\n Mn G ε .\n\n Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = \n\n {t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a me", "Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = \n\n {t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G f", "asurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1\n\nrecall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.\n\n9\n\nASYMPTOTICS OF C", "n (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1\n\nrecall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.\n\n9\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } = \n\n {ω \n\np N q p+1\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t) <\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t)}\n\nwhich, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn", "ONVEX M -ESTIMATION\n\nNow, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } = \n\n {ω \n\np N q p+1\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t) <\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t)}\n\nwhich, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn \n\n2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable sub", "Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn \n\n2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),\n\n t G0 .\n\nWe denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradi", "gradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),\n\n t G0 .\n\nWe denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f", "ents at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(", "at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for al", "t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuo", "l ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K \n\nsince inf u K h( , u) is a measurable map.\n\n10\n\nV.-E. BRUNEL\n\nFinally, let us state an incredibly simple yet pow", "us on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K \n\nsince inf u K h( , u) is a measurable map.\n\n10\n\nV.-E. BRUNEL\n\nFinally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex f", "erful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient", "unctions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradi", "of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (", "ents, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.\n\nRemark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] doe", "t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.\n\nRemark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of", "s not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2\n\nThe other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n11\n\n3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss fun", "our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2\n\nThe other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n11\n\n3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a s", "ction is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a sequence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to", "equence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the em", "the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n .\n4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).\n4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that cas", "pirical minimizer is unique, given by θ n = X n / X n .\n4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).\n4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that case. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn \n\nin probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn", "e. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn \n\nin probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn\n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) \n\nWrite Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square", "(Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn\n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) \n\nWrite Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n] \n\n1\nE[Yn2 ]\nn2\n\n12\n\nV.-E. BRUNEL\n\nwhere Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (", "and the expectation in the last display,\n2\nE[Zi,n\n] \n\n1\nE[Yn2 ]\nn2\n\n12\n\nV.-E. BRUNEL\n\nwhere Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn \n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)\n\n1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1\n\n(4)\n\n ρn (Φ(θ + t/ρn ) Φ(θ)) .\n\nThe law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheor", "t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn \n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)\n\n1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1\n\n(4)\n\n ρn (Φ(θ + t/ρn ) Φ(θ)) .\n\nThe law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is o", "em 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t", "pen, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn \n\nt B(0,ε)\n\nin probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n ε", ") 0\nn \n\nt B(0,ε)\n\nin probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n εη εη/2\n\nby positive homogeneity of h Φ(θ )\n\nby definition of η\n\n> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn \n\n(ρn )n 1 diverging to as n , Lemma 25", "η εη/2\n\nby positive homogeneity of h Φ(θ )\n\nby definition of η\n\n> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn \n\n(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n13\n\nLet C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theo", "implies the desired statement.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n13\n\nLet C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of t", "rem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, s", "he theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn \n\nof Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn", "ince it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn \n\nof Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(", "(t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn \n\nh Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section", "θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn \n\nh Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivat", "rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:\n\nLet g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the\n\n14\n\nV.-E. BRUNEL\n\n(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at", "ives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:\n\nLet g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the\n\n14\n\nV.-E. BRUNEL\n\n(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is ma", "S\nThen,\n\nS\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption", "de in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P )", "on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is a", "for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear con", "utomatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S", "vex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . Whe", "Recall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S\n\nTheorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,\n\nS\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map", "n Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S\n\nTheorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,\n\nS\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn \n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n15\n\nin distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.", ". Therefore, Theorem 7 says that\nn \n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n15\n\nin distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any", "1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the bou", "small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that\n\nn(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)", "ndary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that\n\nn(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)\n\n 1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S\n\nalmost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not", "1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S\n\nalmost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrate", "in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrated in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)\n\n 1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn \n\n(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around", "d in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)\n\n 1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn \n\n(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of", "θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS", "(6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,\n\n16\n\nV.-E. BRUNEL\n\nit shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21", "1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,\n\n16\n\nV.-E. BRUNEL\n\nit shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] impl", ", it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] implies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.\nExample 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1", "ies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.\nExample 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that\n\nn(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn \n\nin distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this sim", "(X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that\n\nn(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn \n\nin distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Co", "ple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .", "nstrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .\n2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies\n\nS\n\n+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn \n\nin distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1", "2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies\n\nS\n\n+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn \n\nin distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )\n\n= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.\n\n3\n\nIn fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4\n\n17\n\nASYMPTOTICS OF CON", "θ )X1 )\n\n= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.\n\n3\n\nIn fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4\n\n17\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nExample 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affin", "VEX M -ESTIMATION\n\nExample 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose\n\nvalue", "e\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose\n\nvalue will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by\n\nt n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compac", "will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by\n\nt n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by\n\nn(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/", "t set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by\n\nn(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2\n\n2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n\n\n(7)\n\n(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +\n\nn 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and\n\nv = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))\n\n= (1/", "2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2\n\n2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n\n\n(7)\n\n(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +\n\nn 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and\n\nv = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))\n\n= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].\n\n(8)\n\nn \n\nHowever, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Y", "n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].\n\n(8)\n\nn \n\nHowever, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0\nn \n\n5\n\nn \n\nSimilarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .\n\n18\n\nV.-E. BRUNEL\n\nand, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn \n\nCombined", "n ] 0. Together with (8), this shows that E[Y ] = 0\nn \n\n5\n\nn \n\nSimilarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .\n\n18\n\nV.-E. BRUNEL\n\nand, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn \n\nCombined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)\n\nn\n\nn\n\nn\n\ni=1\n\ni=1\n\ni=1\n\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn \n\nTherefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn \n\nn\n\nn(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn \n\ni=1\n\nin probability. Now, since we have assumed that Φ is twice d", "with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)\n\nn\n\nn\n\nn\n\ni=1\n\ni=1\n\ni=1\n\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn \n\nTherefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn \n\nn\n\nn(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn \n\ni=1\n\nin probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0\n\n(10)\n\nn \n\nin probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value", "ifferentiable at θ , we finally obtain\nthat\nFn (t) 0\n\n(10)\n\nn \n\nin probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand\n\nn\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1\n\nAs per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)\n\nsup\nt BS (0,R)\n\n G n (t) Gn (t) 0\nn \n\nin probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote b", "is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand\n\nn\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1\n\nAs per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)\n\nsup\nt BS (0,R)\n\n G n (t) Gn (t) 0\nn \n\nin probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1\n\n1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1\n\n1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2\n\nIt is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby\n\nt n = πTSn ( Zn \n\n 1\nnS Φ(θ )).\n\n19", "y Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1\n\n1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1\n\n1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2\n\nIt is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby\n\nt n = πTSn ( Zn \n\n 1\nnS Φ(θ )).\n\n19\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by", "ASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn \n\ncentered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that\n\nS\n 1\n\ntn πTn ( Z nS Φ(θ )) converges to 0 a", "the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn \n\ncentered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that\n\nS\n 1\n\ntn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,\n\n 1\nS\n\nπTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\nalmost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn \n\n 1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in", "lmost surely. Moreover,\n\n 1\nS\n\nπTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\nalmost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn \n\n 1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union", "probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,\n\nG n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2\n\n Gn (t n ) +\n\nHence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlar", "bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,\n\nG n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2\n\n Gn (t n ) +\n\nHence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S\n\nsome Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence t", "ge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S\n\nsome Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn \n\nthis convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d", "hat we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn \n\nthis convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn \n\n20\n\nV.-E. BRUNEL\n\nwhere Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn \n\n5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more gen", "+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn \n\n20\n\nV.-E. BRUNEL\n\nwhere Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn \n\n5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ", "eral convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ) =\n\n1\nϕ(Xi1 , . . . , Xik , θ).\n\n(nk) 1 i1 <...<ik n\n\nEstimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ a", ") =\n\n1\nϕ(Xi1 , . . . , Xik , θ).\n\n(nk) 1 i1 <...<ik n\n\nEstimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -est", "re then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard media", "imators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally", "n of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally, aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , .", ", aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matr", ". . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matrix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n21\n\n5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures),", "ix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n21\n\n5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 ,", "see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )]\n\nn \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).\n\n(k ) 1 i1 < <ik n\nThen,\n\nn(Un E[h(X1 , . .", ". . . , Xik ) E[h(X1 , . . . , Xk )]\n\nn \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).\n\n(k ) 1 i1 < <ik n\nThen,\n\nn(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn \n\nin distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the ter", ". , Xk )]) Nd (0, k 2 Σ)\nn \n\nin distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ", "ms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6\n\nΣ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 ,", "); Z)\n (S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6\n\nΣ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 , . . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.\n\n22\n\nV.-E. BRUNEL\n\n6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimens", ". . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.\n\n22\n\nV.-E. BRUNEL\n\n6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of look", "ional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional centra", "ing at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order h", "l limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "sentence", "num_chunks": 286, "avg_chunk_len": 228.91958041958043, "std_chunk_len": 145.54814586060627, "max_chunk_len": 722, "min_chunk_len": 10, "total_chars": 65471, "compression_ratio": 1.0034213621297978, "chunks": ["Asymptotics of constrained\nM -estimation under convexity\n\narXiv:2511. 04612v1 [math. ST] 6 Nov 2025\n\nVictor-Emmanuel Brunel \n\nAbstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc.", "Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints.", "We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.", "Key words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives. . 1.", "INTRODUCTION\n1. 1 Preliminaries\nWe consider a sequence X1 , X2 , . .", ". of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space.", "Here, d 1 is a fixed integer\nrepresenting the parameter dimension. Let ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 . Set Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 .", "The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , .", ". . , Xn .", "The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn . When minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints.", "In this work, we are concerned\nwith the latter. CREST-ENSAE, victor. emmanuel.", "brunel@ensae. fr\n\n1\n\n2\n\nV. -E.", "BRUNEL\n\nLet Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2. 2 below).", "Standard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n\n\nto the non-empty set Θ .", "If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution? n \n\nIt may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term. For simplicity, here, we only study the properties of exact empirical risk minimizers.", "Our main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1.", "Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R. For instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation.", "If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e. g. , if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ).", "Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter. 2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices.", "There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i. e. , their standard linear average).", "A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54].", "In fact, it is shown\nin [30, Lemma A. 5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).", "3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation.", "If (t) = t , this is median\nregression, etc. In all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 .", "In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas. ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n3\n\nTypically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument.", "Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions. 1.", "2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation. Asymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.", "Asymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases.", "Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below. When the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption.", "[40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss. Asymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27]. In the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29].", "Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators. This work is not concerned with penalized M -estimation.", "Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size. 1. 3 Outline\nIn Section 2, we give some key lemmas that we use in our main results.", "Section 2. 1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory. In Section 2.", "2, which is much more theoretical and could be skipped at first, we deal with the\n\n4\n\nV. -E. BRUNEL\n\nexistence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections.", "Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix.", "However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader. 1. 4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.", "1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , .", ". . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution.", "The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ).", "For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω . 2. The power set of a non-empty set A is denoted by P(A).", "3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary. 4.", "Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd . The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).", "5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .", "S\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .", "7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S . 8.", "The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S . d\n9.", "Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed.", "Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G.", "It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix. 10.", "The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex).", "If C is bounded, then hC only takes\nfinite values. See, e. g.", ", [49, Section 1. 7. 1].", "ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5\n\n11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc. 12.", "Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)).", "This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.", "13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0.", "In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1.", "If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense. This is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e. g.", ", [17, Chapter III]). 14. If f is differentiable at x, we denote by df (x; ) its differential.", "That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd . 15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.", "e. , we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 . 16.", "We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.", "17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X).", "When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S. 18.", "For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V . 2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.", "1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections. Lemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset.", "Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty. t G\n\nRemark 1.", "Recall that a convex function defined on an open convex set is automatically\n\n6\n\nV. -E. BRUNEL\n\ncontinuous on that set [48, Theorem 10.", "1], hence, it automatically reaches its bounds on any\ncompact set. The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.", "Proof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function.", "First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.", "Therefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0). That is, f (t) > f (t0 ).", "Therefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G. In the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .", "The second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries. Lemma 2.", "[48, Theorem 10. 8] Let G0 Rd be an open convex set and f, f1 , f2 , . .", ". be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).", "Then, fn converges uniformly to f on all compact subsets of G0 . An important consequence that we will use extensively is the following corollary. Corollary 1.", "Let f, f1 , f2 , . . .", "be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 .", "Then, for\nn \n\nall compact sets K G0 , supK fn f 0 almost surely (resp. in probability). n \n\nProof.", "Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately. Almost sure convergence. Let C be a dense and countable subset of G0 .", "By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn \n\nfn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .", "Convergence in probability. Again, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability.", "It is necessary and sufficient\nn \n\nto show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3. 3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely.", "Denote by t1 , t2 , . . .", "the elements of C. ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n7\n\nBy assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn \n\nsurely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.", "Similarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 . .", ". ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 .", ". . ψn (n), for all\nn 1.", "This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma. First, note that with probablity 1, fψ1 . .", ". ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1. Second, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 .", ". . ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence).", "Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn \n\nfollows from the first part of the proof (the case of almost sure convergence). In fact, we can also derive a similar corollary for Lp convergence, for any p 1.", "We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4. 2). 2.", "2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections.", "2. 2. 1 Measurable selections\nDefinition 1.", "Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω). There are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38].", "The one that we will need is the following, that follows from combining Theorems\n3. 2 (ii), 3. 5 and 5.", "1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd . Lemma 3.", "Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.", "A multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact). 2. 2.", "2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency. Theorem 1. Let f, f1 , f2 , .", ". . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely.", "Let G G0 be a closed, convex set. Assume\nn \n\n8\n\nV. -E.", "BRUNEL\n\nthat G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.", "n \n\nProof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps.", "First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 . Step 1.", "Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 . First, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}.", "This set is\ncompact, so\n(1)\n\nsup fn (t) f (t) 0. n \n\nt G ε G\n\nLet f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity.", "Therefore, η = mint G ε G f (t) f > 0. Then, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3\n\n f + η η/3\n\nby (1)\nby definition of η\n\n fn (t ) η/3 + η η/3\n\nagain by (1)\n\n= fn (t ) + η/3 > fn (t ). Therefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)\n\n Mn G ε .", "Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = \n\n {t0 } otherwise.", "Let us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection. Fix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.", "First, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}. Since fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively.", "Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F. 1\n\nrecall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity. 9\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable.", "If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } = \n\n {ω \n\np N q p+1\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t) <\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t)}\n\nwhich, therefore, is also measurable. Finally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.", "Step 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.", "n \n\n2. 2. 3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions.", "Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),\n\n t G0 . We denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5.", "In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i. e.", ", that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem. Theorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ).", "Then, f has a measurable subgradient at t0 . Proof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )).", "Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.", "Let K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε).", "Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above).", "First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions. Second, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε). Hence, h(ω, ) is continuous on Rd .", "Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K \n\nsince inf u K h( , u) is a measurable map. 10\n\nV.", "-E. BRUNEL\n\nFinally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 .", "The following lemma shows that\nthis fact still holds for generalized sums of convex functions. Theorem 3. Let f be a random convex function defined on a convex set G0 Rd .", "For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).", "Proof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable.", "Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).", "Since the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P). Now, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 .", "Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof. Remark 2. In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.", "As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case). In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely. To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?", "2\n\nThe other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument. ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n11\n\n3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1.", "In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a sequence is granted by Theorem 1. Theorem 4.", "Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n . The proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.", "Remark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer.", "For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ.", "However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n . 4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).", "4. 1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.", "The following useful property is fundamental in that case. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function. Proposition 1.", "Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn \n\nin probability.", "Proof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .", "ρn\n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) \n\nWrite Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . .", ", n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . .", ". , n. By Theorem 3, each Zi,n , i = 1, .", ". . , n, is square-integrable.", "Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n] \n\n1\nE[Yn2 ]\nn2\n\n12\n\nV. -E. BRUNEL\n\nwhere Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)).", "Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .", ",\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0. n \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn \n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)\n\n1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1\n\n(4)\n\n ρn (Φ(θ + t/ρn ) Φ(θ)) .", "The law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9. As a consequence, we obtain the following theorem.", "Theorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .", "Note that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ . Proof.", "Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ).", "Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1. Now, fix ε > 0.", "Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn \n\nt B(0,ε)\n\nin probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive. Assume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε.", "When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n εη εη/2\n\nby positive homogeneity of h Φ(θ )\n\nby definition of η\n\n> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability.", "Since this must hold for any positive, non-decreasing sequence\nn \n\n(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement. ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n13\n\nLet C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )).", "The next result extends Theorem 5. Theorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.", "Then, with probability going to 1 as n , θ n = θ . The assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C.", "In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ. Proof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact.", "Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.", "Finally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )). Now, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn \n\nof Theorem 5.", "Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10).", "That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn \n\nh Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.", "Remark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.", "Now, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection. 4.", "2 Differentiable case\nLet us first state the main result of this section. Theorem 7. following:\n\nLet g E Θ0 Rd be a measurable selection of subgradients of ϕ.", "Assume the\n\n14\n\nV. -E. BRUNEL\n\n(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).", "has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )). Remark 5 (on the assumptions of the theorem). (i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1].", "The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.", "(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold.", "In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ). S\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.", "1. 6, p. 216].", "In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e. g. , u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.", "). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).", "( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0. Remark 6 (on the conclusion of the theorem).", "1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S\n\nTheorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6. If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,\n\nS\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map.", "Therefore, Theorem 7 says that\nn \n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n15\n\nin distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.", "1]. In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 .", "Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e. g. ,\ntake Θ = B(θ , ε) for any small enough ε).", "Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0). If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that\n\nn(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .", "Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)\n\n 1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S\n\nalmost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.", "g. ,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0.", "In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case.", "This will be illustrated in the examples\nbelow. As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)\n\n 1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn \n\n(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ.", "The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale.", "We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.", "S\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ). ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,\n\n16\n\nV.", "-E. BRUNEL\n\nit shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).", "Note that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.", "1] implies\nthat Π is the gradient of a convex function. Now, let us look at some applications of Theorem 7. Example 1 (Constrained mean estimation).", "Let X1 , X2 , . . .", "be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + .", ". . + Xn ), for all n 1.", "Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that\n\nn(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn \n\nin distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .", "Here, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n . Example 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), .", ". . be iid random pairs in Rd R.", "Assume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .", "2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies\n\nS\n\n+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn \n\nin distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )\n\n= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S. 3\n\nIn fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ .", "This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4\n\n17\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nExample 3 (Geometric median). Let X1 , X2 , . .", ". be iid random vectors with one moment5 . Consider the loss function ϕ(x, θ) = x θ , x, θ Rd .", "Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e. g.", ", [28]. Proof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.", "First, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose\n\nvalue will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by\n\nt n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R).", "This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability. From this, we will then obtain that any minimizer of the first term (one of which is given by\n\nn(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.", "Fix t BS (0, R) and n 1. For i = 1, . .", ". , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ). By definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).", "Squaring and taking the expectation yields that\n2\n\n2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n\n\n(7)\n\n(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +\n\nn 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0.", "Moreover, for n 1, letting u = θ + t/ n and\n\nv = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))\n\n= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y .", "By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ]. (8)\n\nn \n\nHowever, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0.", "Together with (8), this shows that E[Y ] = 0\nn \n\n5\n\nn \n\nSimilarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd . 18\n\nV. -E.", "BRUNEL\n\nand, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0. n \n\nCombined with (7) and using independence of Z1,n , .", ". . , Zn,n , we obtain that\n(9)\n\nn\n\nn\n\nn\n\ni=1\n\ni=1\n\ni=1\n\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.", "n \n\nTherefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn \n\nn\n\nn(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn \n\ni=1\n\nin probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0\n\n(10)\n\nn \n\nin probability, for all t BS (0, R), as desired. For all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ).", "Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand\n\nn\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.", "2\ni=1\n\nAs per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)\n\nsup\nt BS (0,R)\n\n G n (t) Gn (t) 0\nn \n\nin probability. Moreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n . Now, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1\n\n1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1\n\n1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .", "2\n\nIt is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby\n\nt n = πTSn ( Zn \n\n 1\nnS Φ(θ )). 19\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n .", "Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n . Asymptotic behavior of t n .", "First, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn \n\ncentered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 . By Skorohod representation theorem (see [25, Theorem 5. 31] for instance), one may assume\nS\nthat Zn converges almost surely to Z.", "Since πC\nis non-expansive by Lemma 13, it holds that\n\nS\n 1\n\ntn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,\n\n 1\nS\n\nπTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\nalmost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn \n\n 1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution.", "The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed. Convergence in probability of t n t n to 0. Fix ε > 0.", "Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε.", "Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,\n\nG n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η. 2\n\n Gn (t n ) +\n\nHence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε.", "That is, t n t n converges in\nprobability to 0. S\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S\n\nsome Gaussian random variable Z and that t n tn converges to zero in probability, as n .", "Hence, Slutsky s theorem implies the desired result. In the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense.", "Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn \n\nthis convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn \n\n20\n\nV. -E.", "BRUNEL\n\nwhere Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?", "( S\nn \n\n5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e. g.", ", when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , .", ". . , Xk , θ) is integrable.", "Set\nΦ(θ) = E[ϕ(X1 , . . .", ", Xk , θ)] and, for all n k,\nΦn (θ) =\n\n1\nϕ(Xi1 , . . .", ", Xik , θ). (nk) 1 i1 <. .", ". <ik n\n\nEstimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1.", "Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . .", ", xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . .", ", xd , θ, for all x1 , . . .", ", xd , θ Rd . The minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , .", ". . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ.", "We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions. 2.", "Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . .", ". , n} of size k. That is, ϕ(x1 , .", ". . , xk , θ) = x1 +.", ". . +x\n θ , for\nk\nall x1 , .", ". . , xk , θ R.", "The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well. Note that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.", "3. More generally, aggregation of estimators that are based on overlapping subsamples, e. g.", ",\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning. 4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function.", "When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . .", ", Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1. 1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).", "In higher dimensions, one recovers the empirical covariance matrix of X1 , . . .", ", Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0. ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n21\n\n5.", "Empirical risk minimization where the choice of loss function itself depends on the data (e. g. ,\nfor data driven procedures), see, e.", "g. , [53]. Note that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.", "g. , [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.", "Theorem 8. Law of large numbers for U -statistics [20, Theorem 8. 6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ).", "Then,\n1\nh(Xi1 , . . .", ", Xik ) E[h(X1 , . . .", ", Xk )]\n\nn \n(nk) 1 i1 < <ik n\nalmost surely. Theorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.", "1], [20, Theorem 8. 9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , .", ". . , Xk ) X1 ]6 .", "For all n k, let Un = n\nh(Xi1 , . . .", ", Xik ). (k ) 1 i1 < <ik n\nThen,\n\nn(Un E[h(X1 , . .", ". , Xk )]) Nd (0, k 2 Σ)\nn \n\nin distribution. Theorem 4 obviously remains true in the context of U -estimation with convex loss.", "Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .", "Theorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).", "has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . .", ", Xk , θ ) X1 ]). Note the extra k factor in the limit in distribution. 6\n\nΣ can also be written as E[h(X1 , X2 , .", ". . , Xk )h(X1 , X2 , .", ". . , Xk ) ] E[h(X1 , .", ". . , Xk )]E[h(X1 , .", ". . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , .", ". . , Xk ) and h(X1 , X2 , .", ". . , Xk ), where X2 , .", ". . , Xk are such that\nX1 , X2 , .", ". . , Xk , X2 , .", ". . , Xk are iid.", "22\n\nV. -E. BRUNEL\n\n6.", "CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed. In large dimensional problems, asymptotic theory can be approached from different angles.", "First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.", "g. , an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , .", ". . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15].", "However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss. In the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.", "g. , for median-of-means procedures). While the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "paragraph", "num_chunks": 208, "avg_chunk_len": 313.52403846153845, "std_chunk_len": 682.7068574932481, "max_chunk_len": 4009, "min_chunk_len": 1, "total_chars": 65213, "compression_ratio": 1.0073911643383988, "chunks": ["Asymptotics of constrained\nM -estimation under convexity", "arXiv:2511.04612v1 [math.ST] 6 Nov 2025", "Victor-Emmanuel Brunel", "Abstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.\nKey words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..\n1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.", "CREST-ENSAE, victor.emmanuel.brunel@ensae.fr", "1", "2", "V.-E. BRUNEL", "Let Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n", "to the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn", "It may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "3", "Typically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.\n1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.\n1.3 Outline\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the", "4", "V.-E. BRUNEL", "existence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.\n1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd . The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .\nS\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "5", "11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e., we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V .\n2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G", "Remark 1.", "Recall that a convex function defined on an open convex set is automatically", "6", "V.-E. BRUNEL", "continuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all compact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn", "all compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn", "Proof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn", "fn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn", "to show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "7", "By assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn", "surely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn", "follows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).\n2.2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn", "8", "V.-E. BRUNEL", "that G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn", "Proof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)", "sup fn (t) f (t) 0.\nn", "t G ε G", "Let f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3", "f + η η/3", "by (1)\nby definition of η", "fn (t ) η/3 + η η/3", "again by (1)", "= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)", "Mn G ε .", "Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn =", "{t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1", "recall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.", "9", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } =", "{ω", "p N q p+1", "min\nt G B(t0 ,q)", "fn (ω, t) <", "min\nt G B(t0 ,q)", "fn (ω, t)}", "which, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn", "2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),", "t G0 .", "We denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K", "since inf u K h( , u) is a measurable map.", "10", "V.-E. BRUNEL", "Finally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.", "Remark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2", "The other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "11", "3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a sequence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n .\n4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).\n4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that case. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn", "in probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ)", "Write Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n]", "1\nE[Yn2 ]\nn2", "12", "V.-E. BRUNEL", "where Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)", "1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1", "(4)", "ρn (Φ(θ + t/ρn ) Φ(θ)) .", "The law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn", "t B(0,ε)", "in probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n εη εη/2", "by positive homogeneity of h Φ(θ )", "by definition of η", "> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn", "(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "13", "Let C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn", "of Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn", "h Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:", "Let g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the", "14", "V.-E. BRUNEL", "(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S", "Theorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,", "S\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "15", "in distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that", "n(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)", "1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S", "almost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrated in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)", "1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn", "(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,", "16", "V.-E. BRUNEL", "it shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] implies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.\nExample 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that", "n(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn", "in distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .\n2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies", "S", "+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn", "in distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )", "= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.", "3", "In fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4", "17", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Example 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose", "value will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by", "t n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by", "n(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2", "2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n", "(7)", "(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +", "n 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and", "v = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))", "= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].", "(8)", "n", "However, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0\nn", "5", "n", "Similarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .", "18", "V.-E. BRUNEL", "and, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn", "Combined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)", "n", "n", "n", "i=1", "i=1", "i=1", "2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn", "Therefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn", "n", "n(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn", "i=1", "in probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0", "(10)", "n", "in probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand", "n\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1", "As per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)", "sup\nt BS (0,R)", "G n (t) Gn (t) 0\nn", "in probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1", "1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1", "1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2", "It is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby", "t n = πTSn ( Zn", "1\nnS Φ(θ )).", "19", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn", "centered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that", "S\n 1", "tn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,", "1\nS", "πTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "almost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn", "1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,", "G n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2", "Gn (t n ) +", "Hence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S", "some Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn", "this convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn", "20", "V.-E. BRUNEL", "where Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn", "5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ) =", "1\nϕ(Xi1 , . . . , Xik , θ).", "(nk) 1 i1 <...<ik n", "Estimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally, aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matrix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "21", "5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )]", "n \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).", "(k ) 1 i1 < <ik n\nThen,", "n(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn", "in distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6", "Σ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 , . . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.", "22", "V.-E. BRUNEL", "6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "recursive", "num_chunks": 381, "avg_chunk_len": 170.84251968503938, "std_chunk_len": 162.14732414538778, "max_chunk_len": 774, "min_chunk_len": 1, "total_chars": 65091, "compression_ratio": 1.009279316648999, "chunks": ["Asymptotics of constrained\nM -estimation under convexity", "arXiv:2511.04612v1 [math.ST] 6 Nov 2025", "Victor-Emmanuel Brunel", "Abstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check.", "Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics.", "Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc. Key words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives. .", "1. INTRODUCTION\n1. 1 Preliminaries\nWe consider a sequence X1 , X2 , .", ". . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution.", "Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension. Let ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .", "Set Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ).", "For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . .", ", Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn . When minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation.", "If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.", "CREST-ENSAE, victor.emmanuel.brunel@ensae.fr", "1", "2", "V.-E. BRUNEL", "Let Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n", "to the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn", "It may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term. For simplicity, here, we only study the properties of exact empirical risk minimizers. Our main working assumption is that the loss function is convex in its second argument.", "That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.", "For instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.", "g. , if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.", "2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.", "e. , their standard linear average). A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite).", "More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.", "5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]). 3.", "Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.", "In all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ.", "Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "3", "Typically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.", "1. 2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation. Asymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models.", "Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large. Asymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space.", "See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below.", "When the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss. Asymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].", "In the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.", "This work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size. 1.", "3 Outline\nIn Section 2, we give some key lemmas that we use in our main results. Section 2. 1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.", "In Section 2. 2, which is much more theoretical and could be skipped at first, we deal with the", "4", "V.-E. BRUNEL", "existence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5.", "More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader. 1.", "4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions. 1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space.", "We let X1 , X2 , . . .", "be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 .", "We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω . 2.", "The power set of a non-empty set A is denoted by P(A). 3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.", "4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd .", "The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r). 5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane.", "When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S . S\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty.", "When t = 0, we simply write Cu = Cu,t . 7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .", "8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .", "d\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0.", "It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite.", "The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.", "10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C.", "It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.", "g. , [49, Section 1. 7.", "1].", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "5", "11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc. 12.", "Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)).", "This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.", "13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0.", "In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1.", "If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense. This is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e. g.", ", [17, Chapter III]). 14. If f is differentiable at x, we denote by df (x; ) its differential.", "That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd . 15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.", "e. , we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 . 16.", "We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.", "17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X).", "When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S. 18.", "For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V . 2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.", "1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections. Lemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset.", "Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty. t G", "Remark 1.", "Recall that a convex function defined on an open convex set is automatically", "6", "V.-E. BRUNEL", "continuous on that set [48, Theorem 10. 1], hence, it automatically reaches its bounds on any\ncompact set. The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K.", "We need this slightly more technical statement in\norder to deal with constrained M -estimation later. Proof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ).", "Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K.", "Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G. Therefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0).", "That is, f (t) > f (t0 ). Therefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.", "In the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn . The second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.", "Lemma 2. [48, Theorem 10. 8] Let G0 Rd be an open convex set and f, f1 , f2 , .", ". . be convex\nfunctions on G0 .", "Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t). Then, fn converges uniformly to f on all compact subsets of G0 . An important consequence that we will use extensively is the following corollary.", "Corollary 1. Let f, f1 , f2 , . .", ". be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp.", "in probability) for all t G0 . Then, for\nn", "all compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn", "Proof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn", "fn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn", "to show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "7", "By assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn", "surely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely. Similarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ).", "By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 . . .", "ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . .", ". ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.", "First, note that with probablity 1, fψ1 . . .", "ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1. Second, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 . .", ". ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely.", "The rest\nn", "follows from the first part of the proof (the case of almost sure convergence). In fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.", "2). 2. 2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive).", "Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections. 2.", "2. 1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd .", "A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω). There are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.", "2 (ii), 3. 5 and 5. 1 of [21].", "Denote by C the collection of all non-empty, closed subsets of Rd . Lemma 3. Let Γ C be a multifunction.", "Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection. A multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).", "2. 2. 2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.", "Theorem 1. Let f, f1 , f2 , . .", ". be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set.", "Assume\nn", "8", "V.-E. BRUNEL", "that G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn", "Proof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)", "sup fn (t) f (t) 0.\nn", "t G ε G", "Let f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3", "f + η η/3", "by (1)\nby definition of η", "fn (t ) η/3 + η η/3", "again by (1)", "= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)", "Mn G ε .", "Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn =", "{t0 } otherwise. Let us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection.", "Fix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set. First, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}. Since fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}.", "Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F. 1", "recall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.", "9", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } =", "{ω", "p N q p+1", "min\nt G B(t0 ,q)", "fn (ω, t) <", "min\nt G B(t0 ,q)", "fn (ω, t)}", "which, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn", "2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),", "t G0 .", "We denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 .", "Now, if f is a random convex function, the existence of a measurable subgradient\n(i. e. , that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.", "Theorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .", "Proof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values.", "Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3. Let K Rd be any arbitrary compact set.", "Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε).", "Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.", "Second, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε). Hence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK.", "Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K", "since inf u K h( , u) is a measurable map.", "10", "V.-E. BRUNEL", "Finally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.", "Theorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t.", "Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t). Proof.", "Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )).", "Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))). Since the right hand side is in Lp (P) by assumption, so is g(t0 ) v.", "The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P). Now, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.", "Remark 2. In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0. As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).", "In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely. To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )? 2", "The other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "11", "3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ.", "Existence of such a sequence is granted by Theorem 1. Theorem 4. Assume that Θ is compact and non-empty.", "Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n . The proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above. Remark 3.", "Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ.", "Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n .", "4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ). 4.", "1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton. The following useful property is fundamental in that case.", "Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function. Proposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 .", "Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn", "in probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ)", "Write Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n]", "1\nE[Yn2 ]\nn2", "12", "V.-E. BRUNEL", "where Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)", "1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1", "(4)", "ρn (Φ(θ + t/ρn ) Φ(θ)) .", "The law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9. As a consequence, we obtain the following theorem.", "Theorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .", "Note that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ . Proof.", "Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ).", "Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1. Now, fix ε > 0.", "Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn", "t B(0,ε)", "in probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n εη εη/2", "by positive homogeneity of h Φ(θ )", "by definition of η", "> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn", "(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "13", "Let C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.", "Theorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}. Then, with probability going to 1 as n , θ n = θ .", "The assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.", "Proof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous.", "Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0. Finally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )).", "Now, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn", "of Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn", "h Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:", "Let g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the", "14", "V.-E. BRUNEL", "(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )). Remark 5 (on the assumptions of the theorem). (i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1].", "The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.", "(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold.", "In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ). S\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.", "1. 6, p. 216].", "In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e. g. , u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.", "). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).", "( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0. Remark 6 (on the conclusion of the theorem).", "1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S", "Theorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,", "S\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "15", "in distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.", "1]. In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 .", "Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e. g. ,\ntake Θ = B(θ , ε) for any small enough ε).", "Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0). If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that", "n(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)", "1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S", "almost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.", "g. ,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0.", "In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case.", "This will be illustrated in the examples\nbelow. As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)", "1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn", "(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z).", "In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error.", "A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set. S\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).", "( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,", "16", "V.-E. BRUNEL", "it shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity). Note that non-expansiveness is implied by firm monotonicity.", "Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2. 1] implies\nthat Π is the gradient of a convex function.", "Now, let us look at some applications of Theorem 7. Example 1 (Constrained mean estimation). Let X1 , X2 , .", ". . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set.", "Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . .", ". + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive).", "For asymptotic normality, we obtain, from Theorem 7, that", "n(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn", "in distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 . Here, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .", "Example 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . .", ". be iid random pairs in Rd R. Assume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0.", "Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 . 2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ).", "Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies", "S", "+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn", "in distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )", "= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.", "3", "In fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4", "17", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Example 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose", "value will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by", "t n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by", "n(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2", "2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n", "(7)", "(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +", "n 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and", "v = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))", "= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].", "(8)", "n", "However, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0\nn", "5", "n", "Similarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .", "18", "V.-E. BRUNEL", "and, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn", "Combined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)", "n", "n", "n", "i=1", "i=1", "i=1", "2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn", "Therefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn", "n", "n(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn", "i=1", "in probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0", "(10)", "n", "in probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand", "n\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1", "As per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)", "sup\nt BS (0,R)", "G n (t) Gn (t) 0\nn", "in probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1", "1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1", "1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2", "It is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby", "t n = πTSn ( Zn", "1\nnS Φ(θ )).", "19", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn", "centered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that", "S\n 1", "tn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,", "1\nS", "πTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "almost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn", "1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,", "G n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2", "Gn (t n ) +", "Hence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S", "some Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn", "this convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn", "20", "V.-E. BRUNEL", "where Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn", "5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ) =", "1\nϕ(Xi1 , . . . , Xik , θ).", "(nk) 1 i1 <...<ik n", "Estimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , .", ". . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , .", ". . , xd , θ, for all x1 , .", ". . , xd , θ Rd .", "The minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . .", ". , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators).", "More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions. 2. Let E = R and Θ Θ0 = R and k 1.", "[37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . .", ", n} of size k. That is, ϕ(x1 , . .", ". , xk , θ) = x1 +. .", ". +x\n θ , for\nk\nall x1 , . .", ". , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered.", "Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well. Note that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work. 3.", "More generally, aggregation of estimators that are based on overlapping subsamples, e. g. ,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.", "4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , .", ". . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.", "1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0). In higher dimensions, one recovers the empirical covariance matrix of X1 , .", ". . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd .", "Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "21", "5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )]", "n \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).", "(k ) 1 i1 < <ik n\nThen,", "n(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn", "in distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6", "Σ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 , . . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.", "22", "V.-E. BRUNEL", "6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.", "In large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant.", "A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e. g. , an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner.", "When\nθ n is simply the sample mean of X1 , . . .", ", Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss. In the context of U -estimators, we have also let the order k of the U -process be fixed.", "However,\nit may be relevant to also let k grow with the sample size (e. g. , for median-of-means procedures).", "While the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "semantic", "num_chunks": 24, "avg_chunk_len": 2688.7083333333335, "std_chunk_len": 2485.067663048746, "max_chunk_len": 10325, "min_chunk_len": 26, "total_chars": 64529, "compression_ratio": 1.0180693951556665, "chunks": ["Asymptotics of constrained\nM -estimation under convexity\n\narXiv:2511.04612v1 [math.ST] 6 Nov 2025\n\nVictor-Emmanuel Brunel \n\nCREST-ENSAE, victor.emmanuel.brunel@ensae.fr", "Abstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.", "Key words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..", "1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.\n\n1", "2\nV.-E. BRUNEL\n\nLet Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n\n\nto the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn \n\nIt may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.", "ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n3\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.\n\nTypically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.", "1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.", "1.3 Outline\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the\n\n4\nV.-E. BRUNEL\n\nexistence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.", "1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd . The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane.\nS\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].", "ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5\n11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e., we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V .", "2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS", "2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G\nRemark 1. Recall that a convex function defined on an open convex set is automatically\n\n6\nV.-E. BRUNEL\n\ncontinuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all compact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn \n\nall compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn \n\nProof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn \n\nfn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn \n\nto show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.\n\n7\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nBy assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn \n\nsurely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn \n\nfollows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).", "2.2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn \n\n8\nV.-E. BRUNEL\n\nthat G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn \n\nProof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)\n\nsup fn (t) f (t) 0.\nn \n\nt G ε G\n\nLet f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3\n\n f + η η/3\n\nby (1)\nby definition of η\n\n fn (t ) η/3 + η η/3\n\nagain by (1)\n\n= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)\n\n Mn G ε .\n\n Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = \n\n {t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1\nrecall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.\n\n9\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } = \n\n {ω \n\np N q p+1\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t) <\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t)}\n\nwhich, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn \n\n2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),\n\n t G0 .\n\nWe denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K \n\nsince inf u K h( , u) is a measurable map.\n\n10\nV.-E. BRUNEL\n\nFinally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.\n\nRemark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2\nThe other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.", "3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a sequence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n .", "4. ASYMPTOTIC DISTRIBUTION", "4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that case.\nRecall that for a non-empty convex subset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn \nin probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn\n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) \n\nWrite Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n] \n\n1\nE[Yn2 ]\nn2\n\n12\nV.-E. BRUNEL\n\nwhere Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn \n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)\n\n1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1\n\n(4)\n\n ρn (Φ(θ + t/ρn ) Φ(θ)) .\n\nThe law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn \n\nt B(0,ε)\n\nin probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n\nby positive homogeneity of h Φ(θ )\n\n εη εη/2\n\nby definition of η\n\n> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn \n\n(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.", "ASYMPTOTICS OF CONVEX M -ESTIMATION\n\n13\nLet C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn \n\nof Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn \n\nh Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection.", "4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7. Let g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the\nfollowing:\n(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S\n\nTheorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \n\nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,\n\nS\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn \n\nin distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that\n\nn(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)\n\n 1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S\n\nalmost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrated in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)\n\n 1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn \n\n(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,\n\n16\nV.-E. BRUNEL\n\nit shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] implies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.", "Example 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that\n\nn(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn \n\nin distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .\n2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies\n\nS\n\n+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn \n\nin distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )\n\n= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.\n\n17\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nExample 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].", "Proof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose\n\nvalue will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by\n\nt n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by\n\nn(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2\n\n2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n\n\n(7)\n\n(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +\n\nn 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and\n\nv = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))\n\n= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].\n\n(8)\n\nn \n\nHowever, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0\nn \n\nn \n\nand, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn \n\nCombined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)\n\nn\nn\nn\ni=1\ni=1\ni=1\n\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn \n\nTherefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn \n\nn\nn(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn \n\ni=1\n\nin probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0\n\n(10)\n\nn \n\nin probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand\n\nn\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1\n\nAs per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)\n\nsup\nt BS (0,R)\n\n G n (t) Gn (t) 0\nn \n\nin probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1\n\n1\n 2\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1\n\n1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2\n\nIt is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby\n\nt n = πTSn ( Zn \n\n 1\nnS Φ(θ )).\n\n19\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn \n\ncentered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that\n\nS\n 1\n\ntn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,\n\n 1\nS\n\nπTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\nalmost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn \n\n 1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,\n\nG n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2\n\n G n (t n ) +\n\nHence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S\n\nsome Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.", "In the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn \n\nthis convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn \n\nwhere Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn", "5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ) =\n\n1\nϕ(Xi1 , . . . , Xik , θ).\n\n(nk) 1 i1 <...<ik n\n\nEstimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally, aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matrix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.\n\n21\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].", "Note that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )]\nn \n(nk) 1 i1 <...<ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).\n\n(k ) 1 i1 < <ik n\nThen,\n\nn(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn \n\nin distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.", "6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "delimiter", "num_chunks": 208, "avg_chunk_len": 313.52403846153845, "std_chunk_len": 682.7068574932481, "max_chunk_len": 4009, "min_chunk_len": 1, "total_chars": 65213, "compression_ratio": 1.0073911643383988, "chunks": ["Asymptotics of constrained\nM -estimation under convexity", "arXiv:2511.04612v1 [math.ST] 6 Nov 2025", "Victor-Emmanuel Brunel", "Abstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.\nKey words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..\n1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.", "CREST-ENSAE, victor.emmanuel.brunel@ensae.fr", "1", "2", "V.-E. BRUNEL", "Let Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n", "to the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn", "It may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "3", "Typically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.\n1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.\n1.3 Outline\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the", "4", "V.-E. BRUNEL", "existence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.\n1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd . The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .\nS\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "5", "11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e., we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V .\n2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G", "Remark 1.", "Recall that a convex function defined on an open convex set is automatically", "6", "V.-E. BRUNEL", "continuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all compact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn", "all compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn", "Proof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn", "fn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn", "to show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "7", "By assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn", "surely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn", "follows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).\n2.2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn", "8", "V.-E. BRUNEL", "that G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn", "Proof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)", "sup fn (t) f (t) 0.\nn", "t G ε G", "Let f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3", "f + η η/3", "by (1)\nby definition of η", "fn (t ) η/3 + η η/3", "again by (1)", "= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)", "Mn G ε .", "Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn =", "{t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1", "recall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.", "9", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } =", "{ω", "p N q p+1", "min\nt G B(t0 ,q)", "fn (ω, t) <", "min\nt G B(t0 ,q)", "fn (ω, t)}", "which, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn", "2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),", "t G0 .", "We denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K", "since inf u K h( , u) is a measurable map.", "10", "V.-E. BRUNEL", "Finally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.", "Remark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2", "The other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "11", "3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a sequence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n .\n4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).\n4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that case. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn", "in probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ)", "Write Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n]", "1\nE[Yn2 ]\nn2", "12", "V.-E. BRUNEL", "where Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)", "1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1", "(4)", "ρn (Φ(θ + t/ρn ) Φ(θ)) .", "The law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn", "t B(0,ε)", "in probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n εη εη/2", "by positive homogeneity of h Φ(θ )", "by definition of η", "> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn", "(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "13", "Let C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn", "of Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn", "h Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:", "Let g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the", "14", "V.-E. BRUNEL", "(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S", "Theorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,", "S\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "15", "in distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that", "n(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)", "1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S", "almost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrated in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)", "1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn", "(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,", "16", "V.-E. BRUNEL", "it shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] implies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.\nExample 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that", "n(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn", "in distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .\n2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies", "S", "+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn", "in distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )", "= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.", "3", "In fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4", "17", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Example 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose", "value will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by", "t n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by", "n(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2", "2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n", "(7)", "(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +", "n 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and", "v = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))", "= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].", "(8)", "n", "However, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0\nn", "5", "n", "Similarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .", "18", "V.-E. BRUNEL", "and, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn", "Combined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)", "n", "n", "n", "i=1", "i=1", "i=1", "2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn", "Therefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn", "n", "n(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn", "i=1", "in probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0", "(10)", "n", "in probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand", "n\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1", "As per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)", "sup\nt BS (0,R)", "G n (t) Gn (t) 0\nn", "in probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1", "1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1", "1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2", "It is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby", "t n = πTSn ( Zn", "1\nnS Φ(θ )).", "19", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn", "centered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that", "S\n 1", "tn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,", "1\nS", "πTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "almost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn", "1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,", "G n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2", "Gn (t n ) +", "Hence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S", "some Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn", "this convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn", "20", "V.-E. BRUNEL", "where Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn", "5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ) =", "1\nϕ(Xi1 , . . . , Xik , θ).", "(nk) 1 i1 <...<ik n", "Estimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally, aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matrix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "21", "5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )]", "n \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).", "(k ) 1 i1 < <ik n\nThen,", "n(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn", "in distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6", "Σ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 , . . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.", "22", "V.-E. BRUNEL", "6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "token_limit", "num_chunks": 83, "avg_chunk_len": 791.1927710843373, "std_chunk_len": 76.99389082811189, "max_chunk_len": 800, "min_chunk_len": 94, "total_chars": 65669, "compression_ratio": 1.0003959250178929, "avg_chunk_tokens": 197.6024096385542, "max_chunk_tokens": 200, "min_chunk_tokens": 23, "tokenizer": "", "chunks": ["Asymptotics of constrained\nM -estimation under convexity\n\narXiv:2511.04612v1 [math.ST] 6 Nov 2025\n\nVictor-Emmanuel Brunel \n\nAbstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the correspondin", "g M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.\nKey words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..\n1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribu", "tion. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter s", "pace Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.\n\nCREST-ENSAE, victor.emmanuel.brunel@ensae.fr\n\n1\n\n2\n\nV.-E. BRUNEL\n\nLet Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of", "θ n\n\nto the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn \n\nIt may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared E", "uclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A si", "mple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed unde", "r the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\ntha", "t case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n3\n\nTypically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectation", "s in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.\n1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well", "understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in", "the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is,", "when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.\n1.3 Outlin", "e\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the\n\n4\n\nV.-E. BRUNEL\n\nexistence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojectio", "ns, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.\n1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of", "Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd .", "The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .\nS\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is th", "e unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhy", "perplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5\n\n11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with v", "alues in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqen", "ces (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e., we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does no", "t bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with", "mean u and covariance matrix V .\n2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G\n\nRemark 1.\n\n Recall that a convex function defined on an open convex set is automatically\n\n6\n\nV.-E. BRUNEL\n\ncontinuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version,", "when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1)", "> ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all co", "mpact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn \n\nall compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn \n\nProof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn \n\nfn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nunif", "ormly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn \n\nto show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n7\n\nBy assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn \n\nsurely. That is, there is an increasing map ψ1 N N such that", "fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Henc", "e, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn \n\nfollows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).\n2.2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the th", "eory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is calle", "d C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn \n\n8\n\nV.-E. BRUNEL\n\nthat G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn \n\nProof", ". For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)\n\nsup fn (t) f (t) 0.\nn \n\nt G ε G\n\nLet f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open se", "t G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3\n\n f + η η/3\n\nby (1)\nby definition of η\n\n fn (t ) η/3 + η η/3\n\nagain by (1)\n\n= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)\n\n Mn G ε .\n\n Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = \n\n {t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a me", "asurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1\n\nrecall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.\n\n9\n\nASYMPTOTICS OF C", "ONVEX M -ESTIMATION\n\nNow, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } = \n\n {ω \n\np N q p+1\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t) <\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t)}\n\nwhich, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn \n\n2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable sub", "gradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),\n\n t G0 .\n\nWe denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f", "at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for al", "l ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K \n\nsince inf u K h( , u) is a measurable map.\n\n10\n\nV.-E. BRUNEL\n\nFinally, let us state an incredibly simple yet pow", "erful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient", "of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (", "t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.\n\nRemark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of", "our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2\n\nThe other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n11\n\n3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a s", "equence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the em", "pirical minimizer is unique, given by θ n = X n / X n .\n4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).\n4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that case. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn \n\nin probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn", "(Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn\n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) \n\nWrite Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n] \n\n1\nE[Yn2 ]\nn2\n\n12\n\nV.-E. BRUNEL\n\nwhere Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (", "t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn \n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)\n\n1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1\n\n(4)\n\n ρn (Φ(θ + t/ρn ) Φ(θ)) .\n\nThe law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is o", "pen, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn \n\nt B(0,ε)\n\nin probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n ε", "η εη/2\n\nby positive homogeneity of h Φ(θ )\n\nby definition of η\n\n> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn \n\n(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n13\n\nLet C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theo", "rem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, s", "ince it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn \n\nof Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(", "θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn \n\nh Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivat", "ives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:\n\nLet g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the\n\n14\n\nV.-E. BRUNEL\n\n(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is ma", "de in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P )", "for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear con", "vex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . Whe", "n Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S\n\nTheorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,\n\nS\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn \n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n15\n\nin distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.", "1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the bou", "ndary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that\n\nn(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)\n\n 1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S\n\nalmost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not", "in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrated in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)\n\n 1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn \n\n(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around", "θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS", "1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,\n\n16\n\nV.-E. BRUNEL\n\nit shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] impl", "ies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.\nExample 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that\n\nn(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn \n\nin distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this sim", "ple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .", "2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies\n\nS\n\n+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn \n\nin distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )\n\n= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.\n\n3\n\nIn fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4\n\n17\n\nASYMPTOTICS OF CON", "VEX M -ESTIMATION\n\nExample 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose\n\nvalue", "will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by\n\nt n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by\n\nn(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/", "2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2\n\n2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n\n\n(7)\n\n(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +\n\nn 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and\n\nv = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))\n\n= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].\n\n(8)\n\nn \n\nHowever, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Y", "n ] 0. Together with (8), this shows that E[Y ] = 0\nn \n\n5\n\nn \n\nSimilarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .\n\n18\n\nV.-E. BRUNEL\n\nand, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn \n\nCombined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)\n\nn\n\nn\n\nn\n\ni=1\n\ni=1\n\ni=1\n\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn \n\nTherefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn \n\nn\n\nn(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn \n\ni=1\n\nin probability. Now, since we have assumed that Φ is twice d", "ifferentiable at θ , we finally obtain\nthat\nFn (t) 0\n\n(10)\n\nn \n\nin probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand\n\nn\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1\n\nAs per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)\n\nsup\nt BS (0,R)\n\n G n (t) Gn (t) 0\nn \n\nin probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote b", "y Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1\n\n1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1\n\n1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2\n\nIt is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby\n\nt n = πTSn ( Zn \n\n 1\nnS Φ(θ )).\n\n19\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by", "the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn \n\ncentered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that\n\nS\n 1\n\ntn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,\n\n 1\nS\n\nπTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\nalmost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn \n\n 1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in", "probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,\n\nG n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2\n\n Gn (t n ) +\n\nHence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlar", "ge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S\n\nsome Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn \n\nthis convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d", "+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn \n\n20\n\nV.-E. BRUNEL\n\nwhere Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn \n\n5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ", ") =\n\n1\nϕ(Xi1 , . . . , Xik , θ).\n\n(nk) 1 i1 <...<ik n\n\nEstimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -est", "imators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally", ", aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matr", "ix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n21\n\n5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 ,", ". . . , Xik ) E[h(X1 , . . . , Xk )]\n\nn \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).\n\n(k ) 1 i1 < <ik n\nThen,\n\nn(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn \n\nin distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the ter", "ms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6\n\nΣ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 ,", ". . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.\n\n22\n\nV.-E. BRUNEL\n\n6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of look", "ing at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order h", "ave been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "format_aware", "num_chunks": 1, "avg_chunk_len": 65694.0, "std_chunk_len": 0.0, "max_chunk_len": 65694, "min_chunk_len": 65694, "total_chars": 65694, "compression_ratio": 1.0000152220902974, "chunks": ["Asymptotics of constrained\nM -estimation under convexity\n\narXiv:2511.04612v1 [math.ST] 6 Nov 2025\n\nVictor-Emmanuel Brunel \n\nAbstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics. Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc.\nKey words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives..\n1. INTRODUCTION\n1.1 Preliminaries\nWe consider a sequence X1 , X2 , . . . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution. Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension.\nLet ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .\nSet Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ). For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . . , Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn .\nWhen minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation. If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.\n\nCREST-ENSAE, victor.emmanuel.brunel@ensae.fr\n\n1\n\n2\n\nV.-E. BRUNEL\n\nLet Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n\n\nto the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn \n\nIt may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term.\nFor simplicity, here, we only study the properties of exact empirical risk minimizers.\nOur main working assumption is that the loss function is convex in its second argument. That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.\nFor instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.g., if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.\n2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.e., their standard linear average). A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite). More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]).\n3. Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.\nIn all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ. Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n3\n\nTypically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.\n1.2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation.\nAsymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models. Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large.\nAsymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space. See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below.\nWhen the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss.\nAsymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].\nIn the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.\nThis work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size.\n1.3 Outline\nIn Section 2, we give some key lemmas that we use in our main results. Section 2.1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.\nIn Section 2.2, which is much more theoretical and could be skipped at first, we deal with the\n\n4\n\nV.-E. BRUNEL\n\nexistence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5. More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader.\n1.4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions.\n1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space. We let X1 , X2 , . . . be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 . We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω .\n2. The power set of a non-empty set A is denoted by P(A).\n3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.\n4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd . The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r).\n5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane. When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S .\nS\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty. When t = 0, we simply write Cu = Cu,t .\n7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .\n8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .\nd\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0. It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite. The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.\n10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C. It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.g., [49, Section 1.7.1].\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n5\n\n11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc.\n12. Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)). This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.\n13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0. In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1. If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense.\nThis is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e.g., [17, Chapter III]).\n14. If f is differentiable at x, we denote by df (x; ) its differential. That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd .\n15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.e., we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 .\n16. We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.\n17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X). When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S.\n18. For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V .\n2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections.\nLemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset. Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty.\nt G\n\nRemark 1.\n\n Recall that a convex function defined on an open convex set is automatically\n\n6\n\nV.-E. BRUNEL\n\ncontinuous on that set [48, Theorem 10.1], hence, it automatically reaches its bounds on any\ncompact set.\n The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K. We need this slightly more technical statement in\norder to deal with constrained M -estimation later.\nProof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ). Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K. Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G.\nTherefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0). That is, f (t) > f (t0 ).\nTherefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.\nIn the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn .\nThe second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.\nLemma 2. [48, Theorem 10.8] Let G0 Rd be an open convex set and f, f1 , f2 , . . . be convex\nfunctions on G0 . Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t).\nThen, fn converges uniformly to f on all compact subsets of G0 .\nAn important consequence that we will use extensively is the following corollary.\nCorollary 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp. in probability) for all t G0 . Then, for\nn \n\nall compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn \n\nProof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn \n\nfn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn \n\nto show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n7\n\nBy assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn \n\nsurely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely.\nSimilarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ). By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . . . ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.\nFirst, note that with probablity 1, fψ1 ... ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1.\nSecond, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 ... ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely. The rest\nn \n\nfollows from the first part of the proof (the case of almost sure convergence).\nIn fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.2).\n2.2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive). Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections.\n2.2.1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd . A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω).\nThere are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.2 (ii), 3.5 and 5.1 of [21]. Denote by C the collection of all non-empty, closed subsets of Rd .\nLemma 3. Let Γ C be a multifunction. Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection.\nA multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).\n2.2.2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.\nTheorem 1. Let f, f1 , f2 , . . . be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set. Assume\nn \n\n8\n\nV.-E. BRUNEL\n\nthat G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn \n\nProof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)\n\nsup fn (t) f (t) 0.\nn \n\nt G ε G\n\nLet f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3\n\n f + η η/3\n\nby (1)\nby definition of η\n\n fn (t ) η/3 + η η/3\n\nagain by (1)\n\n= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)\n\n Mn G ε .\n\n Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn = \n\n {t0 } otherwise.\nLet us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection.\nFix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set.\nFirst, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}.\nSince fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}. Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F.\n1\n\nrecall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.\n\n9\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } = \n\n {ω \n\np N q p+1\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t) <\n\nmin\nt G B(t0 ,q)\n\nfn (ω, t)}\n\nwhich, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn \n\n2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),\n\n t G0 .\n\nWe denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 . Now, if f is a random convex function, the existence of a measurable subgradient\n(i.e., that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.\nTheorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .\nProof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values. Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3.\nLet K Rd be any arbitrary compact set. Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε). Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.\nSecond, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε).\nHence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK. Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K \n\nsince inf u K h( , u) is a measurable map.\n\n10\n\nV.-E. BRUNEL\n\nFinally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.\nTheorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t. Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t).\nProof. Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )). Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))).\nSince the right hand side is in Lp (P) by assumption, so is g(t0 ) v. The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P).\nNow, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.\n\nRemark 2.\n In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0.\n As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).\n In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely.\n To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )?\n2\n\nThe other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n11\n\n3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ. Existence of such a sequence is granted by Theorem 1.\nTheorem 4. Assume that Θ is compact and non-empty. Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n .\nThe proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above.\nRemark 3. Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ. Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n .\n4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ).\n4.1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton.\nThe following useful property is fundamental in that case. Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function.\nProposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 . Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn \n\nin probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn\n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ) \n\nWrite Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n] \n\n1\nE[Yn2 ]\nn2\n\n12\n\nV.-E. BRUNEL\n\nwhere Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn \n\nFn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)\n\n1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1\n\n(4)\n\n ρn (Φ(θ + t/ρn ) Φ(θ)) .\n\nThe law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9.\nAs a consequence, we obtain the following theorem.\nTheorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .\nNote that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ .\nProof. Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ). Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1.\nNow, fix ε > 0. Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn \n\nt B(0,ε)\n\nin probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n εη εη/2\n\nby positive homogeneity of h Φ(θ )\n\nby definition of η\n\n> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn \n\n(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n13\n\nLet C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.\nTheorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}.\nThen, with probability going to 1 as n , θ n = θ .\nThe assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.\nProof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous. Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0.\nFinally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )).\nNow, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn \n\nof Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn \n\nh Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:\n\nLet g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the\n\n14\n\nV.-E. BRUNEL\n\n(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )).\nRemark 5 (on the assumptions of the theorem).\n(i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1]. The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.\n(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold. In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ).\nS\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.1.6, p. 216]. In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e.g., u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).\n ( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0.\nRemark 6 (on the conclusion of the theorem).\n 1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S\n\nTheorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,\n\nS\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn \n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n15\n\nin distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.1].\n In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 . Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e.g.,\ntake Θ = B(θ , ε) for any small enough ε). Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0).\n If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that\n\nn(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)\n\n 1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S\n\nalmost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.g.,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0. In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case. This will be illustrated in the examples\nbelow.\n As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)\n\n 1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn \n\n(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z). In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error. A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set.\nS\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).\n ( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,\n\n16\n\nV.-E. BRUNEL\n\nit shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity).\nNote that non-expansiveness is implied by firm monotonicity. Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2.1] implies\nthat Π is the gradient of a convex function.\nNow, let us look at some applications of Theorem 7.\nExample 1 (Constrained mean estimation). Let X1 , X2 , . . . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set. Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . . . + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive). For asymptotic normality, we obtain, from Theorem 7, that\n\nn(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn \n\nin distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 .\nHere, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .\nExample 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . . . be iid random pairs in Rd R.\nAssume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0. Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 .\n2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ). Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies\n\nS\n\n+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn \n\nin distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )\n\n= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.\n\n3\n\nIn fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4\n\n17\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nExample 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose\n\nvalue will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by\n\nt n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by\n\nn(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2\n\n2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n\n\n(7)\n\n(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +\n\nn 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and\n\nv = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))\n\n= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].\n\n(8)\n\nn \n\nHowever, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0\nn \n\n5\n\nn \n\nSimilarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .\n\n18\n\nV.-E. BRUNEL\n\nand, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn \n\nCombined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)\n\nn\n\nn\n\nn\n\ni=1\n\ni=1\n\ni=1\n\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn \n\nTherefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn \n\nn\n\nn(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn \n\ni=1\n\nin probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0\n\n(10)\n\nn \n\nin probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand\n\nn\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1\n\nAs per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)\n\nsup\nt BS (0,R)\n\n G n (t) Gn (t) 0\nn \n\nin probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1\n\n1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1\n\n1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2\n\nIt is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby\n\nt n = πTSn ( Zn \n\n 1\nnS Φ(θ )).\n\n19\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\nNow, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn \n\ncentered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that\n\nS\n 1\n\ntn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,\n\n 1\nS\n\nπTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn \n\nalmost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn \n\n 1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,\n\nG n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2\n\n Gn (t n ) +\n\nHence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S\n\nsome Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn \n\nthis convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn \n\n20\n\nV.-E. BRUNEL\n\nwhere Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn \n\n5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ) =\n\n1\nϕ(Xi1 , . . . , Xik , θ).\n\n(nk) 1 i1 <...<ik n\n\nEstimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , . . . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , . . . , xd , θ, for all x1 , . . . , xd , θ Rd .\nThe minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . . . , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators). More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions.\n2. Let E = R and Θ Θ0 = R and k 1. [37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . . , n} of size k. That is, ϕ(x1 , . . . , xk , θ) = x1 +...+x\n θ , for\nk\nall x1 , . . . , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered. Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well.\nNote that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work.\n3. More generally, aggregation of estimators that are based on overlapping subsamples, e.g.,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.\n4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , . . . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0).\nIn higher dimensions, one recovers the empirical covariance matrix of X1 , . . . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd . Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.\n\nASYMPTOTICS OF CONVEX M -ESTIMATION\n\n21\n\n5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )]\n\nn \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).\n\n(k ) 1 i1 < <ik n\nThen,\n\nn(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn \n\nin distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,\n\nS\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn \n\n 1\n\n 1\n\nin distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6\n\nΣ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 , . . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.\n\n22\n\nV.-E. BRUNEL\n\n6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.\nIn large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant. A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e.g., an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner. When\nθ n is simply the sample mean of X1 , . . . , Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss.\nIn the context of U -estimators, we have also let the order k of the U -process be fixed. However,\nit may be relevant to also let k grow with the sample size (e.g., for median-of-means procedures).\nWhile the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
{"method": "hybrid", "num_chunks": 381, "avg_chunk_len": 170.84251968503938, "std_chunk_len": 162.14732414538778, "max_chunk_len": 774, "min_chunk_len": 1, "total_chars": 65091, "compression_ratio": 1.009279316648999, "chunks": ["Asymptotics of constrained\nM -estimation under convexity", "arXiv:2511.04612v1 [math.ST] 6 Nov 2025", "Victor-Emmanuel Brunel", "Abstract: M -estimation, aka empirical risk minimization, is at the\nheart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the\nloss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can\nbe too restrictive or heavy to check.", "Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an\nasymptotic theory for M -estimation with convex loss (which needs not\nbe differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M -estimators depend on an\ninterplay between the loss function and the boundary structure of the\nset of constraints. We extend our results to U -estimators, building on\nthe asymptotic theory of U -statistics.", "Applications of our work include,\namong other, robust location/scatter estimation, estimation of deepest\npoints relative to depth functions such as Oja s depth, etc. Key words and phrases: Constrained M -estimation, empirical risk minimization, convex loss, convex analysis, consistency, asymptotic distribution, U -statistics, metric projections, directional derivatives. .", "1. INTRODUCTION\n1. 1 Preliminaries\nWe consider a sequence X1 , X2 , .", ". . of independent, identically distributed (iid) random variables\ntaking values in some measurable space (E, E) and we denote by P their distribution.", "Let Θ0 Rd\nbe a non-empty set, which can be interpreted as a parameter space. Here, d 1 is a fixed integer\nrepresenting the parameter dimension. Let ϕ E Θ0 R be a function such that ϕ( , θ) is measurable and in L1 (P ), for all θ Θ0 .", "Set Φ(θ) = E[ϕ(X1 , θ)], for all θ Θ0 . The goal of M -estimation (or empirical risk minimization) is\nto estimate a minimizer of Φ when only finitely many samples from P are available. For n 1 and\n1 n\nθ Θ0 , let Φn (θ) = ϕ(Xi , θ).", "For θ Θ, Φ(θ) is called the population risk evaluated at θ, while\nn i=1\nΦn (θ) is the empirical risk based on X1 , . . .", ", Xn . The idea of M -estimation is to use the random\nfunction Φn as a surrogate for Φ and estimate a minimizer of Φ by selecting a minimizer of Φn . When minimization is performed over the whole parameter space Θ0 , we talk about unconstrained\nM -estimation, or simply M -estimation.", "If we minimize Φn on a closed subset Θ of Θ0 , we talk\nabout constrained M -estimation with Θ as the set of constraints. In this work, we are concerned\nwith the latter.", "CREST-ENSAE, victor.emmanuel.brunel@ensae.fr", "1", "2", "V.-E. BRUNEL", "Let Θ Θ be the set of minimizers of Φ on Θ and assume it is not empty. For all n 1, let θ n be a\nminimizer of Φn (provided it exists and can be chosen in a measurable way - see Section 2.2 below).\nStandard asymptotic theory questions (weak or strong) consistency and aims at determining the\nasymptotic distribution of a rescaled version of the M -estimator. That is, does d(θ n , Θ ) converge\n(in probability or almost surely) to zero as n ? Here, d(θ n , Θ ) is simply the distance of θ n", "to the non-empty set Θ . If Θ reduces to a singleton Θ = {θ }, does ρn (θ n θ ) converge in\ndistribution for some rescaling factor ρn and if so, what is the asymptotic distribution?\nn", "It may be convenient to consider, instead of θ n , a near minimizer of Φn , that is, a random variable\nθ n satisfying Φn (θ n ) inf θ Θ Φn (θ) + εn where εn is a (possibly random) small enough error term. For simplicity, here, we only study the properties of exact empirical risk minimizers. Our main working assumption is that the loss function is convex in its second argument.", "That\nis, Θ0 and Θ are convex sets and ϕ(x, ) is convex on Θ0 for P -almost all x E. Relevant examples\ninclude:\n1. Location estimation: E = Θ0 = Rd , ϕ(x, θ) = (x θ) for some convex function Rd R.", "For instance, if is the squared Euclidean norm, we recover mean estimation. If is the\nEuclidean norm, we recover geometric median estimation. If (x) = x (1 2α)u x, where\nα (0, 1) and u Rd with u = 1 are fixed ( being the Euclidean norm), we recover\ngeometric quantile estimation (e.", "g. , if d = 1 and u = 1, Θ is simply the set of α-quantiles\nof P ). Huber s M -estimators, adding robustness to mean estimators, correspond to the loss\n (x) = hc ( x ), x Rd , where for all t 0, hc (t) = t2 if t c, hc (t) = 2ct c2 if t > c and c > 0\nis a given, tuning parameter.", "2. Location estimation on matrix spaces: Let E = Θ0 = Sd+ be the space of d d symmetric,\npositive semi-definite matrices. There are several ways of averaging positive definite matrices,\nbeyond simply taking their arithmetic mean (i.", "e. , their standard linear average). A simple\nexample is that of the harmonic mean, which is simply the inverse of the linear average of\nthe inverses (if the matrices are positive definite).", "More involved ways include (again for\npositive definite matrices) the Karsher mean, which, in the case of 2 such matrices, reduces to\ntheir geometric mean [7]. In the context of optimal transport, a large body of literature has\nbeen interested in the Bures-Wasserstein mean of positive definite matrices, which is related\nto Wasserstein barycenters on the set of Gaussian distributions [2, 54]. In fact, it is shown\nin [30, Lemma A.", "5] that the Bures-Wasserstein mean is the solution to a convex optimization\nproblem. Hence, as it is done in [30], the Bures-Wasserstein barycenter of iid, random, positive\n(semi-)definite matrices can be analyzed under the prism of M -estimation with convex loss,\nand our results also allows to consider the constrained case, as well as robust alternatives to\nBures-Wasserstein barycenters (such as the Bures-Wasserstein median, see [2]). 3.", "Linear regression (here, data are rather denoted as pairs (Xn , Yn ) Rd R, n 1): E = Rd R,\nΘ = Rd , ϕ((x, y), θ) = (y θ x) for some R R (which, again in our context, we assume\nto be convex). If (t) = t2 , we recover least squares estimation. If (t) = t , this is median\nregression, etc.", "In all these examples, we can take Θ0 = Θ = Rd (or Sd+ ), corresponding to unconstrained estimation, but we could also assume that Θ is a closed, strict subset of Θ0 . Perhaps the simplest\nexample is the case when E = Θ0 = Rd , Θ Rd is a compact convex subset and ϕ(x, θ) = x θ 2 . In\nthat case, it is easy to check that θ = πΘ (E[X]) and θ n = πΘ (X n ) are the unique minimizers of Φ\nand Φn respectively, where X n = n 1 ni=1 Xi and πΘ is the metric projection on Θ.", "Of course, this\nexample can be studied with elementary tools, but it is worth keeping it in mind as an illustration\nof our results, in order to fix ideas.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "3", "Typically, proving consistency and finding the asymptotic distribution of M -estimators require\nsome tools from the theory of empirical processes and imposes some smoothness of the loss function\nϕ in its second argument. Moreover, it is often assumed that the partial derivatives of ϕ, with respect\nto its second argument, are locally dominated, allowing the use of dominated convergence to swap\nderivatives and expectations in the analysis. In our context, the full power of convexity comes in\nthrough fairly elementary convex analysis and allows to completely avoid such common technical\nassumptions.", "1. 2 Related works\nM -estimation is a quintessential problem in statistical inference (maximum likelihood estimation\nbeing a particular instance in general) and, as a particular case, constrained M -estimation. Asymptotic theory of statistical estimation has been overlooked in the era of high-dimensional\ndata and models.", "Yet, it provides benchmarks for non-asymptotic theory and asymptotic approximations produce less conservative inference than non-asymptotic approaches, and they are relevant\nwhen the data set contains a lot of samples and their dimension is not too large. Asymptotic theory of M -estimators is well understood when the loss function is smooth and\nsatisfies local domination properties [31,55,56]. Under similar smoothness and domination assumptions, [18] also derived asymptotic properties in the constrained case, when the set of constraints is\na regular closed set and the population minimizer is a local minimum of the population risk in the\nambient space.", "See also [34] for inference on constrained statistical problems and [26,47] for special\ncases. Recently, [35] drew connections between the statistical error of constrained M -estimation\nand the statistical dimension of the constrained set, building on [11, 46] in linear regression and\nGaussian sequence models. Even though these connections belong to the non-asymptotic world, we\nalso discuss such connections at infinitesimal scales in the remarks following Theorem 7 below.", "When the loss function is convex, [19] proved asymptotic normality, only requiring the population\nrisk (that is, Φ) being twice differentiable at the (unique) population minimizer, with positive\ndefinite Hessian at that point - convexity allowing to avoid any local domination assumption. [40]\nproved further asymptotic expansions of the statistical error under stronger smoothness assumptions\nof convex the loss. Asymptotics of penalized M -estimators have also been established [24], in particular for penalized\nregression (such as Lasso) [27].", "In the context of high dimensional linear regression and classification, some recent work has also\ntackled the asymptotics of penalized M -estimators and bagged penalized M estimators in growing\ndimension (that is, when the dimension d also diverges with the sample size) [5, 6, 29]. Related to\nthis line of work are the high-dimensional central limit theorems of [12, 15] which correspond to\nthe squared Euclidean loss in the context of M -estimation. To the best of our knowledge, similar\nhigh-dimensional central limit theorems have not been tackled for general M -estimators.", "This work is not concerned with penalized M -estimation. Indeed, even though penalized and\nconstrained optimization problems are related through Lagrangian functions, in penalized statistical\nproblems, it is standard to let the penalty depend on the sample size in order to enforce some\nregularization and achieve optimal performance, although here, we only consider fixed constraint\nsets, independently of the sample size. 1.", "3 Outline\nIn Section 2, we give some key lemmas that we use in our main results. Section 2. 1 gathers some\nresults about convex functions and sequences of convex functions, which we chose to highlight\nin the first part of this work because they are essential to build the intuition behind the theory.", "In Section 2. 2, which is much more theoretical and could be skipped at first, we deal with the", "4", "V.-E. BRUNEL", "existence of a measurable empirical minimizer, based on results that guarantee the existence of\nmeasurable selections. Section 3 focuses on consistency of convex M -estimators and Section 4 deals\nwith asymptotic distributions of M -estimators. We propose an extension to U -estimators with\nconvex loss in Section 5.", "More lemmas about convex functions, convex sets and cones, and metric\nprojections, which are only used for some technical parts of the main proofs, but not essential to\nbuild the intuition, are deferred to the appendix. However, Section C, in the appendix, on directional\ndifferentiability of metric projections onto convex sets, may be of independent interest to the reader. 1.", "4 Notation and standard definitions/assumptions\nHere, we gather all the notation that we use in this work, as well as several simple definitions. 1. In this work, ( , F, P) is a fixed probability space and we assume that all the random variables\nthat we consider are defined on that space.", "We let X1 , X2 , . . .", "be iid random variables with\nvalues in a measurable space E and we let P = X1 #P be their distribution. The set Θ0 is a\nfixed, open, convex subset of Rd and Θ is a closed, convex subset of Θ0 . The loss function\nϕ E Θ0 R is assumed to be measurable in its first argument and convex in its second,\nand to satisfy ϕ( , θ) L1 (P ) for all θ Θ0 .", "We let Φ(θ) = E[ϕ(X1 , θ)] for all θ Θ0 (referred\nto as population risk ) and for all n 1, ω and θ Θ0 , Φn (ω, θ) = n 1 ni=1 ϕ(Xi (ω), θ)\n(referred to as empirical risk ). For simplicity, unless this amount of precision is needed, we\nsimply write Φn (θ) and skip the dependence on ω . 2.", "The power set of a non-empty set A is denoted by P(A). 3. Given a subset G Rd , we denote by int(G) its interior, cl(G) its closure and G = cl(G) \nint(G) its boundary.", "4. Any symmetric, positive definite matrix S Rd d yields a scalar product by setting, for\n1/2\nx, y Rd , x, y S = x Sy. The associated Euclidean norm is given by x S = x, x S for all\nx Rd .", "The corresponding Euclidean ball with center x Rd and radius r 0 is denoted by\nBS (x, r). 5. Given a vector u Rd , the linear subspace of Rd that is orthogonal to u with respect to , S\nis denoted by u S : If u = 0, u S = Rd and if u 0, u S is some linear hyperplane.", "When L Rd ,\nwe denote by L S the linear subspace of Rd that is orthogonal to L with respect to , S . S\n= {x C \n6. For a set C Rd , a vector u Rd and a real number t R, we denote by Cu,t\nS\nS\n u, x S = t}, which may be empty.", "When t = 0, we simply write Cu = Cu,t . 7. The distance of a point x Rd to a closed set C Rd with respect to the Euclidean norm\nassociated with S is denoted by dS (x, C) = miny C x y S .", "8. The metric projection onto a non-empty, closed convex set C Rd with respect to , S is\nS\nS\n: For all u Rd , πC\n(u) is the unique minimizer of the map t C t u 2S . In\ndenoted by πC\nS\nparticular, dS (u, C) = u πC\n(u) S .", "d\n9. Let G R be a non-empty, closed, convex set and x0 G. The tangent cone to G at x0 is\nthe set of all t Rd such that x0 + εt G for all small enough ε > 0.", "It is a convex cone,\nnot necessarily closed. Its closure is called the support cone to G at x0 . Let S Rd d be\nsymmetric, positive definite.", "The normal cone to G at x0 with respect to S is the set of all\nt Rd satisfying t, x x0 S 0 for all x G. It is a closed, convex cone. When there is no\nmention of a matrix S, it is implicitly assumed to be the identity matrix.", "10. The support function of a non-empty convex set C Rd is the map hC Rd R { }\ndefined by hC (t) = supu C u t. If t 0, it is the largest (signed) distance from the origin to a\nhyperplane orthogonal to t and that is tangent to C.", "It is easy to check that hC is a sublinear\nfunction (that is, positively homogeneous and convex). If C is bounded, then hC only takes\nfinite values. See, e.", "g. , [49, Section 1. 7.", "1].", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "5", "11. In all notation above, when S is the identity matrix, we drop the subscript or superscipt S\nand simply write, for instance, x , B(x, r), u , Cu , πC , etc. 12.", "Given a set C Rd and a function f C R, the set of minimizers (resp. maximizers) of f\non C is denoted by Argminy C f (y) (resp. Argmaxy C f (y)).", "This set may be empty. When\nthis set is a singleton, we denote by argminy C f (y) (resp. argmaxy C f (y)), with lower case\n a , the unique element of that set.", "13. Let f be a function defined on a subset of Rd , with values in Rp for some p 1 (for us,\nin practice, p = 1 or d). Then, given a point x in the interior of the domain of f , we say\nthat f has a directional derivative at x in the direction t Rd if and only if the quantity\nε 1 (f (x + εt) f (x)) has a limit as ε 0, with ε > 0.", "In that case, we denote this limit by\nd+ f (x; t). Note that if f has directional derivatives at x Rd , then it must be continuous\nat x. Moreover, the map d+ f (x; ) is automatically measurable, since the limit can be taken\nalong the sequence ε = 1/k, k 1.", "If the ratio ε 1 (f (x + εt) f (x)) converges uniformly in t on\nall compact subsets of Rd , we say that f has directional derivatives at x in Hadamard sense. This is equivalent to requiring that for all t Rd , for all sequences (tn )n 1 converging to t and\nfor all seuqences (εn )n 1 of positive numbers converging to 0, ε 1\nn (f (x + εn tn ) f (x)) has a\n(finite) limit as n (see, e. g.", ", [17, Chapter III]). 14. If f is differentiable at x, we denote by df (x; ) its differential.", "That is, df (x; t) = d+ f (x, t) =\n f (x) t for all t Rd . 15. Given a convex set G0 Rd , when we talk about a convex function on G0 , we always mean\nthat it takes finite values only, i.", "e. , we only consider convex functions f G0 R, which may\nbe the restriction to G of some lower-semicontinuous convex function f Rd R { } whose\ndomain contains G0 . 16.", "We call random convex function any map f G R, where G Rd is some convex set,\nsuch that f ( , t) is measurable for all t G and f (ω, ) is convex for all ω . We could only\nassume that f (ω, ) is convex for P-almost all ω , but this does not bring significantly more\ngenerality. Unless we need to emphasize the dependence on ω explicitly, we rather write f (t)\ninstead of f (ω, t) for simplicity.", "17. The covariance matrix of a random vector X in Rd with two moments is defined as var(X) =\nE[XX ] E[X]E[X] = E[(X E[X])(X E[X]) ]. That is, for all vectors u, v Rd ,\nu var(X)v = cov(u X, v X).", "When S Rd d is symmetric, positive definite, we denote\nby varS (X) = Svar(X)S = var(SX) so that for all vectors u, v Rd , we have the identity\nu varS (X)v = cov( u, X S , v, X S ). This is the matrix representation of the covariance operator of X corresponding to the Euclidean structure defined by S. 18.", "For all vectors u Rd and symmetric, positive semi-definite matrices V Rd d , we denote by\nNd (u, V ) the d-variate Gaussian distribution with mean u and covariance matrix V . 2. KEY LEMMAS ABOUT DETERMINISTIC AND RANDOM CONVEX FUNCTIONS\n2.", "1 On the behavior of convex functions and sequences of convex functions\nFirst, we state a minimum principle for convex functions, which we will use a few times in the\nnext sections. Lemma 1. Let G0 Rd be an open convex set and G G0 be a closed convex subset.", "Let\nf G0 R be a convex function and K G0 be any compact, convex set. If mint K G f (t) > f (t0 )\nfor some t0 K G, then Argmin f (t) K and it is not empty. t G", "Remark 1.", "Recall that a convex function defined on an open convex set is automatically", "6", "V.-E. BRUNEL", "continuous on that set [48, Theorem 10. 1], hence, it automatically reaches its bounds on any\ncompact set. The phrasing of this lemma is a bit technical, but a simpler version, when G = G0 = Rd , says\nthat if f has one value inside K that is smaller than all values taken on K, then, it has at\nleast one minimizer, and they all lie in K.", "We need this slightly more technical statement in\norder to deal with constrained M -estimation later. Proof. Fix some arbitrary t G K and let us show that necessarily, f (t) > f (t0 ).", "Set ϕ λ \n[0, 1] f (t0 + λ(t t0 )), which is a convex function. First, note that t0 K (or else, t0 would be in\n K G so f (t0 ) min K G f , which would contradict the assumption). Hence, there must be some\nλ (0, 1) such that t0 + λ (t t0 ) K.", "Moreover, since both t0 and t are in G, t0 + λ (t t0 ) G. Therefore, by assumption, ϕ(λ ) > ϕ(0). Hence, convexity of ϕ implies that it must be increasing\non [λ , 1], yielding that ϕ(1) ϕ(λ ) and hence, that ϕ(1) > ϕ(0).", "That is, f (t) > f (t0 ). Therefore, the minimizers (if any) of f on G must be contained in K. Finally, there must be at\nleast one such minimizer since f is continuous on the compact set K G.", "In the main statistical results presented in the next sections, Lemma 1 will be used to localize\nempirical minimizers of Φn . The second key result is due to Rockafellar and shows that, for sequences of convex functions,\nuniform convergence can be deduced from pointwise convergence on a dense subset. From this\nlemma, we will derive two probabilistic corollaries.", "Lemma 2. [48, Theorem 10. 8] Let G0 Rd be an open convex set and f, f1 , f2 , .", ". . be convex\nfunctions on G0 .", "Assume that there is a dense subset C of G0 such that for all t C, fn (t) f (t). Then, fn converges uniformly to f on all compact subsets of G0 . An important consequence that we will use extensively is the following corollary.", "Corollary 1. Let f, f1 , f2 , . .", ". be random convex functions defined on an open convex set\nG0 Rd . Assume that fn (t) f (t) almost surely (resp.", "in probability) for all t G0 . Then, for\nn", "all compact sets K G0 , supK fn f 0 almost surely (resp. in probability).\nn", "Proof. Let us prove the statement for the almost sure convergence and the convergence in\nprobability separately.\nAlmost sure convergence.\nLet C be a dense and countable subset of G0 . By assumption, for each t C, it holds with\nprobability one that fn (t) f (t). Since C is countable, this implies that with probability 1,\nn", "fn (t) f (t) for all t C simultaneously. Hence, by Lemma 2, with probability 1, fn converges\nn \nuniformly to f on all compact subsets of G0 .\nConvergence in probability.\nAgain, let C be a dense and countable subset of G0 and fix a compact subset K of G0 . Our\ngoal is to show that Zn = supt K fn (t) f (t) 0 in probability. It is necessary and sufficient\nn", "to show that every subsequence of (Zn )n 1 has a further subsequence that converges to 0 almost\nsurely [13, Section 3.3, Lemma 2]. With no loss of generality (since we could just renumber the\nterms of the sequence), let us prove that (Zn )n 1 has a subsequence that converges to 0 almost\nsurely. Denote by t1 , t2 , . . . the elements of C.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "7", "By assumption, fn (t1 ) f (t1 ) in probability, so it has a subsequence that converges almost\nn", "surely. That is, there is an increasing map ψ1 N N such that fψ1 (n) (t1 ) f (t1 ) almost\nn \nsurely. Similarly, (fψ1 (n) (t2 ))n 1 being a subsequence of (fn (t2 ))n 1 , it converges almost surely to f (t2 )\nand thus has a further subsequence (fψ1 (ψ2 (n)) (t2 ))n 1 that converges almost surely to f (t2 ).", "By\ninduction, one can construct a sequence of increasing maps ψp N N , p 1, such that for all\nintegers p 1, fψ1 . . .", "ψp (n) (tp ) converges to f (tp ) almost surely. Let ψ(n) = ψ1 . .", ". ψn (n), for all\nn 1. This is an increasing map; Let us prove that Zψ(n) 0 almost surely, which will prove\nn \nthe lemma.", "First, note that with probablity 1, fψ1 . . .", "ψp (n) (tp ) converges to f (tp ) simultaneously for all p 1. Second, for all p 1, (fψ(n) (tp ))n 1 is a subsequence of (fψ1 . .", ". ψp (n) (tp ))n 1 (except maybe for the\nfirst p terms of the sequence). Hence, fψ(n) (tp ) f (tp ) for all p 1, almost surely.", "The rest\nn", "follows from the first part of the proof (the case of almost sure convergence). In fact, we can also derive a similar corollary for Lp convergence, for any p 1. We defer it to\nthe appendix (Section E), because we only use it to formulate an open question, see the end of\nSection 4.", "2). 2. 2 On the existence of measurable minimizers and measurable subgradients\nThe existence of minimizers of a random convex function can often be established quite easily\n(for instance, if the function is coercive).", "Same for subgradients since any convex function defined\non an open convex set has at least one subgradient at any point of that set. However, the existence\nof a measurable minimizer or subgradient is much less trivial and relies on the theory of measurable\nselections. 2.", "2. 1 Measurable selections\nDefinition 1. Let Γ P(Rd ) be a multifunction, that is, a function that maps any ω \nto some non-empty set Γ(ω) Rd .", "A measurable selection of Γ is a measurable map γ Rd\nsuch that for all ω , γ(ω) Γ(ω). There are numerous theorems that guarantee the existence of measurable selections in various\nsetups, see [21,38]. The one that we will need is the following, that follows from combining Theorems\n3.", "2 (ii), 3. 5 and 5. 1 of [21].", "Denote by C the collection of all non-empty, closed subsets of Rd . Lemma 3. Let Γ C be a multifunction.", "Assume that for all compact sets K Rd , the\nset {ω Γ(ω) K } is measurable (that is, it belongs to the σ-algebra F ). Then, Γ has a\nmeasurable selection. A multifunction satisfying this property above is called C-measurable (C as in compact , the\ntest sets K used in Lemma 3 being compact).", "2. 2. 2 Measurable empirical risk minimizers\nFrom Lemma 3, we obtain the following result, which will guarantee the existence of a measurable\nempirical risk minimizer for large enough n, and which will, at the same time, yield its strong\nconsistency.", "Theorem 1. Let f, f1 , f2 , . .", ". be random convex functions defined on an open convex set G0 Rd\nsuch that for all t G0 , fn (t) f (t) almost surely. Let G G0 be a closed, convex set.", "Assume\nn", "8", "V.-E. BRUNEL", "that G = Argmint G f (t) is non-empty and compact. Then, there exists a sequence (tn )n 1 of\nrandom variables with values in G such that with probability 1, tn is a minimizer of fn on G for\nall large enough n. Moreover, d(tn , G ) 0 almost surely.\nn", "Proof. For n 1, let Mn = Argmint G fn (t), possibly empty. We proceed in two steps. First,\nwe prove that with probability 1, Mn is non-empty for all large enough n. Second, we use the\nmeasurable selection to obtain such a sequence (tn )n 1 .\nStep 1. Note that if G is compact, then Mn for all n 1, since fn is convex, hence continuous,\non the open set G0 .\nFirst, Corollary 1 yields that fn converges uniformly to f on any compact subset of G0 , almost\nsurely. Fix some arbitrary, small enough ε > 0 such that G ε = {t Rd d(t, G ) ε}. This set is\ncompact, so\n(1)", "sup fn (t) f (t) 0.\nn", "t G ε G", "Let f = mint G f (t) be the smallest value of f on G (note that f is measurable, since it can\nbe written as the infimum of f (t) for t ranging in a countable, dense subset of G). Convexity of f\non the open set G0 implies its continuity. Therefore, η = mint G ε G f (t) f > 0.\nThen, the following holds with probability 1: For all sufficiently large integers n and for all\nt G ε G,\nfn (t) f (t) η/3", "f + η η/3", "by (1)\nby definition of η", "fn (t ) η/3 + η η/3", "again by (1)", "= fn (t ) + η/3 > fn (t ).\nTherefore, by Lemma 1, it holds with probability 1 that, for all large enough integers n 1,\n(2)", "Mn G ε .", "Mn if Mn \nStep 2. Now, fix an arbitrary element t0 G. For all integers n 1, let Γn =", "{t0 } otherwise. Let us prove that Γn has a measurable selection, for all n 1. Since Mn is always closed (by\ncontinuity of fn ), Γn is always non-empty and closed, so by Lemma 3, it is sufficient to check that\nfor each n 1, the multiset function Γn C is C-measurable in order to guarantee the existence\nof a measurable selection.", "Fix n 1 and let K Rd be any compact set and let us show that the set {ω Γn (ω) K }\nis a measurable set. First, rewrite {ω Γn (ω) K } = {ω Mn (ω) K } {ω Mn (ω) = , t0 K}. Since fn (ω, )1 is continuous for every ω , the first set in this union can be rewritten as {ω \ninf t G fn (ω, t) = inf t K G fn (ω, t)}.", "Again, using continuity of fn (ω, ) for all ω , we can rewrite\ninf t G fn (ω, t) and inf t K G fn (ω, t) as inf t G 1 fn (ω, t) and inf t G 2 fn (ω, t) respectively, where G1\nand G2 are dense, countable subsets of G and K G respectively. Therefore, both inf t G fn (ω, t)\nand inf t K G fn (ω, t) are measurable (as maps from to R { }) and we obtain that {ω \nMn (ω) K } F. 1", "recall that above, we only wrote fn (t) instead of fn (ω, t) for simplicity.", "9", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, {ω Mn (ω) = , t0 K} is empty if t0 K, which is measurable. If t0 K, it reduces to\nthe set {ω Mn (ω) = }, which can be decomposed as\n{ω Mn (ω) = } =", "{ω", "p N q p+1", "min\nt G B(t0 ,q)", "fn (ω, t) <", "min\nt G B(t0 ,q)", "fn (ω, t)}", "which, therefore, is also measurable.\nFinally, Lemma 3 implies the existence of a sequence (tn )n 1 of random variables such that for\nall n 1, tn Γn . Furthermore, by Step 1 of this proof, we also obtain that with probability 1,\ntn Mn for all large enough n.\nStep 3. Finally, following the reasoning of Step 1, (2) yields that for all ε > 0, it holds, with\nprobability 1, that d(tn , G ) ε for all large enough n. That is, d(tn , G ) 0 almost surely.\nn", "2.2.3 Measurable subgradients\nNow, we apply Lemma 3 to show the existence of measurable subgradients for random convex\nfunctions. Recall that for a convex function f defined on a convex set G0 Rd , a subgradient of f\nat a point t0 G0 is any vector u Rd such that\nf (t) f (t0 ) + u (t t0 ),", "t G0 .", "We denote by f (t0 ) the collection of all subgradients of f at t0 . If t0 int(G0 ), then f (t0 ) is nonempty, compact and convex by Lemma 5. In particular, if G0 is open, then f has subgradients at\nevery point of G0 .", "Now, if f is a random convex function, the existence of a measurable subgradient\n(i. e. , that is chosen in a measurable way) at t0 int(G0 ) is granted by the following theorem.", "Theorem 2. Let f be a random convex function defined on a convex set G0 Rd and let\nt0 int(G0 ). Then, f has a measurable subgradient at t0 .", "Proof. Let Γ = f (t0 ) be the set of subgradients of f at t0 (that is, for all ω , Γ(ω) =\n (f (ω, )) (t0 )). Since t0 int(G0 ), Γ only takes non-empty values.", "Moreover, by Lemma 5, it\nalways takes closed values, so Γ is a C-valued multifunction. Hence, it is sufficient to check that it\nis C-measurable in order to apply Lemma 3. Let K Rd be any arbitrary compact set.", "Lemma 4 yields that Γ K if and only if there\nexists u K with the property that supt B(t0 ,ε) (u (t t0 ) f (t) + f (t0 )) 0 where ε > 0 is any\nsmall enough positive number satisfying that B(t0 , ε) int(G0 ). Since f is convex, it is continuous\non int(G) and, hence, on B(t0 , ε). Let C be a fixed dense, countable subset of B(t0 , ε).", "Then,\nΓ K if and only if there exists u K for which supt C (u (t t0 ) f (t) + f (t0 )) 0. Let\nh(ω, u) = supt C (u (t t0 ) f (ω, t) + f (ω, t0 )), for all ω and u Rd (again, here, we emphasize\nthe dependence on ω for clarity, even though it was omitted above). First, note that for all\nu Rd , h( , u) is measurable, as the supremum of a countable family of measurable functions.", "Second, for all ω , the function h(ω, ) is convex as the supremum of affine functions, and it\nonly takes finite values: Indeed, C B(t0 , ε) is bounded and f (ω, ) is continuous on B(t0 , ε). Hence, h(ω, ) is continuous on Rd . Therefore, since K is compact, Γ(ω) K if and only if\nminu K h(ω, u) 0, if and only if inf u K h(ω, u) 0, where K is a fixed, countable, dense subset of\nK.", "Therefore, we obtain {ω Γ(ω) K } = {ω inf h(ω, u) 0} which is measurable,\nu K", "since inf u K h( , u) is a measurable map.", "10", "V.-E. BRUNEL", "Finally, let us state an incredibly simple yet powerful result that shows that for convex functions,\nthere is no need to apply any dominated convergence theorem in order to swap expectations and\n(sub-) gradients. It is very easy to check that if f1 and f2 are two convex functions on a convex set\nG0 Rd , then for all t0 G0 , f1 (t0 ) + f2 (t0 ) (f1 + f2 )(t0 )2 . The following lemma shows that\nthis fact still holds for generalized sums of convex functions.", "Theorem 3. Let f be a random convex function defined on a convex set G0 Rd . For all\nt int(G0 ), let g(t) be a measurable subgradient of f at t.", "Let p 1 be a real number and assume\nthat for all t G0 , f (t) Lp (P) and denote by F (t) = E[f (t)]. Then, F is a convex function and\nfor all t G0 , g(t) Lp (P) and\nE[g(t)] F (t). Proof.", "Fix t0 int(G0 ) and let g(t0 ) be a measurable subgradient of h at t0 (the existence of\nwhich is guaranteed by Theorem 3). In order to check that g(t0 ) Lp (P), it is necessary and sufficient\nto check that each of its d coordinates are in Lp (P) or, equivalently, that for all v Rd , g(t0 ) v p is\nintegrable. Fix an arbitrary v Rd and let ε > 0 be such that t0 + εv and t0 εv are in G0 (such an\nε exists because t0 int(G0 )).", "Then, by definition of subgradients, g(t0 ) v ε 1 (f (t0 + εv) f (t0 ))\nand g(t0 ) v ε 1 (f (t0 εv) f (t0 )). That is,\n g(t0 ) v max(ε 1 (f (t0 + εv) f (t0 )), ε 1 (f (t0 εv) f (t0 ))). Since the right hand side is in Lp (P) by assumption, so is g(t0 ) v.", "The vector v was arbitrary, so\nwe conclude that g(t0 ) Lp (P). Now, for the rest of the proof, simply note that, again, by definition of subgradients,\nf (t) f (t0 ) + g(t0 ) (t t0 )\nholds for all t G0 . Taking the expectation, which is linear, yields that\nF (t) F (t0 ) + E[g(t0 )] (t t0 )\nwhich concludes the proof.", "Remark 2. In fact, to obtain that g(t0 ) Lp (P), it would have been sufficient to assume that f (t) Lp (P)\nfor all t B(t0 , ε), for any arbitrary, small enough ε > 0. As a consequence of Theorem 3, if F is differentiable at t0 int(G0 ), then E[g(t0 )] does not\ndepend on the choice of the measurable selection g(t0 ) and it is automatically equal to F (t0 )\n(since F (t0 ) is the only subgradient of F at t0 , in that case).", "In fact, Lemma 12 shows that if F is differentiable at some t0 int(G0 ), then f is almost surely\ndifferentiable at t0 , so in that case, any measurable selection g(t0 ) must satisfy g(t0 ) = f (t0 )\nalmost surely. To the best of our knowledge, the converse inclusion to Theorem 3 is unknown: Can all\nsubgradients of F at t0 be written as E[g(t0 )] for some measurable g(t0 ) f (t0 )? 2", "The other inclusion is also true if G0 has non-empty interior but, perhaps surprisingly, requires a nontrivial\nargument.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "11", "3. CONSISTENCY\nConsistency of empirical risk minimizers with a convex loss function is automatically granted in\na strong sense, thanks to Lemma 1 which allows to localize the M -estimator, for large enough n, in\nan arbitrarily small neighborhood of the set of population minimizers with probability 1. In what\nfollows, we consider a sequence (θ n )n 1 of random variables such that with probability 1, for all\nlarge enough n, θ n is a minimizer of Φn on Θ.", "Existence of such a sequence is granted by Theorem 1. Theorem 4. Assume that Θ is compact and non-empty.", "Then, d(θ n , Θ ) 0 almost\nn \nsurely, as n . The proof of this theorem can be found in [19] (the only difference here being that we do not\nassume that Θ = Rd ), and it is a direct consequence of Theorem 1 above. Remark 3.", "Theorem 4 shows that any empirical minimizer becomes, with probability 1, arbitrarily close to the set of population minimizers Θ . A converse statement is generally not true,\nthat is, there can be elements of Θ that may never be approached by any empirical minimizer. For\ninstance, let E = Rd , Θ = B(0, 1) and ϕ(x, θ) = x θ.", "Furthermore, assume that X1 has the standard\nnormal distribution. Then, Φ(θ) = E[X] θ = 0 for all θ Θ, so Θ = Θ. However, Φn (θ) = X n θ, so\nwith probability 1, the empirical minimizer is unique, given by θ n = X n / X n .", "4. ASYMPTOTIC DISTRIBUTION\nIn this section, we assume that Argminθ Θ Φ(θ) is a singleton and we denote by θ = argminθ Θ Φ(θ). 4.", "1 Non-differentiable case\nWe first study asymptotic properties of θ n without assuming differentiability of Φ at θ . That\nis, Φ(θ ) may not be not a singleton. The following useful property is fundamental in that case.", "Recall that for a non-empty convex\nsubset C Rd , we denote by hC Rd R { } its support function. Proposition 1. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 .", "Let (ρn )n 1 be any non-decreasing\nsequence of positive numbers diverging to as n . Then, for all θ Θ0 and t Rd ,\nρn (Φn (θ + t/ρn ) Φn (θ)) h Φ(θ) (t)\nn", "in probability.\nProof. Fix θ Θ0 . For all t Rd , define\n1 n\nt g(Xi , θ))\nnρn i=1\n1\n ρn (Φ(θ + t/ρn ) Φ(θ) t E[g(X1 , θ)]) .\nρn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ)", "Write Fn (t) = ni=1 (Zi,n E[Zi,n ]) where Zi,n = ρnn (ϕ(Xi , θ + t/ρn ) ϕ(Xi , θ) (1/ρn )t g(Xi , θ)),\nfor all i = 1, . . . , n. Convexity of ϕ(Xi , ) yields that 0 Zi,n n1 t (g(Xi , θ + t/ρn ) g(Xi , θ)), for\nall i = 1, . . . , n. By Theorem 3, each Zi,n , i = 1, . . . , n, is square-integrable. Hence, taking the square\nand the expectation in the last display,\n2\nE[Zi,n\n]", "1\nE[Yn2 ]\nn2", "12", "V.-E. BRUNEL", "where Yn = t (g(X1 , θ + t/ρn ) g(X1 , θ)). Since (ρn )n 1 is non-decreasing, Lemma 11 implies that\n2\nthe sequence (Yn )n 1 is non-increasing, yielding that E[Zi,n\n] n12 E[Y12 ] and, by independence of\nX1 , X2 , . . .,\nn\nn\nn\nE[Y12 ]\n2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] \n 0.\nn \nn\ni=1\ni=1\ni=1\nWe conclude that Fn (t) 0 in L2 and, hence, in probability. Now, rewrite Fn (t) as\nn", "Fn (t) = ρn (Φn (θ + t/ρn ) Φn (θ))\n(3)", "1 n\n t ( g(Xi , θ) E[g(X1 , θ)])\nn i=1", "(4)", "ρn (Φ(θ + t/ρn ) Φ(θ)) .", "The law of large numbers yields that the term (3) converges to 0 in probability, and the term in\n(4) goes to d+ Φ(θ; t) as n . The result then follows from Lemma 9. As a consequence, we obtain the following theorem.", "Theorem 5. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that 0 int( Φ(θ )). Then, θ n = θ \nwith probability going to 1 as n .", "Note that the assumption that 0 int( Φ(θ )) readily implies that θ must be the unique\nminimizer of ϕ on Θ and even on Θ0 . It also implies that Φ is not differentiable at θ . Proof.", "Let (ρn )n 1 be any non-decreasing sequence of positive numbers diverging to as\nn . Since Θ0 is open, we can find r > 0 such that B(θ , r) Θ0 . For all n 1, denote by\nTn = {t Rd θ + t/ρn Θ} = ρn (Θ θ ).", "Finally, set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )), for all\nt Rd such that θ + t/ρn Θ0 . By definition of θ n , t n = ρn (θ n θ ) is a minimizer of Gn on Tn for\nall large enough n, with probability 1. Now, fix ε > 0.", "Combining Proposition 1, Corollary 1 and Lemma 9, we get\nsup Gn (t) h Φ(θ ) (t) 0\nn", "t B(0,ε)", "in probability (note that B(0, ε) ρn (Θ0 θ ) for all large enough integers n). Now, since 0 \nint( Φ(θ )), the quantity η = minu Rd u =1 h Φ(θ ) (u) is positive.\nAssume that n is large enough so supt B(0,ε) Gn (t) h Φ(θ ) (t) εη/2 with probability at least\n1 ε. When this inequality is satisfied, we get that, for all t Tn with t = ε,\nGn (t) h Φ(θ ) (t) εη/2\n= εh Φ(θ ) (t/ε) εη/2\n εη εη/2", "by positive homogeneity of h Φ(θ )", "by definition of η", "> εη/2\n> 0 = Gn (0)\nyielding, thanks to Lemma 1, that t n cannot be larger than ε. Hence, we have shown that\nfor all large enough n, it holds with probability at least 1 ε that ρn (θ n θ ) ε. That is,\nρn (θ n θ ) 0 in probability. Since this must hold for any positive, non-decreasing sequence\nn", "(ρn )n 1 diverging to as n , Lemma 25 implies the desired statement.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "13", "Let C be the support cone to Θ at θ . Recall that the first order condition (Lemma 10) yields\nthat C h 1\n Φ(θ ) ([0, )). The next result extends Theorem 5.", "Theorem 6. Assume that ϕ( , θ) L2 (P ) for all θ Θ0 and that h Φ(θ ) (t) > 0 for all t C {0}. Then, with probability going to 1 as n , θ n = θ .", "The assumption of the theorem is that the two closed, convex cones C and {t Rd h Φ(θ ) (t) 0}\nhave a trivial intersection. Note that, by the first order condition at θ , this intersection must always\nbe included in the boundary of C. In other words, the assumption of the theorem is that all (nonzero) vectors in C are directions of strict, linear increase of the population risk Φ.", "Proof. A consequence of the assumption of the theorem is that for all ε > 0, {t C h Φ(θ ) (t) \nε} is compact. Indeed, it is closed, since C is closed and h Φ(θ ) is continuous.", "Moreover, the set {t \nC t = 1} is compact, so by continuity of h Φ(θ ) , there is some t0 C with t0 = 1 satisfying, for all\nt C {0}, h Φ(θ ) (t) t h Φ(θ ) (t0 ). The assumption of the theorem implies that h Φ(θ ) (t0 ) > 0. Finally, {t C h Φ(θ ) (t) ε} is bounded, since it is included in B(0, ε/h Φ(θ ) (t0 )).", "Now, let (ρn )n 1 be an arbitrary non-decreasing sequence of positive numbers, diverging to as\nn and fix ε > 0. Proposition 1, Corollary 1 and Lemma 9, yield that supt C h Φ(θ ) (t) ε Gn (t) \nh Φ(θ ) (t) 0 in probability, where we set Gn (t) = ρn (Φn (θ + t/ρn ) Φn (θ )) as in the proof\nn", "of Theorem 5. Let n be large enough so supt C h Φ(θ ) (t) ε Gn (t) h Φ(θ ) (t) ε/2 with probability\nat least 1 ε. Then, with probability at least 1 ε, it holds simultaneously for all t Tn = ρn (Θ θ )\nwith h Φ(θ ) (t) = ε, that\nGn (t) h Φ(θ ) (t) ε/2 = ε/2 > 0 = Gn (0)\nso, by Lemma 1, any minimizer t n of Gn on Tn satisfies h Φ(θ ) (t n ) ε. In particular, we obtain,\nfor all large enough n, that with probability at least 1 ε,\n0 h Φ(θ ) (ρn (θ n θ )) = ρn h Φ(θ ) (θ n θ ) ε\nwhere the first inequality follows from the first order condition for Φ at θ (Lemma 10). That\nis ρn h Φ(θ ) (θ n θ ) 0. Since the sequence (ρn )n 1 was arbitrary, Lemma 25 yields that\nn", "h Φ(θ ) (θ n θ ) = 0 with probability going to 1 as n . Since θ n θ C, this means that\nθ n θ = 0 with probability going to 1 as n , which is the desired statement.\nRemark 4. Results of this section rely on Proposition 1, which imposes square-integrability of\nthe loss function. We do not know whether the same results could be proved under weaker assumptions.\nNow, to obtain a more precise asymptotic description of θ n when Φ is differentiable at θ (this\ncould be the case in Theorem 6, with Φ(θ ) t > 0 for all t C {0}, but not in Theorem 5), we\nwill assume the existence of second order derivatives for Φ at θ . This is the object of the next\nsection.\n4.2 Differentiable case\nLet us first state the main result of this section.\nTheorem 7.\nfollowing:", "Let g E Θ0 Rd be a measurable selection of subgradients of ϕ. Assume the", "14", "V.-E. BRUNEL", "(i) Φ is twice differentiable at θ and S = 2 Φ(θ ) is positive definite;\n(ii) g( , θ ) L2 (P );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(g(X1 , θ )). Remark 5 (on the assumptions of the theorem). (i) Second differentiability of Φ at θ is not a strong restriction, since all convex functions are\ntwice differentiable almost eveywhere in the interior of their domains [1].", "The assumption\nthat 2 Φ(θ ) is definite positive is made in order to obtain n 1/2 convergence rate. This\nassumption could be relaxed, yielding slower rates under further, technical assumptions on\nhigher order derivatives on Φ. In this work, we choose to focus on the n 1/2 rate because it\nonly requires minimal, easy to check, non-restrictive smoothness assumptions.", "(ii) Existence of the map g is guaranteed by Theorem 3. Moreover, the first assumption on Φ\nimplies that it is differentiable at θ , so by Lemma 12, ϕ(X1 , ) is almost surely differentiable\nat θ yielding that g(x, θ ) = (ϕ(x, )) (θ ) for P -almost all x E. Theorem 3 also ensures\nthat it is sufficient that ϕ( , θ) L2 (P ) for all θ Θ0 for the second assumption to hold.", "In\nfact, a straightforward adaptation of Theorem 3 shows that it is even enough to only assume\nthat ϕ( , θ) L2 (P ) for all θ in any arbitrarily small neighborhood of θ . Note that this does\nnot require a uniform domination of ϕ or its derivatives/subgradients in any neighborhood of\nθ but, rather, a pointwise integrability condition of order 0 (that is, on ϕ itself ). S\nS\n(iii-a) Directional differentiability of πΘ θ\n is not a strong restriction in the sense that, πΘ θ being non-expansive (see Lemma 13) it is automatically differentiable almost everywhere by\nRademacher s theorem [16, Section 3.", "1. 6, p. 216].", "In the appendix (Section C), we present\nS\nfor a\nseveral sufficient conditions that guarantee the existence of directional derivatives of πK\nconvex set K, at a direction u, which, in practice, are easily checked (e. g. , u K, or u K and\n K is smooth at πK (u), or K is defined by finitely many linear convex constraints, etc.", "). By\nan obvious linear change of variables, it is clear that the existence of a directional derivative\nS\n 1\nof πΘ θ\n Φ(θ ) in a direction z Rd is equivalent to the existence of a directional\n at S\nderivative of πS 1/2 (Θ θ ) at S 1/2 Φ(θ ) in the direction S 1/2 z. Then, simple algebra yields\nthat\nS\n 1\nd+ πΘ θ\n Φ(θ ); z) = S 1/2 d+ πS 1/2 (Θ θ ) ( S 1/2 Φ(θ ); S 1/2 z).", "( S\nRecall that (θ θ ) Φ(θ ) 0 for all θ Θ: This is granted by the first order condition\nat θ (Lemma 10). That is, Φ(θ ) is in the normal cone to Θ at θ or, equivalently,\n S 1/2 Φ(θ ) is in the normal cone to S 1/2 (Θ θ ) at 0. Remark 6 (on the conclusion of the theorem).", "1\nS\n Lemma 20 yields that for any z Rd , d+ πΘ θ\n Φ(θ ); z) CSS 1 Φ(θ ) = C Φ(θ ) where\n ( S\nC is the support cone to Θ at θ . When Φ(θ ) t > 0 for all t C {0} (that is, Φ(θ ) is\nS\n 1\nin the interior of the normal cone to Θ at θ ), C Φ(θ ) = {0}, d+ πΘ θ\n Φ(θ ); ) = 0 so\n ( S", "Theorem 7 yields that n(θ n θ ) 0 in distribution: This was already a (rather weak)\nn \nconsequence of Theorem 6.\n If θ int(Θ), then the first order condition (Lemma 10) yields that Φ(θ ) = 0 and,", "S\nd+ πΘ θ\nn(θ n θ ) Z\n (0; ) is simply the identity map. Therefore, Theorem 7 says that\nn", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "15", "in distribution. In that case, Theorem 4 implies that, with probability 1, for all large enough\nn, θ n int(Θ). Hence, with probability 1, for all large enough n, θ n (the constrained M estimator) is also a solution to the unconstrained optimization problem minθ Θ0 Φn (θ), and\nwe recover Haberman s theorem [19, Theorem 6.", "1]. In fact, Theorem 7 also encompasses the unconstrained case, by taking Θ = Θ0 = Rd . If Θ0\nis a strict open subset of Rd , one can also consider an unconstrained M -estimator θ n on the\nopen set Θ0 , that is, a minimizer of Φn on Θ0 .", "Assume that θ is the unique minimizer of Φ\non the open set Θ0 and let Θ be any closed subset of Θ0 containing θ in its interior (e. g. ,\ntake Θ = B(θ , ε) for any small enough ε).", "Then, a straight adaptation of Theorem 4 yields\nthat θ n θ almost surely, so θ n Θ for all large enough n, with probability 1. That is, θ n\nn \neventually coincides with a constrained M -estimator and, hence, also satisfies the conclusion\nS\nd\nof Theorem 7, with d+ πΘ θ\n (0; ) being the identity map (note that in the case Θ = Θ0 = R ,\nwe necessarily have that Φ(θ ) = 0). If the boundary of Θ is C 2 in a neighborhood of θ (that is, it can be locally represented\nas the graph of a C 2 mapping from Rd 1 to R) and Φ(θ ) 0, then, Lemma 15 yields that", "n(θ n θ ) converges in distribution to a Gaussian distribution that is supported in the linear\nhyperplane that is parallel to the (unique) supporting hyperplane to Θ at θ .\n Lemmas 23 and 24 imply that for all t, t 0 with t > t,\n(5)", "1\n 1\nS\nS\n Φ(θ ); Z) S\n Φ(θ ); Z) S d+ πΘ θ\n d+ πΘ θ\n ( tS\n ( t S", "almost surely. This can be interpreted as follows. First, note that the set Θ can represent\nsome constraints that are imposed by a specific application, or it can represent a model (e.", "g. ,\nif it is believed that the global minimizer of Φ lies in Θ). In the latter case, the model is\nmisspecified if the global minimizer of Φ is not in Θ, that is, if Φ(θ ) 0.", "In other words,\nthe vector Φ(θ ) (or its rescaled version S 1 Φ(θ ) can be used to quantify the amount\nof model misspecification. In that regard, (5) suggests that more misspecification yields better\nasymptotic error (we do not account for any misspecification bias here). In (5), t = 0 can be\nthought of as corresponding to the well-specified case.", "This will be illustrated in the examples\nbelow. As a consequence of Theorem 7, the mean squared error of θ n satisfies\n(6)", "1\nS\n Φ(θ ); Z) 2S ]\nlim inf nE[ θ n θ 2S ] E[ d+ πΘ θ\n ( S\nn", "(we do not know, in general, whether this is in fact an equality, with the lim inf being a\nsimple limit, see the open question below). The right hand side can be interpreted as a local\nmeasure of the statistical complexity of Θ around θ , relative to the (population) loss function\nΦ. The statistical dimension (or Gaussian width) of a non-empty, closed, convex set G Rd\nis measured as E[ πG (Z) 2 ] where Z Nd (0, Id ), see [3] (in our case, we need to account\nfor a scaling given by S 1 and B in the covariance matrix of Z).", "In (6), we do not have a\nprojection, but the directional derivative of a projection. The right hand side of (6) can rather\nbe seen as a statistical dimension at an infinitesimal scale. We can refer, for instance, to [11]\nwho studied least squares under convex constraint, and proved that the statistical dimension\nat a fixed scale drives the statistical error.", "A similar phenomenon has also been studied for\nconstrained M -estimators in a more general setup [35]. Recall, however, that except in specific\nS\n 1\ncases (see Section C in the appendix), d+ πΘ θ\n Φ(θ ); ) is not the projection onto a\n ( S\nconvex set. S\n 1\n It is worth mentioning some further important properties of Π = d+ πΘ θ\n Φ(θ ); ).", "( S\nAs we have noted above, in general, it is not the projection onto a convex cone. Nevertheless,", "16", "V.-E. BRUNEL", "it shares similar properties as the projection onto a convex cone. Indeed, by Lemma 21, it\nsatisfies the following properties:\n Π(λz) = λΠ(z), for all λ 0 and z Rd (positive homogeneity);\n Π(z ) Π(z) S z z 2S (non-expansiveness);\n Π(z ) Π(z), z z S Π(z ) Π(z) 2S 0 for all z, z Rd (firm monotonicity). Note that non-expansiveness is implied by firm monotonicity.", "Such maps satisfying the last\ntwo properties above have been studied extensively [57]. Moreover, [43, Proposition 2. 1] implies\nthat Π is the gradient of a convex function.", "Now, let us look at some applications of Theorem 7. Example 1 (Constrained mean estimation). Let X1 , X2 , .", ". . be iid random vectors with two\nmoments3 and Θ Rd be a non-empty, closed, convex set.", "Consider the loss function ϕ(x, θ) =\n(1/2) x θ 2 , x, θ Rd . Then, θ = πΘ (E[X1 ]) is the unique minimizer of Φ on Θ and θ n = πΘ (X n )\nwhere X n = n 1 (X1 + . .", ". + Xn ), for all n 1. Consistency, which is a consequence of Theorem 4,\nalso follows directly from the strong law of large numbers, together with continuity of πΘ (since it\nis non-expansive).", "For asymptotic normality, we obtain, from Theorem 7, that", "n(θ n θ ) d+ πΘ θ (E[X1 ] θ ; Z) = d+ πΘ (E[X1 ]; Z)\nn", "in distribution, where Z Nd (0, var(X1 )) (in this example, S = Id ). In this simple case, this result\ncan also be obtained using the central limit theorem, combined with the delta method4 . Here, it is clear that misspecification is favorable for the asymptotic error: For instance, if Θ θ \nis a convex cone and E[X1 ] θ is in the interior of the normal cone to Θ at θ (in particular,\nθ E[X1 ]), then, Theorem 5 yields that θ n = θ with probability going to 1 as n .", "Example 2 (Constrained least squares). Let (X1 , Y1 ), (X2 , Y2 ), . .", ". be iid random pairs in Rd R. Assume that X1 has four moments, E[X1 ] = 0, S = E[X1 X1 ] is definite positive, Y1 X1 θ0 is\nindependent of X1 and has the centered Gaussian distribution with variance σ 2 > 0 for some θ0 Rd\nand σ 2 > 0.", "Let ϕ(x, y, θ) = 1/2(y x θ)2 , for all x Rd , y R and θ Rd . Then, for all θ Rd ,\n1\nΦ(θ) = θ θ0 2S + σ 2 . 2\nLet Θ Rd be a non-empty, closed, convex subset of Rd (here, Θ0 = Rd ).", "Then, Argminθ Θ Φ(θ) =\nS\n{πΘ\n(θ0 )} and, provided that πΘ has directional derivatives at θ0 , the least square estimator θ n ,\ndefined as any minimizer on Θ of Φn (θ) = n 1 ni=1 (Yi Xi θ)2 , θ Rd , satisfies", "S", "+ S\nn(θ n θ ) d+ πΘ θ\n (θ0 θ ; Z) = d πΘ (θ0 ; Z)\nn", "in distribution, where Z Nd (0, S 1 BS 1 ) and\nB = var((Y1 X1 θ )X1 )", "= var((Y1 X1 θ0 )X1 + X1 (θ θ0 )X1 )\n= E[(X1 (θ0 θ ))2 X1 X1 ] + σ 2 S.", "3", "In fact, one moment is enough if one rather uses the loss function ϕ(x, θ) = x θ 2 x 2 , x, θ Rd\nDelta method requires Hadamard directional differentiability of πΘ θ at E[X1 ] θ . This is readily implied by\nthe existence of directional derivatives together with non-expansiveness of πΘ θ \n4", "17", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Example 3 (Geometric median). Let X1 , X2 , . . . be iid random vectors with one moment5 .\nConsider the loss function ϕ(x, θ) = x θ , x, θ Rd . Then, θ is any geometric median and θ n is\nany empirical geometric median. Here, in the unconstrained case, we recover standard results for\ngeometric median M -estimation, provided that the distribution of X1 is not supported on an affine\nline (this guarantees uniqueness of θ ) and that 1/ X1 θ is integrable (this guarantees that Φ is\ntwice differentiable at θ with positive definite Hessian), see, e.g., [28].\nProof of Theorem 7. Recall that we denote by S = 2 Φ(θ ), which is a symmetric, positive\ndefinite matrix, by assumption.\nFirst, since Θ0 is open, there exists some r > 0 such that BS (θ , r) Θ0 . Fix some R > 0, whose", "value will be determined later, and let n 1 be any integer that is large enough so R/ n r. For\nall such integers n, let Fn be the random function defined on B(0, R) by", "t n\n1\nFn (t) = n(Φn (θ + t/ n) Φn (θ )) ( g(Xi , θ ) + t 2 Φ(θ )t)\n2\nn i=1\nfor all t BS (0, R). This is a random convex function. Our first goal is to prove that Fn converges\npointwise (and hence, by Corollary 1, uniformly on the compact set BS (0, R)) to zero in probability.\nFrom this, we will then obtain that any minimizer of the first term (one of which is given by", "n(θ n θ ) for large enough n, with probability 1) is close to the unique minimizer of the second,\nquadratic term.\nFix t BS (0, R) and n 1. For i = 1, . . . , n, let Zi,n = ϕ(Xi , θ +n 1/2 t) ϕ(Xi , θ ) n 1/2 t g(Xi , θ ).\nBy definition of subgradients,\n0 Zi,n n 1/2 t (g(Xi , θ + n 1/2 t) g(Xi , θ )).\nSquaring and taking the expectation yields that\n2", "2\n] n 1 E [(t (g(X1 , θ + n 1/2 t) g(X1 , θ ))) ]\nE[Zi,n", "(7)", "(we replaced i with 1 in the right hand side because the Xi s are iid). Let Yn = t (g(X1 , θ +", "n 1/2 t) g(X\n1 , θ )). As mentioned above, Yn 0. Moreover, for n 1, letting u = θ + t/ n and", "v = θ + t/ n + 1,\nYn Yn+1 = t (g(X1 , u) g(X1 , v))", "= (1/ n 1/ n + 1) 1 (u v) (g(X1 , u) g(X1 , v))\n 0\nby Lemma 11. So the sequence (Yn )n 1 is non-increasing. Hence, Yn converges almost surely to\nsome non-negative random variable Y . By monotone convergence (noting that Y1 is integrable),\nthis implies that\nE[Yn ] E[Y ].", "(8)", "n", "However, for all n 1, E[Yn ] = t (wn Φ(θ )) where wn Φ(θ + t/ n), by Lemma 6. Lemma 7\nyielding that wn w, we obtain that E[Yn ] 0. Together with (8), this shows that E[Y ] = 0\nn", "5", "n", "Similarly to the first example, one need not assume the existence of one moment if the loss function is replaced\nwith ϕ(x, θ) = x θ x , x, θ Rd .", "18", "V.-E. BRUNEL", "and, hence, because Y 0, that Y = 0 almost surely. Therefore, again by monotone convergence\n(noting, this time, that Y12 is iontegrable), E[Yn2 ] E[Y 2 ] = 0.\nn", "Combined with (7) and using independence of Z1,n , . . . , Zn,n , we obtain that\n(9)", "n", "n", "n", "i=1", "i=1", "i=1", "2\nvar ( Zi,n ) = var(Zi,n ) E[Zi,n\n] E[Yn2 ] 0.\nn", "Therefore, by Chebychev s inequality, ni=1 (Zi,n E[Zi,n ]) 0 in probability, that is,\nn", "n", "n(Φn (θ +n 1/2 t) Φn (θ )) n 1/2 t g(Xi , θ ) n(Φ(θ +n 1/2 t) Φ(θ ) n 1/2 t Φ(θ )) 0\nn", "i=1", "in probability. Now, since we have assumed that Φ is twice differentiable at θ , we finally obtain\nthat\nFn (t) 0", "(10)", "n", "in probability, for all t BS (0, R), as desired.\nFor all integers n 1, let Tn = {t Rd θ + n 1/2 t Θ} = n1/2 (Θ θ ) T and Sn = {t Rd \nθ + n 1/2 t Θ0 } = n1/2 (Θ0 θ ). Then, Tn is a closed subset of Sn . Moreover, since θ Θ0 and\nΘ0 is open, BS (0, R) Sn for all large enough integers n (recall that R > 0 is some fixed number,\nwhose value is still to be determined). Define the maps\nG n t Sn n(Φn (θ + n 1/2 t) Φn (θ ))\nand", "n\n1\nGn t Rd n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t.\n2\ni=1", "As per these definitions, Fn = G n Gn , so, (10) and Corollary 1 yield that\n(11)", "sup\nt BS (0,R)", "G n (t) Gn (t) 0\nn", "in probability.\nMoreover, t n = n1/2 (θ n θ ) is a minimizer of G n on Tn , by definition of the empirical risk\nminimizer θ n .\nNow, denote by Zn = n 1/2 S 1 ni=1 g(Xi , θ ) Φ(θ ) and for all t Rd , rewrite Gn (t) as\nn\n1\nGn (t) = n 1/2 t g(Xi , θ ) + t 2 Φ(θ )t\n2\ni=1\nn\n1\n= n 1/2 S 1 g(Xi , θ ), t S + t 2S\n2\ni=1", "1\n= Zn + nS 1 Φ(θ ), t S + t 2S\n2\n 1", "1\n 2\n= t + Zn + nS Φ(θ ) S Zn + nS 1 Φ(θ ) 2S .\n2", "It is now clear that Gn has a unique minimizer on Tn , which we denote by t n and which is given\nby", "t n = πTSn ( Zn", "1\nnS Φ(θ )).", "19", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "Now, our goal is twofold. First, to study the asymptotic behavior of t n and show that it converges\nin distribution, as n . Second, to check, based on (11), that t n approaches t n as n , that is,\nt n t n converges in probability to 0. Using Slutsky s theorem, these two facts will imply convergence\nin distribution of t n .\nAsymptotic behavior of t n .\nFirst, by the central limit theorem, we have that Zn Z in distribution, where Z is is a\nn", "centered Gaussian random variable with covariance matrix given by S 1 var(g(X1 , θ ))S 1 .\nBy Skorohod representation theorem (see [25, Theorem 5.31] for instance), one may assume\nS\nthat Zn converges almost surely to Z. Since πC\nis non-expansive by Lemma 13, it holds that", "S\n 1", "tn πTn ( Z nS Φ(θ )) converges to 0 almost surely. Moreover,", "1\nS", "πTSn ( Z nS 1 Φ(θ )) = π \nn(Θ θ ) ( Z nS Φ(θ ))\n S\n 1/2\n= nπΘ θ\nZ S 1 Φ(θ ))\n ( n\nS\n 1\n d+ πΘ θ\n Φ(θ ); Z)\n ( S\nn", "almost surely, using the third assumption of the theorem. Therefore, we conclude that t n \nn", "1\nS\n Φ(θ ); Z) almost surely and, hence, in distribution. The desired results follows,\nd+ πΘ θ\n ( S\nsince Z and Z are identically distributed.\nConvergence in probability of t n t n to 0.\nFix ε > 0. Since the sequence (t n )n 1 converges in distribution (see the previous paragraph), it\nis tight, that is, there must exist some M > 0 such that for all n 1, P ( t n S M ) 1 ε. Let\nK = BS (0, M + ε) and fix some η > 0 to be chosen below. (11) yields that for all large enough n 1,\nsupt K G n (t) Gn (t) η with probability at least 1 ε. Therefore, by the union bound, for all\nlarge enough n 1, it holds with probability at least 1 2ε that simultaneously for all t Tn with\n t t n S = ε,", "G n (t) Gn (t) η\nε2\n η\n2\nε2\n G n (t n ) η +\n η.\n2", "Gn (t n ) +", "Hence, chosing η = ε2 /8, we obtain that for all large enough integers n, with probability at least\n1 2ε, G n (t) > G n (t n ) simultaneously for all t Tn with t t n S = ε. Corollary 1 yields that for all\nlarge enough integers n, with probability at least 1 2ε, t n t n S ε. That is, t n t n converges in\nprobability to 0.\nS\n 1\nConclusion. We have proved that t n converges in distribution to d+ πΘ θ\n Φ(θ ); Z) for\n ( S", "some Gaussian random variable Z and that t n tn converges to zero in probability, as n .\nHence, Slutsky s theorem implies the desired result.\nIn the proof of Theorem 7, the convergence that we obtained in (10) actually holds in the L2\nsense (see (9)). Therefore, Corollary 2 implies uniform convergence on all compact subsets in the L2\nsense. Yet, it is not clear, from there, how to proceed and prove that t n t n 0 in L2 . Proving\nn", "this convergence would yield an exact asymptotic quantification of the mean squared error of θ n ,\nsince, it would yield that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]\n ( S\nn", "20", "V.-E. BRUNEL", "where Z is a Gaussian vector as in the theorem. We leave the following question open:\nOpen question. Is it true that under the assumptions of Theorem 7, for all large enough n,\nθ n has two moments, and that\nS\n 1\nnE[ θ n θ 2 ] E[ d+ πΘ θ\n Φ(θ ); Z) 2 ]?\n ( S\nn", "5. EXTENSION: CONVEX U -ESTIMATION\nThe previous theory can be easily extended to more general convex empirical risks, e.g., when\nΦn (θ) is a U -statistic. With the same notation as in the previous sections, fix some positive integer\nk and let ϕ E k Θ0 R be symmetric and measurable in its first k arguments and convex in its\nlast. Also assume that for all θ Θ0 , ϕ( , θ) L1 (P k ), that is, ϕ(X1 , . . . , Xk , θ) is integrable. Set\nΦ(θ) = E[ϕ(X1 , . . . , Xk , θ)] and, for all n k,\nΦn (θ) =", "1\nϕ(Xi1 , . . . , Xik , θ).", "(nk) 1 i1 <...<ik n", "Estimators obtained by minimizing such empirical risks are called U -estimators. Some relevant\nexamples include:\n1. Location estimators through depth functions: Let E = Θ0 = Θ = Rd , k = d and ϕ(x1 , .", ". . , xd , θ)\nbe the volume of the d-dimensional simplex spanned by x1 , .", ". . , xd , θ, for all x1 , .", ". . , xd , θ Rd .", "The minimizers of Φ are then called Oja s population medians [44]. Note that ϕ(x1 , . .", ". , xd , θ)\nis the absolute value of an affine function of θ, hence, it is convex in θ. We recover consistency\nand asymptotic normality of Oja s empirical medians (see [45]) as particular cases of our\nasymptotic theorems (see below for U -estimators).", "More generally, we refer to [58] for other\ndefinitions of medians that are U -estimators associated with depth functions. 2. Let E = R and Θ Θ0 = R and k 1.", "[37] proposes a version of the median of mean estimator\ndefined as a U -estimator obtained by computing an empirical median of all empirical averages\nk\nof the form k1 i I Xi , for I {1, . . .", ", n} of size k. That is, ϕ(x1 , . .", ". , xk , θ) = x1 +. .", ". +x\n θ , for\nk\nall x1 , . .", ". , xk , θ R. The difference with standard median of mean estimators [32,33,39] is that\nin [37], all possible subsamples of size k, with overlaps, are considered.", "Other frameworks,\nsuch as geometric medians of means in multivariate settings [36] can be considered as well. Note that in [37], the order k of the U -process is allowed to grow with the sample size n - we\ndo not consider this setup here and leave it for future work. 3.", "More generally, aggregation of estimators that are based on overlapping subsamples, e. g. ,\nrandom forests [9] or bagging [8], which have attracted lots of interest in modern machine\nlearning.", "4. Scatter estimation and robustness: Let E = R, Θ0 = R, k = 2 and ϕ(x1 , x2 , θ) = ( x1 x2 p θ)\nwhere p 1 and = R R is a convex function. When p = 2 and (u) = u2 , u R, θ n is simply\ntwice the empirical variance of X1 , .", ". . , Xn and if = hc for some c > 0 (recall the definition of\nhc from Section 1.", "1), we obtain a robust version of the empirical variance. If now p = 1 and\n (u) = u2 , u R, we obtain Gini s mean absolute difference, while if = , we obtain a proxy\nto a median absolute deviation (and intermediate robust versions if = hc for some c > 0). In higher dimensions, one recovers the empirical covariance matrix of X1 , .", ". . , Xn by setting\n2\nϕ(x1 , x2 , θ) = tr(((x1 x2 )(x1 x2 ) θ)2 ), for all θ Rd d Rd and x1 , x2 Rd .", "Robust\nversions can be defined by taking the square root of the above, or applying Huber s loss hc\nfor some c > 0.", "ASYMPTOTICS OF CONVEX M -ESTIMATION", "21", "5. Empirical risk minimization where the choice of loss function itself depends on the data (e.g.,\nfor data driven procedures), see, e.g., [53].\nNote that U -statistics depending on a parameter (here, Φn (θ), θ Θ0 ) have been studied as\nU -processes, see, e.g., [4, 41, 42]. Here, we first recall the classical law of large numbers and central\nlimit theorem for U -statistics.\nTheorem 8. Law of large numbers for U -statistics [20, Theorem 8.6] Let h E k Rd be a\nsymmetric, measurable map satisfying h L1 (P k ). Then,\n1\nh(Xi1 , . . . , Xik ) E[h(X1 , . . . , Xk )]", "n \n(nk) 1 i1 < <ik n\nalmost surely.\nTheorem 9. Central limit theorem for multivariate U -statistics [22, Theorem 7.1], [20, Theorem 8.9] Let h E k Rd be a symmetric, measurable map satisfying h L2 (P k ). Let Σ be the\n1\ncovariance matrix of E[h(X1 , . . . , Xk ) X1 ]6 . For all n k, let Un = n\nh(Xi1 , . . . , Xik ).", "(k ) 1 i1 < <ik n\nThen,", "n(Un E[h(X1 , . . . , Xk )]) Nd (0, k 2 Σ)\nn", "in distribution.\nTheorem 4 obviously remains true in the context of U -estimation with convex loss. Proposition 1,\nTheorems 5 and 6 require more care but also remain true in this context. Proofs are deferred to\nSection D. Below, we rewrite Theorem 7 for U -estimators, where an extra multiplicative factor k\nappears in the limit, accounting for the dependence of the terms in the new definition of Φn .\nTheorem 10. Asymptotic distribution for U -estimators Let g E k Θ0 Rd be a measurable\nselection of subgradients of ϕ. Assume the following:\n(i) Φ has a unique minimizer θ in Θ, it is twice differentiable at θ and S = 2 Φ(θ ) is positive\ndefinite;\n(ii) g( , θ ) L2 (P k );\nS\n 1\n(iii) πΘ θ\n Φ(θ ).\n has directional derivatives at S\nThen,", "S\n 1\nn(θ n θ ) k d+ πΘ θ\n Φ(θ ); Z)\n (S\nn", "1", "1", "in distribution, where Z Nd (0, S BS ) and B = var(E[g(X1 , . . . , Xk , θ ) X1 ]).\nNote the extra k factor in the limit in distribution.\n6", "Σ can also be written as E[h(X1 , X2 , . . . , Xk )h(X1 , X2 , . . . , Xk ) ] E[h(X1 , . . . , Xk )]E[h(X1 , . . . , Xk )] , that is,\nthe covariance of the random vectors h(X1 , X2 , . . . , Xk ) and h(X1 , X2 , . . . , Xk ), where X2 , . . . , Xk are such that\nX1 , X2 , . . . , Xk , X2 , . . . , Xk are iid.", "22", "V.-E. BRUNEL", "6. CONCLUSION AND FUTURE DIRECTIONS\nWe have established the asymptotic properties of constrained M -estimators with a convex loss\nand a convex set of constraints, under minimal assumptions. In this work, asymptotics are only\nrelative to the sample size n, while the dimension d is kept fixed.", "In large dimensional problems, asymptotic theory can be approached from different angles. First,\none may look at asymptotic distributions of low-dimensional projections of the M -estimator. For\ninstance, in the context of linear regression, [6] proves the asymptotic normality of single coordinates\nof penalized M -estimators when the ratio d/n goes to some fixed, positive constant.", "A second angle\nconsists of looking at the full, joint distribution of (a rescaled version of) the M -estimator θ n , and\nprove that, for some distribution Qd in Rd , some specified distance (e. g. , an integral probability\nmetric) between the distribution of θ n and Qd goes to 0 as n, d in a certain manner.", "When\nθ n is simply the sample mean of X1 , . . .", ", Xn , such an approach has been studied and called high\ndimensional central limit theorems [12, 15]. However, to the best of our knowledge, such results do\nnot exist for other M -estimators, even with convex loss. In the context of U -estimators, we have also let the order k of the U -process be fixed.", "However,\nit may be relevant to also let k grow with the sample size (e. g. , for median-of-means procedures).", "While the asymptotics of U -statistics with increasing order have been studied only recently [14],\nwe leave this direction for future work on U -estimation."]}
