{"method": "ahgc", "num_chunks": 115, "avg_chunk_len": 1046.8347826086956, "std_chunk_len": 212.5004319355112, "max_chunk_len": 1524, "min_chunk_len": 191, "total_chars": 120386, "compression_ratio": 0.8657235891216587, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025 Analyzing the topological structure of composite dynamical systems Michael Robinson Michael L. Szulczewski James T. Thorson September 2025 Abstract This chapter explores dynamical structural equation models (DSEMs) and their nonlinear generalizations into sheaves of dynamical systems. It demonstrates these two disciplines on part of the food web in the Bering Sea. The translation from DSEMs to sheaves passes through a formal construction borrowed from electronics called a netlist that specifies how data route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations can be tested for consistency, how missing data can be inferred, and how uncertainty about the observations can be quantified. Sheaf modeling provides a coherent mathematical framework for studying the interaction of various dynamical subsystems that together determine a larger system. Contents 1 Introduction 1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . .", ". . . . 1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 4 5 2 Dynamical modeling of ecosystems 2.1 DSEM background and motivation . . . . . . . . . . . . . . . . . 2.2 Ecological background and the DSEM system for the Bering Sea 5 5 7 Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public Release Case Number 25-2751. The author s affiliation with The MITRE Corporation is provided for identification purposes only, and is not intended to convey or imply MITRE s concurrence with, or support for, the positions, opinions, or viewpoints expressed by the author. 2025 The MITRE Corporation. ALL RIGHTS RESERVED. 1 3 Sheaf encodings of composite systems 3.1 Netlists . . . . . . . . . . . . . . . . . . . .", "RESERVED. 1 3 Sheaf encodings of composite systems 3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . . 3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . . 8 11 14 18 25 4 Sheaf encoding of the Bering Sea 28 5 The topology of subsystems 33 5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35 5.3 Subsystem decomposition sheaf . . . . . . . . .", "endomorphism of invariant sets . . . . . . . . . . . . 35 5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37 6 Subsystems of the Bering Sea system 49 7 Conclusion 50 1", "Introduction Ecologists often study systems on spatial and temporal scales that cannot be experimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which extrapolating the results of experiments at fine space-time scales is challenging [48]. These systems are also challenging to study because observational data can be noisy and sporadic. A third challenge is the presence of complex, causal relationships between system variables that can change over time. Understanding the dynamics of these kind of large composite models is much easier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20). Subsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical way to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original model (Section 5). This chapter explores dynamical structural equation models (DSEMs) and their nonlinear generalizations via a topologically motivated translation into sheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear", "equation models (DSEMs) and their nonlinear generalizations via a topologically motivated translation into sheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe, which allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related timeseries. The translation from DSEMs to sheaves passes through a formal construction borrowed from electronics called a netlist that specifies how data route through a system. Because the netlist and sheaf methodology is explicit and graphical, we include several illustrative examples (Figures 3 and 5). One real-world example 2 involves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and 6). Sheaves provide many advantages to a modeler. They enable exploring the impact of uncertainty in various ways. They support inference of missing or erroneous data, including system parameters and coefficients (Section 3). They also enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4). Sheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of", "enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4). Sheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems of a DSEM can be read off its associated graph. This is applied to the Bering Sea ecosystem model in Section 6. 1.1 Related work The challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space and time, and can decompose the total effect of one variable on another via a combination of direct and indirect effects [16, 5]. Recently, SCMs have been adapted to the analysis of ecological time series via DSEMs [47]. The key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite systems given by the field of cybernetics, for which Heylighen and Joslyn [17] and Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21],", "[17] and Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51], mathematical systems [9], biology [26], and chemistry [18]. Although algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned with regards to modeling and simulation, formal definitions are harder to find. One challenge is that decompositions are often not unique: for example, one may choose to group state variables based on constraints rather than functional units [8, 24]. These choices are important because they drive the usefulness of the decomposition [27]. For example, overlapping, rather than disjoint, subsystem decompositions are useful for analyzing stability of an entire system [40, 4]. We argue that a properly general and formal definition of a subsystem decomposition must support overlappingness, non-uniqueness, and ambiguous granularity. Because the collection of all subsystems forms a mathematical sheaf (Definition 21), this implies that seeking disjoint, unambiguous subsystems (as is often done) is fraught. Aspects of the formalism we introduce in this chapter are", "subsystems forms a mathematical sheaf (Definition 21), this implies that seeking disjoint, unambiguous subsystems (as is often done) is fraught. Aspects of the formalism we introduce in this chapter are not entirely novel. For instance, Hirono et al. [18] defines a CRN morphism that is a special case of our Definition 20. Additionally, the sheaf of subsystems is based upon a clear graphical representation, which is well known in the analysis of software 3 [29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping, which evokes the analogous term from sheaves (Definition 7). Roughly dual to the notion of a subsystem is that of an invariant set of a dynamical system (our Definition 20 makes this a true duality). Invariant sets are widely used in dynamical systems [44], where they generalize equilibrium sets and attractors. For linear systems, duality between invariant sets and subsystems is immediate and useful. For instance, the design structure matrix [43] yields invariant sets, giving a clear duality to subsystems. Finally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson [34, Sec. 5], and is specialized to subsystem", "the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson [34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In Kearney et al. [22], the dynamics are specified locally and are much easier to specify due to the fact that the system is given a graph structure. 1.2 Contributions This chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered around sheaves. Sheaf modeling provides a coherent mathematical framework for studying the complicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that a sheaf represents a hypothesis about how variables will interact (Definition 10), a non-global assignment represents the observations collected on the variables in its support (Definition 8), minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and the minimal consistency radius is a measure of the consistency between the observations and the hypothesis. This chapter shows that when a dynamical system is described by a linear system, there are three sheaves that", "a measure of the consistency between the observations and the hypothesis. This chapter shows that when a dynamical system is described by a linear system, there are three sheaves that provide increasingly granular data about the interactions between variables: 1. the sheaf of subsystems (Definition 21), 2. the netlist sheaf with timeseries as stalks (Definition 13), and 3. the netlist sheaf with additional stalks for individual observations (Definition 14). 4 1.3 Chapter outline Section 2 describes a model of a food web in the Bering Sea, which we use to illustrate the use of sheaves. This system is large enough to exhibit interesting structures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated (but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear, whereas DSEMs are linear. Section 3 constructs sheaves that model composite systems, and develops the main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the constructions is introduced as it is", "systems, and develops the main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the constructions is introduced as it is needed. Small concrete examples of the construction and use of sheaf models are presented to build intuition as well. In Section 4, we revisit the ecological model from Section 2 using the sheaf tools from Section 3. The interface between observational data, sheaves, and their inference tools is explored in detail. Moreover, we compare differences between the DSEM and sheaf approaches in detail. Section 5 introduces the idea of a general topological dynamical system, and shows that every dynamical system induces a sheaf of subsystems and a cosheaf of invariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary 21). This provides theoretical justification for why DSEMs are a useful way to describe a composite linear system by way of its subsystems. Section 6 revisits the ecological model from Section 2 once again. Because the model satisfies the hypothesis of Corollary 21, we are able to present a clear representation of all the subsystems present", "the ecological model from Section 2 once again. Because the model satisfies the hypothesis of Corollary 21, we are able to present a clear representation of all the subsystems present in the model. Finally, Section 7 concludes the chapter with practical advice for modelers and a brief discussion of future research work. 2", "Dynamical modeling of ecosystems This section begins with a brief recount of modeling linear dynamical systems according to an underlying graph structure, and then presents a representative ecosystem model that will be revisited several times in the chapter. 2.1 DSEM background and motivation Definition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 < < tT } of real valued time lags, a dynamic structural equation model (DSEM) consists of an edge-labeled directed graph G with vertices X Y and edges E such that Causality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 , and 5 Linearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2 called the path coefficient for that edge. The absence of an edge in the graph is assumed to be equivalent to assigning a path coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k . The variables in a DSEM are to be interpreted as C 1 (R) functions, which are continuous timeseries. A directed", "a vertex (xj , tk ) simply as xj,k . The variables in a DSEM are to be interpreted as C 1 (R) functions, which are continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as specifying that a change in xi causes a proportional (linear) change in xi after a lag of (tj tj ), with magnitude controlled by the associated path coefficient γi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of linear differential equations governs the values of the variables: J T dxk (τ t ) X X = γk, ,i,j xi (τ tj ). dτ i=1 j=1 (1) In what follows, we will refer to solutions of Equation 1 as solutions to the DSEM. In the use of Equation (1) with observational data, there are two kinds of errors that need to be considered: exogenous errors and measurement errors. Exogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result, there is a dependence between the exogenous errors of xk at different times. In contrast, measurement errors at", "impacts the value of xk at all later times. As a result, there is a dependence between the exogenous errors of xk at different times. In contrast, measurement errors at different times are assumed to be independent. Exogenous errors will be represented by an additive term, ϵk, , resulting in T J dxk (τ t ) X X γk, ,i,j xi (τ tj ) + ϵk, (τ ). = dτ i=1 j=1 (2) We can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h, dxk (τ t ) 1 (xk (τ t ) xk (τ t h)) , dτ h so that Equation (2) becomes a system of M = T J linear algebraic equations, xk (τ t ) xk (τ t h) + h J X T X i=1 j=1 γk, ,i,j xi (τ tj ) + hϵk, (τ ). (3) If we fix a value of τ and organize the set of values {xk (τ t )} into a vector X of length M ), Equation (3) can be compactly written in matrix form as X PX + E, (4) where the entries of the M M path coefficient matrix P", "of length M ), Equation (3) can be compactly written in matrix form as X PX + E, (4) where the entries of the M M path coefficient matrix P contain both the path coefficients from the DSEM (scaled by h) and the additional nonzero entries due 6 the xk (τ t h) terms. In what follows, we will take h = 1, so that the path coefficients in the DSEM appear unchanged as elements of the matrix P. To obtain the path coefficient matrix P from observations of X, we assume the exogenous errors follow a multivariate normal distribution with variance V, namely E MVN(0, V), where E is the length M vector containing errors ϵtj . Equation (4) can then be re-arranged to yield a Gaussian Markov random field, X MVN(0, Q 1 ) T Q = (id P )V 1 (5) (id P), (6) where id is the identity matrix. The path coefficient matrix P can be obtained from the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we use Template Model Builder [25] to incorporate automatic differentiation and implement the Laplace approximation [39]", "can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we use Template Model Builder [25] to incorporate automatic differentiation and implement the Laplace approximation [39] to marginalize across random effects. Now we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj at time tj . (If one does not wish to model measurement errors explicitly, so that measurement errors are entirely captured by the exogenous error term, this is obtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j for the observation of the variable xk,j . We therefore can express the mean of the distribution of yk,j through a link function gj , via yk,j fj gj 1 ( j + xk,j ), θj , where j is the true mean. The clearest way to obtain the required sparsity in solving for P is to assume additionally that the measurement errors for a given variable do not depend on time tj . Let G be the J J matrix that is diagonal, and whose diagonal terms are given by the link functions gj .", "variable do not depend on time tj . Let G be the J J matrix that is diagonal, and whose diagonal terms are given by the link functions gj . With this in hand, V takes the form V = id T T GGT , (7) where is the Kronecker product. This implies that V is block diagonal, and is thereby efficient to invert. 2.2 Ecological background and the DSEM system for the Bering Sea To demonstrate the use of sheaves for dynamical systems, we make a sheaf from a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus chalcogrammus) in the eastern and northern Bering Sea [47]. The model starts 7 by specifying that abundance of age-0 pollock Rt (termed age-0 recruitment ) can be predicted from the biomass of spawning females St in a given year t: Rt = St eα βSt +ϵt (8) α where e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass increases, and ϵt is additional process error representing unmodeled variation in recruitment. This Ricker stock-recruit model [33] has been used", "expected density-dependent decrease in recruits per spawning biomass as biomass increases, and ϵt is additional process error representing unmodeled variation in recruitment. This Ricker stock-recruit model [33] has been used for over 70 years to represent density-dependent changes in juvenile survival, and as the basis for defining biological reference points that are used worldwide to identify sustainable levels of fishing mortality [42]. The Ricker model is expected to arise for species where adult abundance directly impacts juvenile survival for example, due to cannibalism or interference competition [15]. Alaska pollock are cannibalistic, so the Ricker model has theoretical justification. Usefully, the Ricker model can be linearized as: Rt = α βSt + ϵt (9) log St and a DSEM can be used to elaborate the mechanisms that contribute to process errors ϵt based on prior ecological hypotheses. The DSEM we translate into a sheaf was previously developed by Thorson et al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives residual variation in log-recruits per spawning biomass (Survival ) via two paths, mediated by sea-ice impacts on either copepod abundance (Copepod ) or krill abundance (Krill ), and resulting consumption by juvenile pollock. See Table 1 and 2 for", "via two paths, mediated by sea-ice impacts on either copepod abundance (Copepod ) or krill abundance (Krill ), and resulting consumption by juvenile pollock. See Table 1 and 2 for more details on the variables and mechanisms in the model. The DSEM includes a first-order autoregressive term for each variable, to allow the model to correct for bias that can arise when correlating variables that follow an autoregressive process (summarized in [28]). This first-order autoregression can also be interpreted to represent Gompertz density-dependence and therefore has some scientific interest [23], although it is not further discussed here.", "3 Sheaf encodings of composite systems In this section, we explain how to construct a netlist sheaf whose global sections correspond bijectively to the solutions of a DSEM. This is performed in two main steps: (1) the DSEM is translated into a netlist, and (2) the netlist is translated into the netlist sheaf. Since the machinery of sheaves is not in wide usage, Section 3.2 provides the necessary background. With the machinery and the translation in place, Theorem 6 establishes that the two representations, the DSEM and the netlist sheaf, are equivalent. The global sections of the netlist sheaf are in bijective correspondence with solutions to the DSEM. Moreover, a process called consistency radius minimization in the sheaf finds approximate solutions to the DSEM, and this process is robust to perturbations. 8 Table 1: Variables that describe Alaska pollock recruitment used in the DSEM and sheaf. All except Spawners are transformed by the natural logarithm and then centered (i.e., subtracted by their mean) prior to analysis. Timeseries of the variables are taken from [47]. Name SeaIce Description Average spatial extent (km2 ) of sea ice in the Bering Sea from Oct.15 to Dec.15 the preceding year, from the National Snow", "are taken from [47]. Name SeaIce Description Average spatial extent (km2 ) of sea ice in the Bering Sea from Oct.15 to Dec.15 the preceding year, from the National Snow and Ice Center s Sea Ice Index, Version 3 [14] ColdPool Spatial extent (km2 ) of waters with temperatures 2 C near the seafloor, interpolated from measurements by the eastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37] Spawners Female spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by the age-structured stock assessment model used for management [20] Survival Age-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count) estimated by the age-structured stock assessment model [20] divided by Spawners Copepods Density of 2 mm copepods (count/m3 ) from the Bering Sea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept. to early Oct. [12] (calculated by Dave Kimmel, pers. comm.) Krill Index of euphausiid abundance (count/m3 ) [32] obtained from backscatter measured during a summer acoustic-trawl survey in the eastern Bering Sea and converted to abundance using a target-strength model [41]", "Index of euphausiid abundance (count/m3 ) [32] obtained from backscatter measured during a summer acoustic-trawl survey in the eastern Bering Sea and converted to abundance using a target-strength model [41] DietCopepods Biomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface trawl, total catch of juvenile pollock is weighed, individual pollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The diet index is calculated as the average across subsampled stomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews, pers. comm.). DietKrill Same as DietCopepods, but for euphausiids (krill) 9 Table 2: List of path coefficients connecting variables (defined in Table 1), supporting ecological hypotheses, and hypothesized sign for the path used in the DSEM case study. We also include a first-order autoregressive term for each variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in Section 2.2. Path SeaIce ColdP ool Ecological hypothesis and evidence Sea ice formation (SeaIce) causes variation in summer cold-pool extent (ColdPool ) Sign + ColdP ool Copepods Warmer water temperatures", "Section 2.2. Path SeaIce ColdP ool Ecological hypothesis and evidence Sea ice formation (SeaIce) causes variation in summer cold-pool extent (ColdPool ) Sign + ColdP ool Copepods Warmer water temperatures (ColdPool ) result in higher copepod metabolism and therefore earlier onset of winter diapause, resulting in a decrease in fall copepod abundance (Copepods) [10] + ColdP ool Krill Water temperatures (ColdPool ) might affect krill overwinter survival, affecting summer krill abundance (Krill ) ? Copepods DietCopepods Increased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents (DietCopepods), due to pollock being hypothesized to be a relative nonselective predator + Krill DietKrill Same as Copepods DietCopepods but for krill + DietCopepods Survival Increased fraction of fall diet from copepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter (Survival ) [19] + DietKrill Survival Same as DietCopepods Survival, but for krill + Spawners Survival Increased spawning (Spawners) will cause a dependent decrease in (Survival ) [15] 10 biomass densitysurvival SeaIce out ColdPool f n ColdPool in Copepods_block out Krill_block out Copepods Krill Krill in Copepods_block Krill_block in DietCope_block Diet_Cop Diet_Krill Spawners out Diet_Cop Survival in_copepods out Spawners out Diet_Cop in_copepods", "out ColdPool f n ColdPool in Copepods_block out Krill_block out Copepods Krill Krill in Copepods_block Krill_block in DietCope_block Diet_Cop Diet_Krill Spawners out Diet_Cop Survival in_copepods out Spawners out Diet_Cop in_copepods in_spawners Diet_Krill Spawners in_krill Survival_block in_spawners g2 g1 n id n h id n k n n pr1 pr2 n n Survival m n (b) (c) (d) out n n Survival out (a) in DietKrill_block DietCope_block out Diet_Krill in_krill Survival_block Krill in in DietKrill_block n out Copepods id id in in out Copepods n ColdPool_block out ColdPool id in ColdPool_block SeaIce n SeaIce in n n pr3 Figure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b) its wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear relationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1); and in (d), they denote restriction functions (Sec. 3.2). While the DSEM also estimates a first-order autoregressive term for each variable (not shown in (a) to simplify presentation), there is no autoregressive structure assumed in the sheaf model. This remedied in Section 3.4. Throughout this", "first-order autoregressive term for each variable (not shown in (a) to simplify presentation), there is no autoregressive structure assumed in the sheaf model. This remedied in Section 3.4. Throughout this section, we refer to Figure 1 for intuition. Figure 1(a) shows the DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist translation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a different representation of the netlist that is more expedient for the construction of the netlist sheaf. Proposition 3 establishes that the two representations of netlists (Figures 1(b) (c)) determine each other, so we may use whichever is more convenient. Finally, the netlist-to-sheaf translation, described in Section 3, results in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries models as netlist sheaves, which ultimately makes handling missing data both transparent and automatic within the netlist sheaf. 3.1 Netlists The term netlist appears to have entered the technical lexicon in the early days of computing, when IBM started to automate the wiring of mainframe back planes [3]. Since that time, the term netlist has been in wide usage but often without a precise definition. In order to formalize the concept, we say", "of mainframe back planes [3]. Since that time, the term netlist has been in wide usage but often without a precise definition. In order to formalize the concept, we say that a netlist describes a system of parts interconnected with nets, which carry time-varying signals (briefly, variables). Each variable consists of the specification of a set of possible values for a net. In this chapter, the values for a variable in a net are initially assumed to be continuous timeseries, usually of the form C 1 (R). We will also consider sampled timeseries of the form Rn , where n is the length of the timeseries. In Section 3.4, we show how to handle missing values in such a timeseries. Each part has a number of ports, to which connections can be made. Each port is either an output, which means that it determines the value of the variable 11 Part 2 (capacitor) Net 1 in out Net 2 in out Part 1 (Battery) Part 3 (resistor) in out Net 3 Figure 2: A netlist for an electric circuit, described in Example 1. of a net connected to it, or an input, which means that it does not determine the", "3 Figure 2: A netlist for an electric circuit, described in Example 1. of a net connected to it, or an input, which means that it does not determine the value of the variable of a net connected to it. Each net specifies that a collection of distinct ports on a pair of parts (which need not be distinct) are connected, with the requirement that not more than one of these ports be an output. Finally, each part specifies an input-output function for each output port. The domain of an input-output function is from the product of the set of its input variables, and its codomain (range) is the set of output variables at the output port. This formulation leaves open the possibility of nets that are not attached to any output ports, which are called external inputs, and nets which are not attached to any input ports, which are called external outputs. Clearly each external output must attach to exactly one port, which must be an output port. Example 1. Figure 2 shows an electrical circuit with three parts: a battery, a capacitor, and a resistor. These parts are connected to each other by three nets: 1. Connecting the", "1. Figure 2 shows an electrical circuit with three parts: a battery, a capacitor, and a resistor. These parts are connected to each other by three nets: 1. Connecting the positive (output) port of the battery to the input port of the capacitor, 2. Connecting the output port of the capacitor to the input port of the resistor, and 3. Connecting the output port of the resistor to the input port of the battery. The values of the variables on the nets specify electrical currents flowing along them. We note that the labeling ports as input and output in this kind of circuit is arbitrary, since the electrical current can flow in either direction along a net. The input-output functions simply recount classical Ohm s law for each of the parts in the circuit. This circuit contains no external inputs nor external outputs. A DSEM graph can be translated into a netlist via the following construction. Definition 2. Given a DSEM, its corresponding netlist is given by the following recipe: each DSEM variable (node) becomes a net, 12 each DSEM variable with more than one input becomes a part, each net is connected to input ports via its out-neighbors, each", "each DSEM variable (node) becomes a net, 12 each DSEM variable with more than one input becomes a part, each net is connected to input ports via its out-neighbors, each net is connected to output ports via matching the name of the net to the part with the same name (if any exist), and the part s input-output function is collected from the matrix block in Equation (4) corresponding to the input and output variables. There are two combinatorial structures associated to a netlist, the wiring hypergraph and the netlist graph. Definition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled partition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net. The label on each vertex is simply the name of the part corresponding to that vertex. The vertices within a hyperedge correspond to the parts connected to the corresponding net. The label on each hyperedge is an ordered triple, consisting of the inputs port of the net (if any), the output port of the net (if any), and the variable name of the net. The partition direction of each hyperedge separates the output port from the input ports; either of these", "port of the net (if any), and the variable name of the net. The partition direction of each hyperedge separates the output port from the input ports; either of these may be empty. Because the labeling on the wiring hypergraph is complicated, we represent it with a standard visual grammar borrowed from electronics. Each part is represented by a rectangle with its label in the center of the rectangle. Each net is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree structure. The label of the variable of the net is shown next to the path, but the name of the net s input and output ports are shown inside the connected parts rectangles, around the edge of the rectangle. The input-output functions are not shown explicitly. Figure 1(b) shows the wiring hypergraph for the netlist constructed using Definition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds to a hyperedge of size 3 in the wiring hypergraph, because it is connected to one output port and two input ports. Proposition 1. The solutions to a DSEM", "corresponds to a hyperedge of size 3 in the wiring hypergraph, because it is connected to one output port and two input ports. Proposition 1. The solutions to a DSEM are in bijective correspondence with labelings of the nets with values of variables that are consistent with the netlist s input-output functions. Proof. The solutions to the DSEM are characterized by Equation (4), which is a matrix block assembly of everything that is needed to construct the netlist. Assume we have a set of variables for all nets that are consistent with the input-output functions. As noted above, each variable takes values in a set of the form C 1 (R). On the other hand, each input-output function was constructed from a matrix block in Equation (4). Because all of the DSEM variables appear as nets in the netlist, all such matrix blocks appear as input-output functions 13 somewhere in the netlist. This means that Equation (4) is satisfied by construction. Assume that we have a solution to Equation (4). Definition 2 constructed the input-output function from the subblock of Equation (4), so there is nothing further to prove. The wiring hypergraph is closely related to the DSEM, but for", "Definition 2 constructed the input-output function from the subblock of Equation (4), so there is nothing further to prove. The wiring hypergraph is closely related to the DSEM, but for constructing the netlist sheaf in Section 3, it is more convenient to use another combinatorial representation. Definition 4. The netlist graph is a vertex- and edge-labeled directed graph that has a vertex for each part, a vertex for each variable, and two edges for each net. The label on a vertex is simply the name of the corresponding part or variable. The two edges for each net are defined as follows. The first edge is labeled with the input port of the net, and leads from that corresponding part to the net. The second edge is labeled with the output port of the net, and leads from that corresponding part to the net. Figure 1(c) shows the netlist graph for the Bering Sea example. Corollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above the parts to which it is connected. Proposition 3. The netlist graph is the incidence bipartite graph of", "set of parts and variables. In the preorder, each variable is above the parts to which it is connected. Proposition 3. The netlist graph is the incidence bipartite graph of the wiring hypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the wiring hypergraph determine each other fully. As we will see, the correspondence between the wiring hypergraph and the netlist graph is convenient. Although Proposition 1 showed that the wiring hypergraph is most closely related to the DSEM, we will later show that the netlist graph is most closely related to the netlist sheaf (Theorem 6). 3.2 Sheaves and cosheaves Sheaves and cosheaves are topological constructions that allow one to study the local consistency structure of a model. In the case of a DSEM, locality is useful because variables that are near one another in the graph are likely to be related. This nearness can be most easily formalized by using the netlist graph defined in the previous section. Since the netlist graph is a directed acyclic graph, it naturally induces a pre-ordered set on the vertices. That is, if a b in a", "defined in the previous section. Since the netlist graph is a directed acyclic graph, it naturally induces a pre-ordered set on the vertices. That is, if a b in a directed graph, we define a b. When the graph is directed and acyclic, generalizing to paths within the graph results in a relation that is reflexive and transitive. Pre-ordered sets have a natural notion of neighborhoods, hence a natural topology. A topological space is a mathematical formalism that captures the notion of neighborhoods. 14 Definition 5. A topology on an arbitrary set X is a collection T of subsets of X satisfying the following four axioms: Empty set The empty set is an element of T , Whole set The set X is an element of T , Finite intersection If U and V are elements of T , then U V is an element of T , and Arbitrary union If U T then U is an element of T . The ordered pair (X, T ) is called a topological space. Often, rather than specifying T directly, we specify a collection of subsets U of X that generate the topology, which is the smallest topology (in the sense", "a topological space. Often, rather than specifying T directly, we specify a collection of subsets U of X that generate the topology, which is the smallest topology (in the sense of inclusion) that contains U. The following are elementary examples of topological spaces, Discrete topology For any set X, let T be the power set of X, Trivial topology For any set X, let T = { , X}, Euclidean topology For X = R, the usual topology T is generated by the set of open intervals (a, b) for a < b R. Additionally, there is a powerful combinatorial theory of topological spaces (X, T ) in which the topology T is a finite set [7]. For our purposes, the most interesting of these finite topological spaces are those that arise naturally from a pre-ordered set, given by the definition below. Definition 6. Suppose that (P, ) is a pre-ordered set, which is to say that is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on (P, ) is the topology generated by all subsets of P of the form Ux = {x y : y P }. The idea of sheaves and cosheaves is that each", ") is the topology generated by all subsets of P of the form Ux = {x y : y P }. The idea of sheaves and cosheaves is that each open set an element of the a topology is associated with a set of values, called the stalk (for sheaves) or costalk (for cosheaves). Definition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on (X, T ) consists of the following specification: 1. For each open set U T , a set S(U ), called the stalk at U , 2. For each pair of open sets U V , there is a function S(U V ) : S(V ) S(U ), called a restriction function (or just a restriction), such that 3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V W ) and 4. S(U U ) is the identity function. 15 Dually, a precosheaf C of sets on (X, T ) consists of the opposite specification: 1. For each open set U T , a set C(U ), called the costalk at U , 2. For each pair of open sets U V", "the opposite specification: 1. For each open set U T , a set C(U ), called the costalk at U , 2. For each pair of open sets U V , there is a function C(U V ) : C(U ) C(V ), called an extension function (or just a extension), such that 3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U V ) and 4. C(U U ) is the identity function. If for every U T there is a pseudometric dU on the (co)stalk at U , and each restriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric spaces. As Definition 7 makes clear, pre(co)sheaves on a topological space are only sensitive to the poset of open sets, and not to the points in those open sets. In our context, the set of values should be interpreted as the set of values that a collection of variables in a DSEM can take. Definition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of", "variables in a DSEM can take. Definition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ). The direct product is in general not the direct sum, since the topology may be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a coassignment supported on U T is an element of ! G C(U ) . U U If U = T , we usually say that the (co)assignment is global. (Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them (co)sections. Definition 9. A global section of a presheaf S on a topological space (X, T ) is a global assignment s such that for all open V U then S(V U ) (s(U )) = s(V ). Dually, a global cosection of a precosheaf C on a topological space is a global coassignment c of the disjoint union under an equivalence, G C(X) = C(U ) / , U open where is the equivalence relation generated by c1 c2", "is a global coassignment c of the disjoint union under an equivalence, G C(X) = C(U ) / , U open where is the equivalence relation generated by c1 c2 whenever c1 C(U1 ), c2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 . Local (co)sections are defined similarly, but refers to some collection U of open sets. 16 Intuitively, a (co)section corresponds to data that is fully consistent with the hypothesis posed by a (co)sheaf. The set of global sections of a presheaf on a topological space may be quite different from S(X). It is for this reason that when studying presheaves over topological spaces, an additional gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves. Definition 10. Let P be a presheaf on the topological space (X, T ). We call P a sheaf on (X, T ) if for every open set U T and every collection of open sets U T with U = U , then P(U ) is isomorphic to the space of sections over the set of elements U. Dually, a precosheaf C is a cosheaf on (X, T ) if for", ", then P(U ) is isomorphic to the space of sections over the set of elements U. Dually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T and every collection of open sets U T with U = U, then C(U ) is isomorphic to the space of cosections over the set of elements U . For the time being, we will focus on sheaves. Cosheaves will reappear in Section 5. Given that most assignments are not sections, it is useful to be able to measure how far away an assignment is from being a section. When we have pseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius. Definition 11. If S is a presheaf of pseudometric spaces on a topological space (X, T ) and a is a global assignment, the p-norm consistency radius of a is the quantity 1/p cS (a) := X X U T , V T :V U p (dV (a(V ), S(V U )a(U ))) , (10) where p 1. In all of our examples, p = 2 is used. A subtle point is that the relative weight of each of", "S(V U )a(U ))) , (10) where p 1. In all of our examples, p = 2 is used. A subtle point is that the relative weight of each of the different terms in Equation (10) is implicitly carried by the pseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean pseudometric could be written dV (x, y) = αV n X k=1 !1/p p |xk yk | , where αV > 0 is a constant that weighs the importance of the value in the stalk on V in the overall consistency radius. In some cases, for instance if different units of measure are involved, the correct choice of αV is clear. In others, the αV is a nuisance parameter that needs to be explored by the modeler. Corollary 4. If s is a global section of a presheaf S of pseudometric spaces, then cS (s) = 0. 17 Consistency radius is stable under perturbations, which means that it can be reliably estimated. Theorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment. We will often need to consider local assignments as well. A natural definition is to define", "5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment. We will often need to consider local assignments as well. A natural definition is to define the consistency radius of a local assignment to be the consistency radius of the best extension of the local assignment to a global one. Definition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a topological space (X, T ) and a is an assignment supported on U T , then its consistency radius is ( ) Y S(U ) such that b(U ) = a(U ) if U U . cS (a; U) := min cS (b) : b U T We will use the phrase minimizing the consistency radius of a as a shorthand for finding the global assignment ( ) Y b := argmin cS (b) : b S(U ) such that b(U ) = a(U ) if U U . U T As the rest of this chapter shows, minimizing the consistency radius of a given local assignment is the primary tool for sheaf-based inference. 3.3 The netlist sheaf The key result of this section is that inference for a DSEM corresponds", "radius of a given local assignment is the primary tool for sheaf-based inference. 3.3 The netlist sheaf The key result of this section is that inference for a DSEM corresponds to consistency radius minimization. In general, it is enabled by Definition 2 that translates a DSEM into a netlist, and Definition 13 that translates a netlist into a sheaf, in such a way that solutions correspond to global sections (Theorem 6). In order to motivate the construction, and to explain some of its subtleties, we delay the formal construction (Definition 13) until after we have discussed two examples. The first example represents a classic linear regression problem first as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf. This progression is summarized in Figure 3. Before delving into the details, let us consider the meaning of the arrows shown in Figure 3. The arrows in each of the frames of Figure 3 mean different things. In the SEM the arrows have a causal interpretation: the value of x determines that of y. This interpretation carries over into the netlist, where ports are either inputs or outputs. In the sheaf diagram the arrows are functions", "the value of x determines that of y. This interpretation carries over into the netlist, where ports are either inputs or outputs. In the sheaf diagram the arrows are functions between the stalks. Since the stalks represent the set of possible values for each variable, the functions represented by the arrows will be used to extract data stored on the ports and place them on the nets regardless of whether they are inputs or outputs. There is no intuitive issue with the outputs. An output variable is determined by the 18 Constraints x x m b x x m b pr1 x n pr2 pr3 y = mx + b y = mx + b y y y y y n (a) (b) (c) (d) n f Assignment support Figure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded coefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To solve the linear regression problem, the partial assignment supported on the darkest shaded region is supplied by the observations, and then the assignment is extended to the remaining stalks. Finally, the copies of m, b, and x that should be constrained so", "shaded region is supplied by the observations, and then the assignment is extended to the remaining stalks. Finally, the copies of m, b, and x that should be constrained so that they are identical are shown by the three lighter shadings. data within the part it is attached to. However, for an input, the only thing the arrow does is extract the corresponding port s value unmodified. This seems paradoxical! The point is that when two parts are connected to each other on a net, they both have a claim on what the value of the variable should be. If the values correspond to a global section of the sheaf, this is the assertion that both claims on that variable agree, namely the variable produced by the output of one port is the same as the variable that reaches the input port attached to the same net. Beginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n points in the plane R2 . As a modeling choice, we suppose that the x values can be used to predict the y values, or alternatively that x is an explantory variable", "the plane R2 . As a modeling choice, we suppose that the x values can be used to predict the y values, or alternatively that x is an explantory variable and y is a response variable. If we assert that the model should be linear, we are assuming y b + mx, where b and m are parameters to be found. To express this modeling assumption graphically, we write an arrow x y, yielding the SEM graph in Figure 3(a). The netlist for the problem represents the same information as in the SEM. As shown in Figure 3(b), the netlist consists of two variables (x and y), and one part (the linear equation that predicts y from x). The prediction process depends on the two parameters b and m, which can also be considered as inputs. This change results in a netlist with four variables (x, y, b, and m) and the same part as before, shown in Figure 3(c). The sheaf representation of the same system is shown in Figure 3(d). It is considerably more explicit about variable type information. The stalk over m and b is R, since each of these parameters takes a real value. On the", "Figure 3(d). It is considerably more explicit about variable type information. The stalk over m and b is R, since each of these parameters takes a real value. On the other hand, 19 the stalk over x and y is Rn , since they are each a sequence of n real values. The stalk over the single part is the set of its inputs, namely R R Rn , corresponding to m, b, and x, respectively. The restriction maps from the part to the inputs are all projection maps, which select the different inputs. Explicitly, pr1 (m, b, (x1 , . . . , xn )) = m, pr2 (m, b, (x1 , . . . , xn )) = b, and pr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ). The remaining restriction map f shown in Figure 3(d) performs the prediction process, and is given by (y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b). (11) The function f applies the common coefficients (b and", "b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b). (11) The function f applies the common coefficients (b and m) to each of the input values xk to yield the corresponding output values yk . The space of global assignments for the sheaf shown in Figure 3(d) is given by the product of all of the stalks. This means there are two copies of m, b, and x in the space of global assignments, one for the value of the variable and one as a component of the part. A typical global assignment a is of the form a := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m, e eb, (f x1 , . . . , x fn ) , (12) where we have listed the four variables first followed by the part. The consistency radius of this assignment is c(a) = p p |m e m| + |eb b| + n X k=1 p |f xk xk | + n X k=1 !1/p p |b + mf x k yk | (13) for a given p.", "+ |eb b| + n X k=1 p |f xk xk | + n X k=1 !1/p p |b + mf x k yk | (13) for a given p. In what follows, we will take p = 2, so as to agree with classical linear regression. The problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius subject to the constraint that each pair of copies of m, b, and x is equal, and that only m and b are allowed to vary will recover linear regression from the sheaf. These copies are identified in the lighter shaded regions in Figure 3(d). To follow the paradigm of consistency radius minimization, we specify a local assignment to the variables x and y, and then extend the assignment to a global one. The support of the local assignment is expressed by the darkest shaded region in Figure 3(d). Notice that the nets have no higher elements in the partial order shown in Figure 3, so the support of this assignment is U = {{x}, {y}}. Explicitly, we start with a non-global assignment supported on U, ( , , (x1", "order shown in Figure 3, so the support of this assignment is U = {{x}, {y}}. Explicitly, we start with a non-global assignment supported on U, ( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) , 20 (14) where the dashes indicate stalks outside the support of the assignment. If we seek a global assignment g such that g = argmin {c(b) : g(U ) = a(U ) for U U}, this means that we wish to find the entries in the assignment in Equation (12) that are marked with the dashes in Equation (14), namely m, e eb, m, b, and (f x1 , . . . , x fn ). Minimizing consistency radius is therefore given by the problem argmin m, e e b,m,b,(x1 ,...,xn ) |m e m|2 + |eb b|2 + n X k=1 |f xk xk |2 + n X k=1 !1/2 |b + mf x k y k |2 But since both m e and m, and eb and b are being minimized, the consistency radius reduces to !1/2 n n X X 2 2 argmin m,b,(x1 ,...,xn ) |f xk xk |", "e and m, and eb and b are being minimized, the consistency radius reduces to !1/2 n n X X 2 2 argmin m,b,(x1 ,...,xn ) |f xk xk | + |b + mf x k yk | . k=1 k=1 This permits the values of the variables x and y to differ from their copies, subject to a penalty. Instead of least squares regression, this problem is what is usually called total least squares; see Figure 4. After minimization, the differences between each of the copies |f xk xk | expresses the uncertainty of their values if the model is to be taken as a given. To obtain classical least squares regression, we must constrain x fk = xk for all k. The global assignment we seek is of the form g = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) , so that the consistency radius minimization problem subject to this constraint becomes !1/2 n X 2 argmin m,b |b + mxk yk | . k=1 Consistency radius minimization unifies several different inference tasks in Figure 3, depending", "subject to this constraint becomes !1/2 n X 2 argmin m,b |b + mxk yk | . k=1 Consistency radius minimization unifies several different inference tasks in Figure 3, depending on the support of the initial assignment: Forward prediction Choose an assignment supported on x, b, and m, of the form (m, b, (x1 , . . . , xn ), , ) . Consistency radius minimization will infer the values for y. Because the above assignment extends to a global section, namely, (m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) , consistency radius minimization does not require constraints in this case. 21 . y y1 y = mx + b b + mx~1 unconstrained consistency radius b + mx1 constrained consistency radius x x1 x~1 Figure 4: Geometric meanings of the terms contributing to consistency radius in Equation 13. Backward prediction Choose an assignment supported on y and b, and m, of the form (m, b, , (y1 , . . . , yn ), ) . Consistency radius minimization will infer the values for", "on y and b, and m, of the form (m, b, , (y1 , . . . , yn ), ) . Consistency radius minimization will infer the values for x. If m = 0, this always results in a global section, (m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) , so consistency radius minimization does not require constraints. If m = 0 then the minimizers of consistency radius all have the same consistency radius, and are assignments of the form (0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) . Noting that the two copies of the x variable are always identical, applying constraints does not change the result. Regression (model fitting) (Details above, included for completeness here.) Choose an assignment supported on x and y, of the form ( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) . Consistency radius minimization will infer the values for b and", ", , (x1 , . . . , xn ), (y1 , . . . , yn ), ) . Consistency radius minimization will infer the values for b and m. As noted above, without constraints consistency radius minimization solves total least squares, while constraints are necessary to recover classical regression. 22 Constraints pr1 ... pr3 pr2 prn+2 f1 f2 n Assignment support fn ... Figure 5: Modification to the sheaf in Figure 3(d) to allow for missing data. Hybrid versions of the above problems can also be addressed. Assignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly requires that we have access to all of the n data points, since the stalks for x and y are each Rn . If there is missing data, a different sheaf construction is possible, in which each separate component of x and y is given its own stalk. Figure 5 shows the resulting construction. The fk restriction maps appearing in Figure 5 are the individual components of the f restriction map in Figure 3(d), namely given Equation (11), yk = fk (m, b, (x1 , . . . , xn )) = mxk + b. The set of global assignments", "map in Figure 3(d), namely given Equation (11), yk = fk (m, b, (x1 , . . . , xn )) = mxk + b. The set of global assignments for the sheaf in Figure 3(d) is the same as that for the sheaf in Figure 5, but its components are delineated differently. A typical global assignment a for the sheaf in Figure 5 is given by a := m, b, x1 , . . . , xn , y1 , . . . , yn , m, e eb, x f1 , . . . , x fn , where the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both sheaves is given by exactly the same formula. As in the previous sheaf, we can express the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown by the darkest shaded regions in Figure 5) is extended to a global assignment, subject to the constraint that each of the copies of the duplicated variables are identical (shown by the three lighter shaded regions", "Figure 5) is extended to a global assignment, subject to the constraint that each of the copies of the duplicated variables are identical (shown by the three lighter shaded regions in Figure 5). But now, if there is a missing xk or yk value, this can simply be excluded from the support of the initial assignment, leaving the specification of the task as a consistency radius minimization unchanged. Feedback connections are easily represented in all of the frameworks under consideration. Moreover, depending on the set of variables that are permissible, the resulting sheaf will or will not have global sections (Definition 9). 23 X x x out f g g id X X id f in g f in out y y X (a) (b) (c) Figure 6: Feedback connections can be handled: (a) a (D)SEM model with feedback, (b) its netlist, (c) its sheaf representation. Consider the setting shown in Figure 6: X = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the two variables have the same value. X = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both variables to be 0. X", "two variables have the same value. X = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both variables to be 0. X = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2. X = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist. Feedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.4. With the preliminary intuition established by the previous two examples, we are now in a position to discuss the general translation algorithm. Definition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net is the set of variables for that net. The stalk on each part is the product of its input ports. The restriction from a part to a net along an input port is the projection function for the corresponding variable set. The restriction from a part to a net along an output port is the", "to a net along an input port is the projection function for the corresponding variable set. The restriction from a part to a net along an output port is the function that computes the output variable from the set of input variables. It is often useful to have individual observations on their own stalks, like we did in Figure 5. The following modification to Definition 13 allows for missing data in general. Definition 14. Starting with a netlist sheaf as defined in Definition 13, add an additional element to the preorder of the netlist graph for each observation of each variable. These elements are located above their respective variables in the preorder. The restriction map from each variable to each observation is the projection that selects the corresponding observation from its parent timeseries. 24 x1, ... xn S in a1, ... ak coef LCF(k) pr2 k S out yn = a1 xn-1 + a2 xn-2 + ... ak xn-k (a) pr1 k S (b) Figure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist wiring hypergraph and (b) netlist sheaf. Theorem 6. Variable values on the netlist correspond bijectively to DSEM solutions and to", "LCF(k) with a sliding window size k as (a) netlist wiring hypergraph and (b) netlist sheaf. Theorem 6. Variable values on the netlist correspond bijectively to DSEM solutions and to global sections. Proof. (see also [34][Prop. 6]) There is a direct correspondence between the values of variables on the nets and the nodes in the DSEM. If these are values correspond to a solution, then they directly imply consistency with the restriction maps. Moreover, according to [35, Thm. 1] there is stability in consistency radius when we perturb away from a consistent set of variables. This is classical in the case of the linear regression example, because the linear regression coefficients m and b are stable with respect to perturbations in the data variables x and y. 3.4 Sheaves modeling autoregressive timeseries Autoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of the form xn = a1 xn 1 + a2 xn 2 + + ak xn k , for some fixed a1 , . . . , ak . We say that such a sequence is AR(k) autoregressive. Autoregressive timeseries can be modeled using the graphical framework being developed", "some fixed a1 , . . . , ak . We say that such a sequence is AR(k) autoregressive. Autoregressive timeseries can be modeled using the graphical framework being developed in this chapter by the use of feedback connections. It is easiest to see how the construction of autoregressive timeseries works by starting with a one-step delayed Linear Causal Filter with sliding window size k (which we write as LCF(k) for short in diagrams). Like the linear regression example from the previous section, a variable x is considered an explanatory variable that predicts the values of a response variable y. This prediction is given by yn = a1 xn 1 + a2 xn 2 + + ak xn k where the a1 , . . . ak are constants. We can realize this equation as a netlist with an input for x, an input for a, and an output for y shown in Figure 7(a). Using Definition 13, we obtain the 25 ... x1, ... xn s out in identity LCF(k) coef a1, ... ak out in id pr2 s k s id xn = a1 xn-1 + a2 xn-2 + ... ak xn-k s (a) (b) pr1 k Figure", "LCF(k) coef a1, ... ak out in id pr2 s k s id xn = a1 xn-1 + a2 xn-2 + ... ak xn-k s (a) (b) pr1 k Figure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries. netlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of real numbers. To handle autoregressive timeseries, we merely need to consider the pair of equations ( yn = a1 xn 1 + a2 xn 2 + + ak xn k , xn = yn . This is implemented as a netlist with two parts and a feedback connection, as shown in Figure 8(a), where again S is the set of infinite sequences of real numbers. The linear causal filter part is the same as before, but the identity part implements the second equation above. Error terms are not explicitly mentioned, because they are accounted for in the consistency radius calculation (Equation (10)). The associated netlist sheaf is shown in Figure 8(b). Again, consistency radius measures how well the data x fit the model given with coefficients a. Following a theme already present in the linear regression example, there is duplication of data", "consistency radius measures how well the data x fit the model given with coefficients a. Following a theme already present in the linear regression example, there is duplication of data in the sheaf model. Indeed, the values of x are effectively duplicated in four places: the x and y = x variables, and in the two parts. Once again, if we consider an assignment supported on the two variables (with the same values on each!), minimizing consistency radius will infer the values of the a coefficients. Once again, if we run an unconstrained optimization, this assumes that some uncertainty is permitted in the values of x. When the timeseries are finite in length, the equation defining an AR(k) sequence cannot represent any of the first k time steps. Therefore, instead of the identity part in Figure 8, the sheaf for an AR(k) sequence of length n must crop off the first k components of the vector in the stalk, resulting in a sequence of length n k. The resulting construction is shown in Figure 9, where we note that a slight abuse of definition occurs in Figure 9(a) because the two outputs are connected to each other. While this means", "is shown in Figure 9, where we note that a slight abuse of definition occurs in Figure 9(a) because the two outputs are connected to each other. While this means that the netlist is not valid as such, the sheaf constructed in Figure 9(b) correctly represents an autoregressive sequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k) sequences of length n. 26 x1, ... xn n in in crop LCF(k) a1, ... ak coef out out id pr2 n k n prk..n xn = a1 xn-1 + a2 xn-2 + ... ak xn-k n-k (a) (b) pr1 k Figure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries. n in k n DietCope_lag Copepods in pr2 out id crop n h DietCope_block out LCF(k) prk..n n - k n Diet_Cop (a) (b) Figure 10: Modification to Figure 1(d) to support autoregressive timeseries, shown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in Figure 13. 27 Autoregressive sequences can be modeled in the sheaf shown in Figure 1(d), our ecological example. All that is needed is a modification to", "Figure 1 resulting in Figure 13. 27 Autoregressive sequences can be modeled in the sheaf shown in Figure 1(d), our ecological example. All that is needed is a modification to each variable in the netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in Figure 1(b) must be duplicated to represent a lagged copy of the variable, and there must be a new part added for each variable to perform the autoregression itself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part is cropped to be only the most recent samples (since the timeseries is finite), and then that is what is attached to the output port of the LCF part. The transformation that is required for the Copepods variable is shown in Figure 10.", "4 Sheaf encoding of the Bering Sea We now return to the ecological DSEM example introduced in Section 2.2, and refer the reader to Figure 1. The reader is directed to [36] for the software that generates the sheaf results presented in this section. The DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its netlist sheaf is shown in Figure 1(d). The netlist sheaf in Figure 1(d) does not express the path coefficients as variables, as they are instead hard coded within each part. Nevertheless, if the path coefficients are known (for instance, they can be taken from [46]), then the sheaf model can be used to predict the values of each of the variables, starting from SeaIce and Spawners. If we apply the modification to the sheaf to require AR(1) timeseries so that missing data values are interpolated, and use the path coefficients stated in [46] (see Table 3), the resulting timeseries are shown in Figure 11. The DSEM was constrained to fit the measurements exactly, whereas the sheaf had no such constraints applied. Where the sheaf differs from the measurements, the", "are shown in Figure 11. The DSEM was constrained to fit the measurements exactly, whereas the sheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the value of the variable at the given time. This uncertainty is composed of both the measurement and exogenous errors; the sheaf model does not distinguish between the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected mean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because both the sheaf and the DSEM approach are approximations to the same DSEM solution. There are some differences on the behavior of the earlier inferred data, because many of the observations are missing there. In these regions, the sheaf tends to yield somewhat less variable predictions than the DSEM (except in the case of the Krill variable). As noted earlier, we will compute consistency radius using the Euclidean p = 2 norm. Lacking other information, we chose to weight the terms in Equation (10) equally. The consistency radius of", "earlier, we will compute consistency radius using the Euclidean p = 2 norm. Lacking other information, we chose to weight the terms in Equation (10) equally. The consistency radius of the assignment after minimization is 28 SeaIce ColdPool ln (ColdPool ) [ln(km2 )] ln (SeaIce) [ln(km2 )] 1 0.5 0.0 0.5 1.0 0 1 2 2018 ColdPool 3 DietCopepods ln (DietCopepods) [ ] 3 ln (Copepods) [ln(count/m )] Copepods 2.5 0.0 2.5 5.0 1 0 1 2 DietKrill 3 ln (Krill ) [ln(count/m )] Krill ln (DietKrill ) [ ] 0.5 0.0 0.5 2016 Krill 1.0 0.5 0.0 0.5 1.0 Spawners Survival ln (Survival ) [103 count/kg] Spawners[106 kg] 4 3 2 1 1960 1970 1980 1990 2000 2010 2020 measurement 2 1 0 1 2 1960 DSEM 1970 1980 1990 2000 2010 2020 sheaf Figure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM was constrained to fit the measurements exactly, whereas the sheaf had no such constraints applied. 29 Copepods_pc Copepods n pr2 pc in DietCope_block pr1 out Diet_Cop n g~1 n (a) (b) Figure 12: Modification to the netlist to include path coefficients and", "such constraints applied. 29 Copepods_pc Copepods n pr2 pc in DietCope_block pr1 out Diet_Cop n g~1 n (a) (b) Figure 12: Modification to the netlist to include path coefficients and constants as an input. 11.9. Since this is not zero, this means that the fit between the data and the model is not perfect. While the DSEM fits the data for maximum likelihood, the sheaf fits for minimum inconsistency. This difference in optimization task results in the observed differences between the sheaf and the DSEM. Taking a cue from Figure 3 in the previous section, we can break out path coefficients as separate variables so that they can be adjusted or estimated. Figure 12 shows how one of the parts in the netlist shown in Figure 1(b) can be modified so that its path coefficients are inputs. To handle missing data, we apply Definition 14 to the netlist sheaf, which results in Figure 13. Using the sheaf shown in Figure 13, we can infer the path coefficients and autoregressive coefficients by consistency radius minimization. Specifically, we construct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency", "by consistency radius minimization. Specifically, we construct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency radius, the values of the path coefficients, autoregressive coefficients, and any missing observations will be inferred. The resulting global assignment has a complete timeseries no missing observations for each variable as well as path coefficients and autoregressive coefficients. Because the approach explained in Section 2.1 uses a different strategy for approximating solutions to the problem posed by the DSEM, the inferred path coefficients and missing observations will be somewhat different from those inferred by the sheaf. There are some differences between the sheaf and the measurement data. The contributions to consistency radius are not uniformly distributed over the sheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to the fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the consistency radius are 1. the autoregression cell for Copepods (labeled Copepods lagvar ), and 2. the year 2018 observations of", "it is shown that the two largest contributors to the consistency radius are 1. the autoregression cell for Copepods (labeled Copepods lagvar ), and 2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ). The second of these is easier to interpret. We should suspect that the 2018 observation of ColdPool is an outlier (in the L2 sense) from what was expected 30 SeaIce SeaIce_lag SeaIce_lagvar ColdPool_block ColdPool_lagvar SeaIce_pc ColdPool_lag ColdPool 2018_ColdPool ColdPool_Copepods_pc ColdPool_Krill_pc Krill_block Copepods_block 2016_Krill Copepods Krill Copepods_lag Copepods_pc DietCopepods_block Copepods_lagvar Krill_lag Krill_pc DietKrill_block Krill_lagvar DietCopepods Spawners DietKrill DietCopepods_pc DietKrill_pc DietCopepods_lag DietKrill_lag Spawners_pc DietCopepods_lagvar Spawners_lag DietKrill_lagvar Spawners_lagvar Survival_block Survival cells restrictions projection map other function (see text) inferred variable (shown in Fig.11) observed variable highlighted in Fig.11 pseudometric not present pseudometric present 0 2 4 consistency radius contribution Figure 13: The full sheaf for the DSEM described in Section 2.2. Its structure reflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells represent inferred variables, with the variable names shown in italics. Variable names that are also bold correspond to variables plotted in Fig. 11. White cells represent variables that are observed. All observed variables except for two are not labeled for clarity.", "names that are also bold correspond to variables plotted in Fig. 11. White cells represent variables that are observed. All observed variables except for two are not labeled for clarity. The two that are labeled have their names in white italics with black backgrounds. These variables exhibit relatively large contributions to the consistency radius and are highlighted in Fig. 11. 31 Source Target SeaIce ColdPool ColdPool Copepods ColdPool Krill Copepods DietCopepods Krill DietKrill DietCopepods Survival DietKrill Survival Spawners Survival Consistency radius Runtime (s) DSEM [46] AR(1) 0.6 1.79 0.18 0.29 0.06 0.15 0.13 0.59 11.9 2 none 1.68 4.45 0.44 0.32 0.52 0.50 7.56 0.82 6.60 2848 Sheaf AR(1) AR(2) 1.81 1.78 4.38 4.47 0.38 0.41 0.35 0.36 0.70 0.65 0.12 0.05 5.29 7.19 0.65 0.55 9.48 9.03 2637 2679 AR(10) 1.74 4.17 0.39 0.34 0.56 0.32 5.63 0.74 7.93 2907 Table 3: Comparison between path coefficients estimated from the DSEM and the sheaf from the model, and that these differences may have propagated into other parts of the model. This probably explains why the 2018 observations of Krill and DietKrill are substantially different from the sheaf predictions in Figure 11. We should interpret the largest contributor to consistency radius", "This probably explains why the 2018 observations of Krill and DietKrill are substantially different from the sheaf predictions in Figure 11. We should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries. Notice that the Copepods observations contribute equally to consistency radius, since the small white diamonds encircling the Copepods variable are about the same size. This suggests that it is simply that the assumption of Copepods being represented by an AR(1) timeseries is faulty, rather than any particularly bad observation. Table 3 shows the path coefficients inferred by the DSEM (using maximum likelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by the sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for space considerations.) The DSEM-derived path coefficients were obtained using the assumption of AR(1) timeseries. Several different sheaves were constructed with autoregressive sequences of different window sizes. As a consequence of the construction of consistency radius, minimizing consistency radius infers the following information: (1) missing observations in any variable, (2) all path coefficients, and (3) autoregressive coefficients for each", "consequence of the construction of consistency radius, minimizing consistency radius infers the following information: (1) missing observations in any variable, (2) all path coefficients, and (3) autoregressive coefficients for each variable. There is broad agreement about the values of the path coefficients between the sheaves with different autoregressive window sizes, and some agreement between the DSEM and the sheaves. Since the DSEM does not natively imply a consistency radius, the consistency radius shown for the DSEM is that for the sheaf using AR(1) timeseries and the hard-coded path coefficients as shown. Because the consistency radius minimization process on that sheaf cannot adjust the path coefficients it can only adjust the missing observation values and the autoregressive coefficients the consistency radius is notably higher in this case. Some caution in comparing consistency radius across the columns of Table 32 Variable ColdPool SeaIce Copepods Krill Spawners DietCopepods DietKrill AR(1) lag 1 0.582 0.361 0.828 0.692 1.01 0.886 0.060 AR(2) lag 1 lag 2 0.480 0.202 0.287 0.190 1.16 -0.442 0.308 0.411 1.78 -0.768 1.68 -0.924 0.0596 0.0445 Table 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2) models. 3 is needed. The number of terms in the consistency radius is", "-0.768 1.68 -0.924 0.0596 0.0445 Table 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2) models. 3 is needed. The number of terms in the consistency radius is the same for each of the sheaves in all but the non-autoregressive case (the fourth column from the left). This is because the autoregressive coefficients and timeseries are bundled as shown in Figure 9. Naturally enough, the non-autoregressive sheaf s consistency radius contains no terms pertaining to the autoregressive coefficients, and so is expected to be smaller than the others. The sheaf column listed as none means that no autoregressive timeseries assumptions were applied. Because with no autoregressive assumptions in play, the resulting sheaf diagram is smaller, consequently the consistency radius is smaller. Interestingly, the consistency radius is smallest for the AR(10) case, which suggests that more flexibility in the autoregressive coefficients leads to somewhat better prediction accuracy in the measurement data. Runtimes shown in Table 3 are representative when run on an Intel Core Ultra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited and consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower than the DSEM. This is", "GHz with 32 GB RAM. The process was not memory limited and consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower than the DSEM. This is because the DSEM solves a sparse linear problem, while the sheaf methodology supports fully nonlinear, non-convex problems. The sheaf software does not attempt to detect whether the problem is linear, so the consistency radius minimization is always performed as a nonlinear, non-convex optimization problem.", "5 The topology of subsystems Classically, dynamical systems have been studied using the structure of invariant sets. These are subsets of the space of variable values that are preserved by the action of the dynamical system. This section shows that invariant sets are one half of a duality pair. We can take two different perspectives of a multi-scale dynamical system: invariant sets (which lead to cosheaves) versus subsystems (which lead to sheaves). We will establish that a dynamical system induces a cosheaf of invariant sets. The cosheaf of invariant sets breaks the global state of the system into different regimes of behavior, which are parameterized by the open sets of the 33 base space topology. Conversely, there is also a sheaf of subsystems that splits the variables into nested collections that each act independently. We will formalize the topology of subsystems as a finite topological space, by using the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked together according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the preorder yields more abstracted high-level systems. This is not entirely compatible with", "to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the preorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised! (The intuition of the presentation here is compatible with Kearney et al. [22], where the system is modeled as a graph. In Kearney et al. [22], vertices are the loci of state variables, and are above edges in the preorder constructed in that paper. Our presentation is also compatible with Steward [43], after transitive closure.) 5.1 Dynamical systems Definition 15. A dynamical system is a continuous bijection f : S S. The set S in this case is called the set of states of the dynamical system. It is a classical fact that for a fixed timestep, the solutions to a smooth first order differential equation of the form (1) induce a dynamical system [44]. As a consequence, the DSEM, netlist, and sheaf models of the previous sections represent dynamical systems. Definition 16. For a dynamical system f : S S, a subset V S is called an invariant set if f (V ) V. Corollary 7. If V is", "systems. Definition 16. For a dynamical system f : S S, a subset V S is called an invariant set if f (V ) V. Corollary 7. If V is an invariant set of f : S S, then f restricts to a function f : V V . Definition 17. Suppose that A B. The inclusion is the function i : A B is a function such that i(x) = x for every x A. Notice that (i|A) i = i. Dually, a projection is a function p : B A such that p p = p and p|A = id A . Proposition 8. Suppose that U and V are two invariant sets for a dynamical system f : S S and that U V . Then the following diagram U f i V f /U /V i commutes, where i and i are appropriate inclusion maps, which is to say that f i = i f. 34 Proof. Suppose that x U . Since U is an invariant set, f (U ) U . However, since U V , x V . Therefore, f (x) V because V is also an invariant set. Definition 18. The category", "invariant set, f (U ) U . However, since U V , x V . Therefore, f (x) V because V is also an invariant set. Definition 18. The category Dyn of dynamical systems has as its objects dynamical systems. Each morphism of Dyn is a commutative diagram of the form f1 / S1 S1 g g S2 f2 / S2 Composition of morphisms is given by composing the g functions. Proposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems. 5.2 The cosheaf endomorphism of invariant sets The state space of a dynamical system can be decomposed as the (non-disjoint) union of all its invariant sets. This collection of invariant sets of a dynamical system is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be defined to capture the relationship between an invariant set and the invariant sets that contain it. To this end, the cosheaf identifies duplicate points within these invariant sets with each other. We begin by observing that the invariance of a collection of subsets with respect to a dynamical system is not necessary to define a cosheaf; it can", "with each other. We begin by observing that the invariance of a collection of subsets with respect to a dynamical system is not necessary to define a cosheaf; it can be constructed generally. Lemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set X. Consider the inclusion partial order on U, given by U V whenever U V . Define the following precosheaf CU on the Alexandrov topology of the inclusion partial order (U , ): 1. CU (U ) = U 2. CU (U V ) = CU (U V ) : U V via the inclusion map. Then CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial order (U, ). Proof. Suppose that V U, and that V U is a collection of subsets with V = V. We need to establish that the space of global cosections on V is identical to CU (V ) = V . The space of global cosections on V is ! ! G G [ CU (W ) / = W / = W = V = V, W V W V W V since the equivalence identifies points that agree", "! G G [ CU (W ) / = W / = W = V = V, W V W V W V since the equivalence identifies points that agree on overlaps. 35 The above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of morphisms for sheaves and cosheaves. Definition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ), and that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY such that the following diagram commutes whenever U, V TY and U V , R(f 1 (V )) R(f 1 (U ) f 1 (V )) mV / S(V ) S(U V ) R(f 1 (U )) mU / S(U ) Dually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf morphism m : R S is a collection", ") Dually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf morphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such that the following diagram commutes whenever U, V TY and U V , R(f 1 (V )) O mV / S(V ) O R(f 1 (U ) f 1 (V )) S(U V ) R(f 1 (U )) mU / S(U ) With the definition of morphisms in hand, we can now establish that the cosheaf construction in Lemma 10 is functorial. Lemma 11. There is a functor Top CoShv that takes a topological space (X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U V ) is the inclusion U , V . Proof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined cosheaf on (X, T ). Suppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to a cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V", "that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to a cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two open sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X. Therefore, the following diagram commutes C(X,TX ) (f 1 (U )) = f 1 (U ) FU :=f |U C(X,TX ) (f 1 (U ) f 1 (V )) C(X,TX ) (f 1 (V )) = f 1 (V ) / C(Y,T ) (U ) = U C(Y,TY ) (U V ) FV :=f |V / C(Y,T ) (V ) = V Y which establishes definitions for the component maps of F , and therefore that F is a cosheaf morphism. 36 Now suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and g : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of cosheaf morphisms G F is the equal to the one induced by (g f ). This follows immediately because the components maps of the cosheaf morphism G F are simply", "cosheaf morphisms G F is the equal to the one induced by (g f ). This follows immediately because the components maps of the cosheaf morphism G F are simply restrictions of the composition (g f ). Suppose that f : S S is a dynamical system. The invariant sets of f are indeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant sets of f . Proposition 12. A dynamical system f : S S induces an morphism m : S S on the cosheaf of invariant sets, and for which the induced map on global cosections is mS = f . Proof. Suppose that U is an invariant set of f . Let mU : U U be the restriction of f to U . If U V are two invariant sets, then Proposition 8 implies that U mU =f /U i V mV =f /V i commutes, where i is the inclusion map. It is immediate that this is exactly the condition that the m maps are the components of a cosheaf morphism. Moreover, since S is itself an invariant set, the proof", "It is immediate that this is exactly the condition that the m maps are the components of a cosheaf morphism. Moreover, since S is itself an invariant set, the proof is complete. 5.3 Subsystem decomposition sheaf Rather than carving up the state space into different regimes of behavior, we can instead carve it into non-interacting collections of variables. In this way, we arrive at the subsystem sheaf instead of the invariant set cosheaf. The global sections combine variables together into vectors, whereas global cosections paste subsets of values together. Dualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant set for f . If i : U S is the inclusion map, then the diagram at left below commutes: f f /S /S SO S O i U p i f |U B /U p g /B Dually, the diagram at right above captures the situation where B is a subsystem of f . 37 Definition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p) consisting of a dynamical system g : B", "of f . 37 Definition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p) consisting of a dynamical system g : B B and a surjection p : S B such that p f = g p. We will call p the subsystem projection. When p is clear from context, we will often say g is a subsystem of f . We can think of the function g as a dynamical system in its own right. The idea of a subsystem is neatly compatible with the DSEM construction. As will be shown later in Corollary 21, when the DSEM graph is acyclic, the subsystems can be read off directly. For the moment, a few examples will build the necessary intuition. Example 2. Consider the DSEM with two variables A and B, given by the graph with one edge A B. The variable A is a subsystem on its own, whereas B cannot be a subsystem on its own because its value cannot be predicted from B alone. As a result, there are two nested subsystems: {A} and {A B}. To see this explicitly, suppose that the values of A are given by", "be predicted from B alone. As a result, there are two nested subsystems: {A} and {A B}. To see this explicitly, suppose that the values of A are given by the timeseries {an } and the values of B are given by the timeseries {bn }, with the prediction of B from A given by the formula bn+1 = β(an , an 1 , . . . ). The dynamical system implied by this DSEM is represented by shifting the timeseries by one timestep. Specifically, the dynamical system is given by the function f : A B A B given by f (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ). Because of this formula, it should be clear that {B} cannot be a subsystem because the values of the {bn } timeseries depend on the values of {an }. Under a projection that", "formula, it should be clear that {B} cannot be a subsystem because the values of the {bn } timeseries depend on the values of {an }. Under a projection that removes the {an } from the domain, the values of {bn } cannot be determined. The subsystem {A} arises using the subsystem projection p : A B A, namely p(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ). The subsystem dynamical map g : A A is simply g(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ). Verification that (g, p) is a subsystem is then simply a calculation, (p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ),", ". . . ) = p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ) = (. . . , an+1 , an , . . . ) = g(. . . , an , an 1 , . . . ) = (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ). 38 Example 3. ?B A C Following the logic of Example 2, the subsystems are {A}, {A B}, {A C}, and the original system. Example 4. Consider the DSEM with three variables A, B, and C given by the graph A ?C B Following the logic of Example 2, the subsystems are {A}, {B}, and the original system. Notice that {C} cannot be a subsystem on its own because its values are determined by both A and B. When a dynamical system is described by a DSEM with feedback, there are often fewer subsystems because the values of the variables cannot", "are determined by both A and B. When a dynamical system is described by a DSEM with feedback, there are often fewer subsystems because the values of the variables cannot be determined in isolation. Example 5. Consider the DSEM on variables A and B given by the graph ) Ah B (See also Figure 6 for the sheaf model.) In this case, the only subsystem is the entire system, because the values of A cannot be determined without knowing B, and conversely the values of B cannot be determined without knowing A. Linear systems are special because invariant sets and subsystems reduce to the same thing, as the next example shows. Example 6. Let V be a finite dimensional vector space and f : V V be a linear isomorphism. If we use the usual Euclidean norm on V , f is continuous, so it is also a dynamical system. Subsystems and invariant subspaces of f are in bijective correspondence. To see this, suppose that v V is an eigenvector for f , namely f (v) = λv 39 for some λ. Then the subspace spanned by v is an invariant set. Conversely, every invariant set of f is a", "for f , namely f (v) = λv 39 for some λ. Then the subspace spanned by v is an invariant set. Conversely, every invariant set of f is a linear subspace, spanned by a set of eigenvectors (possibly with complex eigenvalues). Since V was assumed to be finite dimensional, every subspace W V also has an associated orthogonal projection prW : V W . If W is an invariant set for f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which can be written as the decomposition u + w, where w W and prW (u) = 0. Because f is a linear isomorphism, the assumption on u means that prW (f (u)) = 0. All that remains is to verify that the definition of subsystem holds, (prW f )(v) = prW (f (u + w)) = prW (f (u) + f (w)) = prW (f (u)) + f (w) = f (w) = (f |W ) (w) = (f |W ) (prW (u + w)) = (f |W prW )(v). Lemma 13. The relation is a subsystem of is a preorder, or in other words a reflexive, transitive relation.", "|W ) (prW (u + w)) = (f |W prW )(v). Lemma 13. The relation is a subsystem of is a preorder, or in other words a reflexive, transitive relation. Proof. Suppose that f : S S is a dynamical system. Reflexivity follows immediately by taking (f, id S ) as a subsystem. For transitivity, suppose that (g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we have the commutative diagram f S p2 p1 p2 B2 p2 g2 p1 B1 /S / B2 p1 p2 p1 g1 / B1 so that (g1 , (p1 p2 )) is a subsystem of f . Intuitively, the preorder specifies how data can flow from one subsystem to the next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is also a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of g2 . Example 7. Consider the dynamical system f : Z3 Z3 given by f (x, y, z) := ((1 x), y(1 x) + zx, z(1", "influence the state of g2 . Example 7. Consider the dynamical system f : Z3 Z3 given by f (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx). 40 This has a nontrivial subsystem pr1 : Z3 Z, since the map g(x) := 1 x makes the following diagram commute Z3 pr1 Z f / Z3 pr1 g /Z In this case, the x variable in the subsystem acts as an input to the overall system, even though its behavior is isolated from the rest of the system. It is not necessarily the case that subsystems are invariant sets. Example 8. Consider the dynamical system f : R2 R2 , given by f (x, y) := (x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem, since the following diagram commutes R2 f p B / R2 p id /B where p(x, y) = (x, 0), even though the set B is not an invariant set. However, conversely, invariant sets of subsystems do determine invariant sets of their parent system. Lemma 14. Suppose that f : S S is a dynamical system with g : B B is", "invariant sets of subsystems do determine invariant sets of their parent system. Lemma 14. Suppose that f : S S is a dynamical system with g : B B is a subsystem with subsystem projection p : S B. If V B is an invariant set of g, then p 1 (V ) is an invariant set of f . Proof. The hypotheses posit a commutative diagram of the form S f p B /S p g /B Suppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the commutative diagram above. Noting that p(x) V by construction, and that V is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so f (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f . 41 Lemma 15. Suppose that f : S S is a dynamical system and that Y S is an invariant set for f . If g : B B is a subsystem of f with subsystem projection p, then g is also a subsystem of f |Y . Proof. Suppose that i : Y S", "g : B B is a subsystem of f with subsystem projection p, then g is also a subsystem of f |Y . Proof. Suppose that i : Y S is the inclusion map. The hypotheses state that the diagram of solid arrows below commutes: (f |Y ) Y /Y i i /S f S p p B /B g The conclusion follows by completing the diagram s dashed arrows with the composition p i as the subsystem projection for g as a subsystem of f |Y . A related statement to Lemma 15 could consider the conditions under which a subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections defined by the dashed arrows in the diagram below could be constructed: (f |Y ) Y /Y i i f S B g /S /B Therefore, when studying a dynamical system, one will often encounter problems of the following form. Question 1. When do lifts to the dashed arrows in the diagram above exist? Answers to this question relate closely to the expected behavior of systems when they are rewritten with new variables. This routinely happens", "the dashed arrows in the diagram above exist? Answers to this question relate closely to the expected behavior of systems when they are rewritten with new variables. This routinely happens with compiled software, as the next example shows. Example 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a dynamical system f : X X. For this example, f is not bijective. The way that the computer is used is that the user loads an executable and then runs it. The initial state of the executable is a point within a subset U X. The user does not have control over the entire state of the machine, 42 but rather can constrain it to a smaller portion of the state space. It makes sense to require that U is an invariant set, which means that not only the initial state is included, but all possible future states as well. Therefore, the execution of the executable is completely determined by the commutative diagram U f |U X /U f /X As an example in PDP-11 assembly, we could have U = {PC {0, 1}, memory =", "is completely determined by the commutative diagram U f |U X /U f /X As an example in PDP-11 assembly, we could have U = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}}, where all values of the unspecified parts of the machine state (other registers, the rest the memory) are included in U . If the program counter PC is initialized to 0, the program will execute the instructions at 0 and 1, and then will halt. Evidently, if PC = 1, then the program halts immediately. No modifications to memory can occur given an initialization with U , and PC cannot be moved outside of those two instructions. This ensures that f (U ) U is indeed an invariant set. We might instead imagine that the executable specified by U was the result of a compiled, high-level program. Such a program would necessarily be of the form g : Y Y , where Y holds the values of the two registers R1 and R2. For a PDP-11, this means Y = ({0, 1}16 )2 , and g(x, y) := (x, x + y), which is to say that R1 is unchanged by the program,", "For a PDP-11, this means Y = ({0, 1}16 )2 , and g(x, y) := (x, x + y), which is to say that R1 is unchanged by the program, and R2 takes the sum of R1 and R2. The compilation process essentially ensures that we have the following commutative diagram U f |U q Y g /U /Y q where the q maps select the two registers R1 and R2 from the entirety of the machine state. Notice that we may write q = p , where is the inclusion of U , X, and p still selects the two registers R1 and R2 from the entirety of the machine state. Since the machine state is very large in comparison to U , the following diagram does not commute: f /U X p Y g 43 /Y p Values of X for which the commutativity fails egregiously are instances of weird machine states [13]. However, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can formalize this idea by way of an initialization function i : Y U that is a right inverse to q, namely q i", "to avoid weird machine states. We can formalize this idea by way of an initialization function i : Y U that is a right inverse to q, namely q i = (p ) i = id Y . This means that we have the following commutative diagrams UO f |U i Y g /U /Y f XO q i Y g /X /Y p For instance, in the example PDP-11 program, we could use i(x, y) := {PC = 0, R1 = x, R2 = y, R[3-6] = 0, memory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}}, Notice that since i does not have the ability to change the program counter PC, the following diagram does not commute UO f |U /U O i Y i g /Y Inspired by Example 9, suppose that we have a commutative diagram XO f i Y g /X /Y p where i is injective, p is surjective, and f , g are bijective. This leads to another question that is often of interest when studying system behaviors. Question 2. Under what conditions does X f p Y g 44 /X /Y p commute? Clearly if g is bijective, then", "is often of interest when studying system behaviors. Question 2. Under what conditions does X f p Y g 44 /X /Y p commute? Clearly if g is bijective, then a sufficient condition is that p = g 1 p f . It is probably the case that p i = id Y in most applications, but it is unlikely to be the case that i p = id X . Lemma 16. The subsystem preorder is a meet-semilattice. That is, if we have two subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S, there is a common subsystem f3 : S3 S3 of both of them (which might be trivial) that satisfies the following universal property. If f4 : S4 S4 is another common subsystem of f1 and f2 , then f4 is a subsystem of f3 . Proof. We start with two subsystems of a common dynamical system f : S S, so that we have a commutative diagram SO 1 f1 / S1 O p1 p1 S /S f p2 S2 p2 f2 / S2 We want to construct a subsystem of all three of these f3 :", "SO 1 f1 / S1 O p1 p1 S /S f p2 S2 p2 f2 / S2 We want to construct a subsystem of all three of these f3 : S3 S3 , that is as large as possible. Realize that what is needed to satisfy the universal property is a definition for the dashed arrows in S p1 p 3 p2 S2 / S1 p 3 / S3 such that this diagram is a colimit. Since each of the Si are sets, there is a standard colimit construction, namely S3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with p1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply this construction twice, there is a unique f3 completing the diagram below S p1 p 3 p2 S2 / S1 p 3 f1 / S3 S1 f3 f2 S2 45 p 3 p 3 / S3 Proposition 17. Restrict attention to f : S S being a (not necessarily linear) bijection on a vector space S, and require that the subsystem projection p : S B for each subsystem (g, p)", "f : S S being a (not necessarily linear) bijection on a vector space S, and require that the subsystem projection p : S B for each subsystem (g, p) of f is a linear surjection. In this case, the relation is a subsystem of is also antisymmetric up to conjugacy by linear isomorphisms. As a result, data feedback loops are confined to happen within a given subsystem. Proof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 ) is a subsystem of g2 : B2 B2 , so that we have the commutative diagram B1 g1 p2 B2 p2 g2 / B2 g1 / B1 p1 B1 / B1 p1 Since p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1 is a linear surjection. Since it also evidently preserves dimension, it must be a linear isomorphism. Because both p1 and p2 are surjective, this implies that both must also be injective. Hence both p1 and p2 must also be linear isomorphisms, 1 which establishes that g2 = p2 g1 p 1 2 and g1 = p1 g2 p1 as claimed. Example", "Hence both p1 and p2 must also be linear isomorphisms, 1 which establishes that g2 = p2 g1 p 1 2 and g1 = p1 g2 p1 as claimed. Example 10. There is no function h that will make the diagram below commute Z2 id ZO 2 f / Z2 id h / Z2 O id id Z2 g / Z2 where f (x, y) = (x, 1 x), and g(x, y) = (y, y). 46 There is also no function h that will make the diagram below commute Z2 pr1 ZO f /Z id h /Z O pr2 Z2 id g /Z where f (x, y) = 1 x, and g(x, y) = y. Suppose that f : S S is a dynamical system in which S is a vector space and the subsystem projections are all linear surjections, as required by Proposition 17. Let (B, ) be the collection of all subsystems of f , with the partial order established by Lemma 13 and Proposition 17. Each element of B is a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity, if g1 is a subsystem of", "of B is a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity, if g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such that p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ). Definition 21. Define the sheaf Ff of subsystems of f according to the following recipe: Stalks Ff ((gB , pB )) := B, and Restrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 . Even if the subsystem projections are not linear surjections, the Alexandrov topology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the stalks of Ff are not necessarily well defined, since there is no guarantee that the subsystems of a given cycle have the same state spaces. Lemma 18. For a dynamical system f : S S, the space of global sections of Ff is precisely S. Proof. First of all, notice that id S : S S meets the criteria for a subsystem. We merely need to verify that the", "sections of Ff is precisely S. Proof. First of all, notice that id S : S S meets the criteria for a subsystem. We merely need to verify that the definition of global sections for Ff doesn t conflict with this. The space of assignments for Ff is M M Ff (p) = B. p:S B subsystem p:S B subsystem Suppose that we have a global section s. On the other hand, if (gB , pB ) (f, id S ), then (Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B). 47 Therefore, the value of s on the subsystem id S : S S determines the values of s on every other subsystem. Proposition 19. A dynamical system f : S S induces an endomorphism on the sheaf of all subsystems, and for which the induced map on global sections is f. Proof. This follows immediately from the definition, as soon as we notice that for a subsystem p : S B, the g map guaranteed by the definition is the corresponding component map for the sheaf morphism. In short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on", "g map guaranteed by the definition is the corresponding component map for the sheaf morphism. In short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps fx : S(x) S(x). One may also consider the action of different semigroups on stalks to model continuous dynamical systems. We are now ready to establish the main result of this section, which relates the sheaf of subsystems of a DSEM to its graph representation. As we have seen in Example 5, feedback loops in the DSEM graph must be confined to being entirely within a subsystem. Because we can collapse all feedback loops in an arbitrary directed graph to obtain an acyclic graph, we will assume that the DSEM graph is acyclic without loss of generality. The key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact its value. Any variable with a directed path leading to our variable of interest will therefore need to be included in the subsystem. Definition 22. In a directed graph G = (V, E)", "variable with a directed path leading to our variable of interest will therefore need to be included in the subsystem. Definition 22. In a directed graph G = (V, E) an in-closed subset I V is a set of vertices such that if v I, then if e = (w, v) E, then w I. Lemma 20. If a dynamical system is defined by a DSEM, every in-closed subset of variables is a subsystem. Proof. Suppose that I is a in-closed subset of variables in a DSEM on a directed graph G. If v I then all of the dependencies of v are also in I, so the next timestep of v can be predicted from the variables in I. Therefore, projecting out just the variables in I from the set of all variables will result in a new dynamical update map when restricted to I. As a consequence of Lemma 20, we have the following result that explains why modeling with DSEM is a good idea. Corollary 21. If a dynamical system is defined by a DSEM on a partially ordered set, then the Alexandrov topology of the dual order is a subspace of the base space topology of", "a dynamical system is defined by a DSEM on a partially ordered set, then the Alexandrov topology of the dual order is a subspace of the base space topology of its subsystem sheaf. Corollary 21 does not establish that the Alexandrov topology of the dual order of the DSEM is the subsystem sheaf. This is because if the original variables in the DSEM are chosen coarsely, there may be additional subsystems that are hidden within them. These hidden subsystems will be present in the subsystem sheaf, but will not correspond to distinct in-closed subsets of the DSEM graph. 48 f k pr1 k pr1 k k pr1 pr1 ( k ) ( k ) pr1 pr1 g ( k ) ( k ) pr1,2,5,6 pr1,2,3,4 pr1 ( k ) pr1 pr1 ( k ) pr1 ( k ) ( k ) pr1,2,5,6 k pr1,2,3,4 pr7 k k k pr7 k k Figure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are the subsystem projection maps; dashed arrows are the dynamical system state update maps. Maps f and g are explained in the text.", "6 Subsystems of the Bering Sea system Figure 14 shows the sheaf of subsystems for the Bering Sea example, with the stalks organized in the same way as shown in Figure 13. The function f performs an AR (k) update: ! k 1 X ai xk i , f (x1 , . . . , xk ) = x2 , . . . , xk , i=0 while the function g performs the dynamical update for the subsystem containing the Krill variables: ! k 1 X g(x1 , . . . , xk , y, z) = x2 , . . . , xk , ai xk i , y + cxk , z + dy . i=0 Notice how f is obtained from g by projecting out the first k components, in accordance with the commutativity of Figure 14. Although Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are different but are in a natural bijective correspondence. The three sheaves offer three distinct perspectives, with increasing granularity, Definition 21: Figure 14 Stalks are nested collections of dynamically related", "are different but are in a natural bijective correspondence. The three sheaves offer three distinct perspectives, with increasing granularity, Definition 21: Figure 14 Stalks are nested collections of dynamically related variables, each represented by sliding windows of timeseries, 49 Definition 13: Figure 1(d) Each variable is an entire timeseries and appears alone in at least one stalk, and Definition 14: Figure 13 Each observation (a timestep for a single variable) appears alone in at least one stalk. With this perspective, the boundaries between subsystems are easily seen in Figure 13: those restriction maps that are identity maps from parts to nets are those that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections involved. Moreover, the state spaces arise as one time step of the space of local sections over each subsystem, once cut.", "7 Conclusion In this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological model of the Bering Sea and a dynamical model of low-level computer software. Sheaf modeling provides a coherent mathematical framework for studying the complicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that a sheaf represents a hypothesis about how variables will interact, a non-global assignment represents the observations collected on the variables in its support, minimizing consistency radius predicts values of the variables that were not observed, and the minimal consistency radius is a measure of the consistency between the observations and the hypothesis. This chapter shows that when a dynamical system is described by a DSEM, there are three sheaves that provide increasingly granular data about the interactions between variables: 1. the sheaf of subsystems (Definition 21), 2. the netlist sheaf with timeseries as stalks (Definition 13), and 3. the netlist sheaf with additional stalks for individual observations (Definition 14). With these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data.", "stalks for individual observations (Definition 14). With these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf encodings allow the modeler to perform a variety of standard inferences (e.g. forward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports 50 hybrid versions, for instance performing simultaneous forward and backward predictions, or simultaneously performing regression and prediction. Since the sheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks. It remains future work to compare estimates of uncertainty computed by the DSEM (appearing in the V and E matrices) to the consistency radius of the corresponding sheaf. In particular, it seems possible to view consistency radius as a test statistic for the distributional model posited by the DSEM. Indeed, Equation (10) is strikingly close to the log likelihood if the distributions of measurement errors are assumed to follow an exponential model. If this is true, then it should be possible to lift the sheaf modeling discipline described here into a standard statistical hypothesis testing framework. Acknowledgments The linear", "an exponential model. If this is true, then it should be possible to lift the sheaf modeling discipline described here into a standard statistical hypothesis testing framework. Acknowledgments The linear regression example in Section 3.3 is due to Donna Dietz. This article is based upon work supported by the Office of Naval Research (ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under contract HR001119C0072, and the MITRE Corporation s Independent Research and Development (IR&D) Program. Any opinions, findings and conclusions or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of ONR, DARPA, or MITRE."], "num_sections": 8, "num_graph_nodes": 124, "num_graph_edges": 230}
{"method": "fixed", "num_chunks": 131, "avg_chunk_len": 795.1068702290077, "std_chunk_len": 50.44386978217948, "max_chunk_len": 800, "min_chunk_len": 220, "total_chars": 104159, "compression_ratio": 1.0005952438099444, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025\n\nAnalyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025\n\nAbstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can", "be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.\n\nContents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n3\n4\n5\n\n2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea\n\n5\n5\n7\n\n Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for ide", "ntification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.\n\n1\n\n3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .\n\n8\n11\n14\n18\n25\n\n4 Sheaf encoding of the Bering Sea\n\n28\n\n5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35", "5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system\n\n49\n\n7 Conclusion\n\n50\n\n1\n\nIntroduction\n\nEcologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking,", "a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sh", "eaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2\n\ninvolves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system para", "meters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.\n\n1.1\n\nRelated work\n\nThe challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time", "series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Ev", "en though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint", ", unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software\n\n3\n\n[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between inv", "ariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.\n\n1.2\n\nContributions\n\nThis chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studyi", "ng the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Def", "inition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\n\n4\n\n1.3\n\nChapter outline\n\nSection 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs", "sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof i", "nvariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.\n\n2\n\nDynamical modeling of ecosystems\n\nThis section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then", "presents a representative\necosystem model that will be revisited several times in the chapter.\n\n2.1\n\nDSEM background and motivation\n\nDefinition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5\n\nLinearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be i", "nterpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1\n\n(1)\n\nIn what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a varia", "ble xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1\n\n(2)\n\nWe can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h\n\nJ X\nT\nX\ni=1 j=1\n\nγk, ,i,j xi (τ tj ) + hϵk, (τ ).\n\n(3)\n\nIf we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equati", "on (3) can be compactly written in matrix form as\nX PX + E,\n\n(4)\n\nwhere the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6\n\nthe xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT\n\nQ = (id P )V\n\n 1\n\n(5)\n(id P),\n\n(6)\n\nwhere id is the identity matrix. The path coefficient matrix P can", "be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nt", "he distribution of yk,j through a link function gj , via\n\nyk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,\n\n(7)\n\nwhere is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.\n\n2.2\n\nEcological background and the DSEM system for the\nBering Sea\n\nTo demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-wint", "er survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7\n\nby specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt\n\n(8)\n\nα\n\nwhere e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of", "fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:\n\nRt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabun", "dance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.\n\n3\n\nSheaf encodings of composite systems\n\nIn this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist,", "and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8\n\nTable 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries", "of\nthe variables are taken from [47].\nName\nSeaIce\n\nDescription\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]\n\nColdPool\n\nSpatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]\n\nSpawners\n\nFemale spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]\n\nSurvival\n\nAge-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessmen", "t model [20]\ndivided by Spawners\n\nCopepods\n\nDensity of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)\n\nKrill\n\nIndex of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]\n\nDietCopepods\n\nBiomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for sub", "sampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).\n\nDietKrill\n\nSame as DietCopepods, but for euphausiids (krill)\n9\n\nTable 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool\n\nEcological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )\n\nSign\n+\n\nColdP", "ool Copepods\n\nWarmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]\n\n+\n\nColdP ool Krill\n\nWater temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )\n\n?\n\nCopepods DietCopepods\n\nIncreased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator\n\n+\n\nKrill DietKrill\n\nSame as Copepods DietCopepods\nbut for krill\n\n+\n\nDietCopepods Survival\n\nIncreased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survi", "val ) [19]\n\n+\n\nDietKrill Survival\n\nSame as DietCopepods Survival,\nbut for krill\n\n+\n\nSpawners Survival\n\nIncreased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]\n\n10\n\nbiomass\ndensitysurvival\n\nSeaIce\n\nout\n\nColdPool\n\nf\n n\n\nColdPool\n\nin\n\nCopepods_block\nout\n\nKrill_block\nout\n\nCopepods\n\nKrill\n\nKrill\n\nin\n\nCopepods_block\n\nKrill_block\n\nin\n\nDietCope_block\nDiet_Cop\n\nDiet_Krill\n\nSpawners\n\nout\n\nDiet_Cop\nSurvival\n\nin_copepods\n\nout\n\nSpawners\n\nout\n\nDiet_Cop\nin_copepods\n\nin_spawners\n\nDiet_Krill\n\nSpawners\n\nin_krill\n\nSurvival_block\n\nin_spawners\n\ng2\n\ng1\n\n n\n\nid\n n\nh\n\nid\n n\nk\n n\n\n n\npr1\n\npr2\nn\n\nn\n\nSurvival\n\nm\n n\n\n(b)\n\n(c)\n\n(d)\n\nout\n\n n\n\n n\n\nSurvival\n\nout\n\n(a)\n\nin\n\nDietKrill_block\n\nDietCope_block\n\nout\n\nDiet_Krill\nin_krill\n\nSurvival_block\n\nKrill\n\nin\n\nin\n\nDietKrill_block\n\n n\n\nout\n\nCopepods", "id\n\nid\n\nin\nin\n\nout\n\nCopepods\n\n n\n\nColdPool_block\n\nout\n\nColdPool\n\nid\n\nin\n\nColdPool_block\nSeaIce\n\n n\n\nSeaIce\n\nin\n\nn\n\n n\npr3\n\nFigure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a", ") shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.\n\n3.1\n\nNetlists\n\nThe term netlist appears to have entered the technical lexicon in the early\ndays of", "computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made.", "Each\nport is either an output, which means that it determines the value of the variable\n11\n\nPart 2 (capacitor)\nNet 1\n\nin\n\nout\n\nNet 2\nin\n\nout\n\nPart 1\n(Battery)\n\nPart 3 (resistor)\nin\n\nout\n\nNet 3\n\nFigure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output", "variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets sp", "ecify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12\n\n each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the sam", "e name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. Th", "e partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergrap", "h for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was const", "ructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13\n\nsomewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply th", "e name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist", "graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).\n\n3.2\n\nSheaves and cosheaves\n\nSheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally", "induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborhoods. \n14\n\nDefinition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological", "space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P,", ") is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U", ") is the identity function.\n15\n\nDually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to th", "e points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U\n\nIf U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S o", "n a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,\n\nG\nC(X) = \nC(U ) / ,\nU open\n\nwhere is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16\n\nIntuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional", "gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the", "stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p\n\ncS (a) := \n\nX\n\nX\n\nU T , V T :V U\n\np\n\n(dV (a(V ), S(V U )a(U ))) \n\n,\n\n(10)\n\nwhere p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV\n\nn\nX\nk=1\n\n!1/p\np\n\n|xk yk |\n\n,\n\nwhere αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if d", "ifferent\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.\n\n17\n\nConsistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an as", "signment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T\n\nWe will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T\n\nAs the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.\n\n3.3\n\nThe netlist sheaf\n\nThe key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (T", "heorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalk", "s represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18\n\nConstraints\n\nx\n\nx\n\nm b x\n\nx\n\nm b\n\npr1\n\nx\n\n n\n\npr2\n\npr3\n\ny = mx + b\n\ny = mx + b\n\ny\n\ny\n\ny\n\ny\n\ny\n\n n\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nn\n\nf\n\nAssignment support\n\nFigure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the", "remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n", "points in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) a", "nd the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19\n\nthe stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the", "prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).\n\n(11)\n\nThe function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form\n\na := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =\n\np\n\np\n\n|m\ne m| + |eb", "b| +\n\nn\nX\nk=1\n\np\n\n|f\nxk xk | +\n\nn\nX\nk=1\n\n!1/p\np\n\n|b + mf\nx k yk |\n\n(13)\n\nfor a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shade", "d\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20\n\n(14)\n\nwhere the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )\n\n|m\ne m|2 + |eb b|2 +\n\nn\nX\nk=1\n\n|f\nxk xk |2 +\n\nn\nX\nk=1\n\n!1/2\n|b + mf\nx k y k", "|2\n\nBut since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1\n\nk=1\n\nThis permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimiza", "tion problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1\n\nConsistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21\n\n.\n\ny\n\ny1\n\ny = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius\n\nb + mx1\nconstrained\nconsistency\nradius\n\nx\nx1\n\nx~1\n\nFigure 4: Geometric meanings of the terms contribut", "ing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,\n\n(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for c", "ompleteness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22\n\nConstraints\n\npr1\n\n...\n\npr3\n\npr2\n\nprn+2\n\nf1 f2\n\nn\n\nAssignment support\n\nfn\n\n... \n\nFigure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf con", "struction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by\n\na := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by ex", "actly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible", ",\nthe resulting sheaf will or will not have global sections (Definition 9).\n23\n\nX\n\nx\n\nx\nout\n\nf\n\ng\n\ng\n\nid\n\nX\n\nX\n\nid\n\nf\n\nin\n\ng\n\nf\n\nin\n\nout\n\ny\n\ny\n\nX\n\n(a)\n\n(b)\n\n(c)\n\nFigure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model auto", "regressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The", "following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries.\n24\n\nx1, ... xn\nS\nin\n\na1, ... ak\n\ncoef\n\nLCF(k)\n\npr2\nk\n\n S\n\nout\n\nyn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n(a)\n\npr1\n\n k\n\nS\n(b)\n\nFigure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsol", "utions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.\n\n3.4\n\nSheaves modeling autoregressive timeseries\n\nAutoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1", ", . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure", "7(a). Using Definition 13, we obtain the\n25\n\n... x1, ... xn\n\ns\n\nout\n\nin\n\nidentity\n\nLCF(k)\n\ncoef\n\na1, ... ak\n\nout\n\nin\n\nid\n\npr2\n\ns\n\n k s\n\nid\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\ns\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms", "are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.", "When the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.\n\n26\n\nx1, ... xn\n n\nin\n\nin\n\ncrop\n\nLCF(k)\n\na1, ... ak\n\ncoef\n\nout", "out\n\nid\n\npr2\n\n n\n\n k n\n\nprk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n n-k\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\n\n n\n\nin\n\n k n\n\nDietCope_lag\n\nCopepods\nin\n\npr2\n\nout\n\nid\n\ncrop\n\n n\nh\n\nDietCope_block\nout\n\nLCF(k)\nprk..n\n\n n - k\n\n n\n\nDiet_Cop\n(a)\n\n(b)\n\nFigure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.\n\n27\n\nAutoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specificall", "y, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.\n\n4\n\nSheaf encoding of the Bering Sea\n\nWe now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to Figure 1. The reader is directed to [36", "] for the software that\ngenerates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the res", "ulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\ns", "olution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is\n\n28\n\nSeaIce\n\nColdPool\nln (ColdPool ) [ln(km2 )]\n\nln (SeaIce) [ln(km2 )]\n\n1\n0.5\n0.0\n 0.5\n 1.0\n\n0\n 1\n 2\n\n2018 ColdPool\n\n 3\n\nDietCopepods\nln (DietCopepods) [ ]\n\n3\n\nln (Copepods) [ln(count/m )]\n\nCopepods\n2.5\n0.0\n 2.5\n 5.0\n\n1\n0\n 1\n 2\n\nDietKrill\n\n3\n\nln (Krill ) [ln(count/m )]\n\nKrill\nln (DietKrill ) [ ]\n\n0", ".5\n0.0\n 0.5\n\n2016 Krill\n\n 1.0\n\n0.5\n0.0\n 0.5\n 1.0\n\nSpawners\n\nSurvival\nln (Survival ) [103 count/kg]\n\nSpawners[106 kg]\n\n4\n3\n2\n1\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nmeasurement\n\n2\n1\n0\n 1\n 2\n1960\n\nDSEM\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nsheaf\n\nFigure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.\n\n29\n\nCopepods_pc\n\nCopepods\n\n n\npr2\n\npc\n\nin\n\nDietCope_block\n\npr1\n\nout\n\nDiet_Cop\n\n n\ng~1\n n\n\n(a)\n\n(b)\n\nFigure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the", "DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that", "correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency", "is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30\n\nSeaIce\nSeaIce_lag\nSeaIce_lagvar\n\nColdPool_block\n\nColdPool_lagvar\n\nSeaIce_pc\n\nColdPool_lag\nColdPool\n\n2018_ColdPool\n\nColdPool_Copepods_pc\n\nColdPool_Krill_pc\nKrill_block\n\nCopepods_block\n\n2016_Krill\nCopepods\n\nK", "rill\nCopepods_lag\n\nCopepods_pc\nDietCopepods_block\n\nCopepods_lagvar\n\nKrill_lag\n\nKrill_pc\nDietKrill_block\n\nKrill_lagvar\n\nDietCopepods\n\nSpawners\n\nDietKrill\nDietCopepods_pc DietKrill_pc\n\nDietCopepods_lag\n\nDietKrill_lag\nSpawners_pc\n\nDietCopepods_lagvar\n\nSpawners_lag\n\nDietKrill_lagvar\nSpawners_lagvar\n\nSurvival_block\n\nSurvival\n\ncells\n\nrestrictions\nprojection map\nother function (see text)\n\ninferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present\n\n0\n2\n4\nconsistency radius contribution\n\nFigure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that a", "re also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31\n\nSource\n\nTarget\n\nSeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)\n\nDSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2\n\nnone\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848\n\nSheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03", "2637\n2679\n\nAR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907\n\nTable 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by", "an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregre", "ssive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table\n\n32\n\nVariable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods", "DietKrill\n\nAR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060\n\nAR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445\n\nTable 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were appli", "ed. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linea", "r, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.\n\n5\n\nThe topology of subsystems\n\nClassically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n3", "3\n\nbase space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as", "a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)\n\n5.1\n\nDynamical systems\n\nDefinition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f :", "S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU\n\nf\n\ni\n\nV\n\nf\n\n/U\n\n/V\n\ni \n\ncommutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34\n\nProof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative dia", "gram of the\nform\nf1\n/ S1\nS1\ng\n\ng\n\nS2\n\nf2\n\n/ S2\n\nComposition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.\n\n5.2\n\nThe cosheaf endomorphism of invariant sets\n\nThe state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance", "of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V\n\nW", "V\n\nW V\n\nsince the equivalence identifies points that agree on overlaps.\n35\n\nThe above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))\n\nmV\n\n/ S(V )\nS(U V )\n\nR(f 1 (U )) mU / S(U )\n\nDually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphi", "sm m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO\n\nmV\n\n/ S(V )\nO\n\nR(f 1 (U ) f 1 (V ))\n\nS(U V )\n\nR(f 1 (U ))\n\nmU\n\n/ S(U )\n\nWith the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U", ") f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )\n\nFU :=f |U\n\nC(X,TX ) (f 1 (U ) f 1 (V ))\n\nC(X,TX ) (f 1 (V )) = f 1 (V )\n\n/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )\n\nFV :=f |V\n\n/ C(Y,T ) (V ) = V\nY\n\nwhich establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36\n\nNow suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subs", "ets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU\n\nmU =f\n\n/U\n\ni\n\nV\n\nmV =f\n\n/V\n\ni\n\ncommutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.\n\n5.3\n\nSubsystem decomposition sheaf\n\nRather than carving up the state space into", "different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni\n\nU\n\np\n\ni\n\nf |U\n\nB\n\n/U\n\np\n\ng\n\n/B\n\nDually, the diagram at right above captures the situation where B is a subsystem\nof f .\n\n37\n\nDefinition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g :", "B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explici", "tly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subs", "ystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n\n= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )\n\n= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38\n\nExample 3.\n?B\nA\n\nC\n\nFollowing the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B,", "and C given by\nthe graph\nA\n\n?C\n\nB\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)\n\nAh\n\nB\n\n(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next", "example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv\n\n39\n\nfor some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomp", "osition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))\n\n= prW (f (u) + f (w))\n\n= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)\n\n= (f |W ) (prW (u + w))\n= (f |W prW )(v).\n\nLemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf\n\nS\np2\n\np1 p2\n\nB2\n\np2\ng2\n\np1\n\nB1\n\n/S\n\n/ B2\n\np1 p2\n\np1\n\ng1\n\n/ B1\n\nso that (g1 , (p1 p2", ")) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).\n\n40\n\nThis has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1\n\nZ\n\nf\n\n/ Z3\npr1\n\ng\n\n/Z\n\nIn this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R", "2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2\n\nf\n\np\n\nB\n\n/ R2\np\n\nid\n\n/B\n\nwhere p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS\n\nf\n\np\n\nB\n\n/S\np\n\ng\n\n/B\n\nSuppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means", "that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .\n\n41\n\nLemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\n\n/S\n\nf\n\nS\np\n\np\n\nB\n\n/B\n\ng\n\nThe conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this c", "onsists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\nf\n\nS\n\nB\n\ng\n\n/S\n\n/B\n\nTherefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user", "loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42\n\nbut rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU\n\nf |U\n\nX\n\n/U\n\nf\n\n/X\n\nAs an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program", "will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU\n\nf |U\n\nq\n\nY\n\ng\n\n/U", "/Y\n\nq\n\nwhere the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np\n\nY\n\ng\n\n43\n\n/Y\n\np\n\nValues of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutat", "ive diagrams\nUO\n\nf |U\n\ni\n\nY\n\ng\n\n/U\n\n/Y\n\nf\n\nXO\nq\n\n i\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nFor instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,\n\nR1 = x,\nR2 = y,\n\nR[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO\n\nf |U\n\n/U\nO\n\ni\n\nY\n\ni\ng\n\n/Y\n\nInspired by Example 9, suppose that we have a commutative diagram\nXO\n\nf\n\ni\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nwhere i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX\n\nf\n\np\n\nY\n\ng\n\n44\n\n/X\n\n/Y\n\np\n\ncommute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case", "that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1\n\nf1\n\n/ S1\nO\n\np1\n\np1\n\nS\n\n/S\n\nf\n\np2\n\nS2\n\np2\n\nf2\n\n/ S2\n\nWe want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition", "for the dashed arrows in\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\n/ S3\n\nsuch that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\nf1\n\n/ S3\n\nS1\nf3\n\nf2\n\nS2\n\n45\n\np \n3\n\np 3\n\n/ S3\n\nProposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.", "As a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1\n\ng1\n\np2\n\nB2\n\np2\n\ng2\n\n/ B2\n\ng1\n\n/ B1\n\np1\n\nB1\n\n/ B1\n\np1\n\nSince p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid\n\nZO 2\n\nf\n\n/ Z2\nid\n\nh\n\n/ Z2\nO\n\ni", "d\n\nid\n\nZ2\n\ng\n\n/ Z2\n\nwhere\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).\n\n46\n\nThere is also no function h that will make the diagram below commute\nZ2\npr1\n\nZO\n\nf\n\n/Z\nid\n\nh\n\n/Z\nO\n\npr2\n\nZ2\n\nid\ng\n\n/Z\n\nwhere\nf (x, y) = 1 x,\n\nand\n\ng(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f acco", "rding to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf", "(p) =\nB.\np:S B subsystem\n\np:S B subsystem\n\nSuppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47\n\nTherefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S v", "ia self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of inte", "rest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.\n\nLemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynam", "ical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48\n\nf\n\n k\npr1\n\n k\npr1\n\n k \n\n k \n\npr1\n\npr1\n\n( k ) \n\n( k ) \npr1\n\npr1\n\ng\n\n( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4\n\npr1\n( k ) \npr1\n\npr1\n( k ) \npr1\n\n( k ) \n( k ) \npr1,2,5,6\n\n k\n\npr1,2,3,4\n\npr7\n\n k k \n\n k\n\npr7\n\n k k \n\nFigure 14: Sheaf of subsystems", "for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.\n\n6\n\nSubsystems of the Bering Sea system\n\nFigure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0\n\nwhile the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0\n\nNotice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with mod", "ifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49\n\nDefinition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those res", "triction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.\n\n7\n\nConclusion\n\nIn this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling a", "re that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system model", "er can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50\n\nhybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding she", "af. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.\n\nAcknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Indepen", "dent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "sliding", "num_chunks": 260, "avg_chunk_len": 798.8692307692307, "std_chunk_len": 11.13061506458544, "max_chunk_len": 800, "min_chunk_len": 620, "total_chars": 207706, "compression_ratio": 0.5017717350485783, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025\n\nAnalyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025\n\nAbstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can", "ing\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.\n\nContents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . .", "be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.\n\nContents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n3\n4\n5\n\n2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea\n\n5\n5\n7\n\n Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for ide", ". . . . . . . . . . . . .\n\n2\n3\n4\n5\n\n2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea\n\n5\n5\n7\n\n Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.\n\n1\n\n3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . .", "ntification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.\n\n1\n\n3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .\n\n8\n11\n14\n18\n25\n\n4 Sheaf encoding of the Bering Sea\n\n28\n\n5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35", ". . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .\n\n8\n11\n14\n18\n25\n\n4 Sheaf encoding of the Bering Sea\n\n28\n\n5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35\n5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system\n\n49\n\n7 Conclusion\n\n50\n\n1\n\nIntroduction\n\nEcologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the", "5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system\n\n49\n\n7 Conclusion\n\n50\n\n1\n\nIntroduction\n\nEcologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking,", "results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore", "a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sh", "dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and", "eaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2\n\ninvolves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system para", "sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2\n\ninvolves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosys", "meters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.\n\n1.1\n\nRelated work\n\nThe challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time", "tem model in Section 6.\n\n1.1\n\nRelated work\n\nThe challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsy", "series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Ev", "stems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example,", "en though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint", "overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software\n\n3\n\n[29, 1]. Moreover, Abadi an", ", unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software\n\n3\n\n[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between inv", "d Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKear", "ariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.\n\n1.2\n\nContributions\n\nThis chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studyi", "ney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.\n\n1.2\n\nContributions\n\nThis chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of", "ng the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Def", "the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\n\n4\n\n1.3\n\nChapter outline\n\nSection 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational d", "inition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\n\n4\n\n1.3\n\nChapter outline\n\nSection 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs", "ata [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological", "sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof i", "model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis", "nvariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.\n\n2\n\nDynamical modeling of ecosystems\n\nThis section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then", "of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.\n\n2\n\nDynamical modeling of ecosystems\n\nThis section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter.\n\n2.1\n\nDSEM background and motivation\n\nDefinition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presenc", "presents a representative\necosystem model that will be revisited several times in the chapter.\n\n2.1\n\nDSEM background and motivation\n\nDefinition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5\n\nLinearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be i", "e of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5\n\nLinearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs t", "nterpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1\n\n(1)\n\nIn what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a varia", "he values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1\n\n(1)\n\nIn what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1\n\n(2)\n\nWe can approxi", "ble xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1\n\n(2)\n\nWe can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h\n\nJ X\nT\nX\ni=1 j=1\n\nγk, ,i,j xi (τ tj ) + hϵk, (τ ).\n\n(3)\n\nIf we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equati", "mate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h\n\nJ X\nT\nX\ni=1 j=1\n\nγk, ,i,j xi (τ tj ) + hϵk, (τ ).\n\n(3)\n\nIf we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,\n\n(4)\n\nwhere the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6\n\nthe xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coeffici", "on (3) can be compactly written in matrix form as\nX PX + E,\n\n(4)\n\nwhere the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6\n\nthe xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT\n\nQ = (id P )V\n\n 1\n\n(5)\n(id P),\n\n(6)\n\nwhere id is the identity matrix. The path coefficient matrix P can", "ent matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT\n\nQ = (id P )V\n\n 1\n\n(5)\n(id P),\n\n(6)\n\nwhere id is the identity matrix. The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement", "be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nt", "errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via\n\nyk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in", "he distribution of yk,j through a link function gj , via\n\nyk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,\n\n(7)\n\nwhere is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.\n\n2.2\n\nEcological background and the DSEM system for the\nBering Sea\n\nTo demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-wint", "hand, V takes the form\nV = id T T GGT ,\n\n(7)\n\nwhere is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.\n\n2.2\n\nEcological background and the DSEM system for the\nBering Sea\n\nTo demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7\n\nby specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt\n\n(8)\n\nα\n\nwhere e is the maximum expected recruits per spawning biomass, β is the expected density-depe", "er survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7\n\nby specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt\n\n(8)\n\nα\n\nwhere e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of", "ndent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:\n\nRt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the m", "fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:\n\nRt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabun", "echanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be", "dance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.\n\n3\n\nSheaf encodings of composite systems\n\nIn this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist,", "interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.\n\n3\n\nSheaf encodings of composite systems\n\nIn this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto t", "and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8\n\nTable 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries", "he DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8\n\nTable 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].\nName\nSeaIce\n\nDescription\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]\n\nColdPool\n\nSpatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and c", "of\nthe variables are taken from [47].\nName\nSeaIce\n\nDescription\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]\n\nColdPool\n\nSpatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]\n\nSpawners\n\nFemale spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]\n\nSurvival\n\nAge-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessmen", "ompiled in Rpackage coldpool [37]\n\nSpawners\n\nFemale spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]\n\nSurvival\n\nAge-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners\n\nCopepods\n\nDensity of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)\n\nKrill\n\nIndex of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsur", "t model [20]\ndivided by Spawners\n\nCopepods\n\nDensity of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)\n\nKrill\n\nIndex of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]\n\nDietCopepods\n\nBiomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for sub", "vey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]\n\nDietCopepods\n\nBiomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).\n\nDietKrill\n\nSame as DietCopepods, but for euphausiids (krill)\n9\n\nTable 2: List of path coefficients connecting variables (defined in Table 1),\nsu", "sampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).\n\nDietKrill\n\nSame as DietCopepods, but for euphausiids (krill)\n9\n\nTable 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool\n\nEcological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )\n\nSign\n+\n\nColdP", "pporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool\n\nEcological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )\n\nSign\n+\n\nColdP ool Copepods\n\nWarmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]\n\n+\n\nColdP ool Krill\n\nWater temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )\n\n?\n\nCopepods DietCopepods\n\nIncreased copepod abundance will res", "ool Copepods\n\nWarmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]\n\n+\n\nColdP ool Krill\n\nWater temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )\n\n?\n\nCopepods DietCopepods\n\nIncreased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator\n\n+\n\nKrill DietKrill\n\nSame as Copepods DietCopepods\nbut for krill\n\n+\n\nDietCopepods Survival\n\nIncreased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survi", "ult in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator\n\n+\n\nKrill DietKrill\n\nSame as Copepods DietCopepods\nbut for krill\n\n+\n\nDietCopepods Survival\n\nIncreased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]\n\n+\n\nDietKrill Survival\n\nSame as DietCopepods Survival,\nbut for krill\n\n+\n\nSpawners Survival\n\nIncreased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]\n\n10\n\nbiomass\ndensitysurvival\n\nSeaIce\n\nout\n\nColdPool\n\nf\n n\n\nColdPool\n\nin\n\nCopepods_block\nout\n\nKrill_block\nout\n\nCopepods\n\nKrill\n\nKrill\n\nin\n\nCopepods_block\n\nKrill_block\n\nin\n\nDietCope_block\nDiet_Cop\n\nDiet_Krill\n\nSpawner", "val ) [19]\n\n+\n\nDietKrill Survival\n\nSame as DietCopepods Survival,\nbut for krill\n\n+\n\nSpawners Survival\n\nIncreased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]\n\n10\n\nbiomass\ndensitysurvival\n\nSeaIce\n\nout\n\nColdPool\n\nf\n n\n\nColdPool\n\nin\n\nCopepods_block\nout\n\nKrill_block\nout\n\nCopepods\n\nKrill\n\nKrill\n\nin\n\nCopepods_block\n\nKrill_block\n\nin\n\nDietCope_block\nDiet_Cop\n\nDiet_Krill\n\nSpawners\n\nout\n\nDiet_Cop\nSurvival\n\nin_copepods\n\nout\n\nSpawners\n\nout\n\nDiet_Cop\nin_copepods\n\nin_spawners\n\nDiet_Krill\n\nSpawners\n\nin_krill\n\nSurvival_block\n\nin_spawners\n\ng2\n\ng1\n\n n\n\nid\n n\nh\n\nid\n n\nk\n n\n\n n\npr1\n\npr2\nn\n\nn\n\nSurvival\n\nm\n n\n\n(b)\n\n(c)\n\n(d)\n\nout\n\n n\n\n n\n\nSurvival\n\nout\n\n(a)\n\nin\n\nDietKrill_block\n\nDietCope_block\n\nout\n\nDiet_Krill\nin_krill\n\nSurvival_block\n\nKrill\n\nin\n\nin\n\nDietKrill_block\n\n n\n\nout\n\nCopepods", "s\n\nout\n\nDiet_Cop\nSurvival\n\nin_copepods\n\nout\n\nSpawners\n\nout\n\nDiet_Cop\nin_copepods\n\nin_spawners\n\nDiet_Krill\n\nSpawners\n\nin_krill\n\nSurvival_block\n\nin_spawners\n\ng2\n\ng1\n\n n\n\nid\n n\nh\n\nid\n n\nk\n n\n\n n\npr1\n\npr2\nn\n\nn\n\nSurvival\n\nm\n n\n\n(b)\n\n(c)\n\n(d)\n\nout\n\n n\n\n n\n\nSurvival\n\nout\n\n(a)\n\nin\n\nDietKrill_block\n\nDietCope_block\n\nout\n\nDiet_Krill\nin_krill\n\nSurvival_block\n\nKrill\n\nin\n\nin\n\nDietKrill_block\n\n n\n\nout\n\nCopepods\n\nid\n\nid\n\nin\nin\n\nout\n\nCopepods\n\n n\n\nColdPool_block\n\nout\n\nColdPool\n\nid\n\nin\n\nColdPool_block\nSeaIce\n\n n\n\nSeaIce\n\nin\n\nn\n\n n\npr3\n\nFigure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), t", "id\n\nid\n\nin\nin\n\nout\n\nCopepods\n\n n\n\nColdPool_block\n\nout\n\nColdPool\n\nid\n\nin\n\nColdPool_block\nSeaIce\n\n n\n\nSeaIce\n\nin\n\nn\n\n n\npr3\n\nFigure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a", "hey point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whicheve", ") shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.\n\n3.1\n\nNetlists\n\nThe term netlist appears to have entered the technical lexicon in the early\ndays of", "r is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.\n\n3.1\n\nNetlists\n\nThe term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of", "computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made.", "possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made. Each\nport is either an output, which means that it determines the value of the variable\n11\n\nPart 2 (capacitor)\nNet 1\n\nin\n\nout\n\nNet 2\nin\n\nout\n\nPart 1\n(Battery)\n\nPart 3 (resistor)\nin\n\nout\n\nNet 3\n\nFigure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach ne", "Each\nport is either an output, which means that it determines the value of the variable\n11\n\nPart 2 (capacitor)\nNet 1\n\nin\n\nout\n\nNet 2\nin\n\nout\n\nPart 1\n(Battery)\n\nPart 3 (resistor)\nin\n\nout\n\nNet 3\n\nFigure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output", "t specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three pa", "variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets sp", "rts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated", "ecify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12\n\n each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the sam", "into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12\n\n each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vert", "e name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. Th", "ex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as need", "e partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergrap", "ed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-ou", "h for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was const", "tput functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13\n\nsomewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is", "ructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13\n\nsomewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply th", "nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The ne", "e name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist", "tlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).\n\n3.2\n\nSheaves and cosheaves\n\nSheaves and cos", "graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).\n\n3.2\n\nSheaves and cosheaves\n\nSheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally", "heaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborho", "induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborhoods. \n14\n\nDefinition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological", "ods. \n14\n\nDefinition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the", "space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P,", "usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7.", ") is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U", "Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U ) is the identity function.\n15\n\nDually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U", ") is the identity function.\n15\n\nDually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to th", "U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For", "e points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U\n\nIf U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S o", "this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U\n\nIf U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,\n\nG\nC(X) = \nC(U ) / ,\nU open\n\nwhere is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) =", "n a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,\n\nG\nC(X) = \nC(U ) / ,\nU open\n\nwhere is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16\n\nIntuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional", "c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16\n\nIntuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for", "gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the", "every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p\n\ncS (a) := \n\nX\n\nX\n\nU T , V T :V U\n\np\n\n(dV (a(V ), S(V U )a(U ))) \n\n,\n\n(10)\n\nwhere p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the", "stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p\n\ncS (a) := \n\nX\n\nX\n\nU T , V T :V U\n\np\n\n(dV (a(V ), S(V U )a(U ))) \n\n,\n\n(10)\n\nwhere p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV\n\nn\nX\nk=1\n\n!1/p\np\n\n|xk yk |\n\n,\n\nwhere αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if d", "relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV\n\nn\nX\nk=1\n\n!1/p\np\n\n|xk yk |\n\n,\n\nwhere αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.\n\n17\n\nConsistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius i", "ifferent\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.\n\n17\n\nConsistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an as", "s a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T\n\nWe will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T\n\nAs the rest of this chapter shows, minimizing the consistency radius of a\ngiv", "signment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T\n\nWe will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T\n\nAs the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.\n\n3.3\n\nThe netlist sheaf\n\nThe key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (T", "en local assignment is the primary tool for sheaf-based inference.\n\n3.3\n\nThe netlist sheaf\n\nThe key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the d", "heorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalk", "etails, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18\n\nConstraints\n\nx\n\nx\n\nm b x\n\nx\n\nm b\n\npr1\n\nx\n\n n\n\npr2\n\npr3\n\ny = mx + b\n\ny = mx + b\n\ny\n\ny", "s represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18\n\nConstraints\n\nx\n\nx\n\nm b x\n\nx\n\nm b\n\npr1\n\nx\n\n n\n\npr2\n\npr3\n\ny = mx + b\n\ny = mx + b\n\ny\n\ny\n\ny\n\ny\n\ny\n\n n\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nn\n\nf\n\nAssignment support\n\nFigure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the", "y\n\ny\n\ny\n\n n\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nn\n\nf\n\nAssignment support\n\nFigure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both", "remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n", "have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n\npoints in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM gra", "points in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) a", "ph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19\n\nthe stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part i", "nd the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19\n\nthe stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the", "s the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).\n\n(11)\n\nThe function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copi", "prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).\n\n(11)\n\nThe function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form\n\na := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =\n\np\n\np\n\n|m\ne m| + |eb", "es of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form\n\na := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =\n\np\n\np\n\n|m\ne m| + |eb b| +\n\nn\nX\nk=1\n\np\n\n|f\nxk xk | +\n\nn\nX\nk=1\n\n!1/p\np\n\n|b + mf\nx k yk |\n\n(13)\n\nfor a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is eq", "b| +\n\nn\nX\nk=1\n\np\n\n|f\nxk xk | +\n\nn\nX\nk=1\n\n!1/p\np\n\n|b + mf\nx k yk |\n\n(13)\n\nfor a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shade", "ual, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20\n\n(14)\n\nwhere the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such t", "d\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20\n\n(14)\n\nwhere the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )\n\n|m\ne m|2 + |eb b|2 +\n\nn\nX\nk=1\n\n|f\nxk xk |2 +\n\nn\nX\nk=1\n\n!1/2\n|b + mf\nx k y k", "hat\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )\n\n|m\ne m|2 + |eb b|2 +\n\nn\nX\nk=1\n\n|f\nxk xk |2 +\n\nn\nX\nk=1\n\n!1/2\n|b + mf\nx k y k |2\n\nBut since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1\n\nk=1\n\nThis permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After", "|2\n\nBut since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1\n\nk=1\n\nThis permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimiza", "minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1\n\nConsistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y", "tion problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1\n\nConsistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21\n\n.\n\ny\n\ny1\n\ny = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius\n\nb + mx1\nconstrained\nconsistency\nradius\n\nx\nx1\n\nx~1\n\nFigure 4: Geometric meanings of the terms contribut", ". Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21\n\n.\n\ny\n\ny1\n\ny = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius\n\nb + mx1\nconstrained\nconsistency\nradius\n\nx\nx1\n\nx~1\n\nFigure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,\n\n(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization", "ing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,\n\n(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for c", "does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for completeness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22\n\nConstraints\n\npr1\n\n...\n\npr3\n\npr2\n\nprn+2\n\nf1 f", "ompleteness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22\n\nConstraints\n\npr1\n\n...\n\npr3\n\npr2\n\nprn+2\n\nf1 f2\n\nn\n\nAssignment support\n\nfn\n\n... \n\nFigure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf con", "2\n\nn\n\nAssignment support\n\nfn\n\n... \n\nFigure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nt", "struction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by\n\na := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by ex", "hat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by\n\na := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the th", "actly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible", "ree lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9).\n23\n\nX\n\nx\n\nx\nout\n\nf\n\ng\n\ng\n\nid\n\nX\n\nX\n\nid\n\nf\n\nin\n\ng\n\nf\n\nin\n\nout\n\ny\n\ny\n\nX\n\n(a)\n\n(b)\n\n(c)\n\nFigure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever", ",\nthe resulting sheaf will or will not have global sections (Definition 9).\n23\n\nX\n\nx\n\nx\nout\n\nf\n\ng\n\ng\n\nid\n\nX\n\nX\n\nid\n\nf\n\nin\n\ng\n\nf\n\nin\n\nout\n\ny\n\ny\n\nX\n\n(a)\n\n(b)\n\n(c)\n\nFigure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model auto", "the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each pa", "regressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The", "rt is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nproje", "following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries.\n24\n\nx1, ... xn\nS\nin\n\na1, ... ak\n\ncoef\n\nLCF(k)\n\npr2\nk\n\n S\n\nout\n\nyn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n(a)\n\npr1\n\n k\n\nS\n(b)\n\nFigure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsol", "ction that selects the corresponding observation from its parent timeseries.\n24\n\nx1, ... xn\nS\nin\n\na1, ... ak\n\ncoef\n\nLCF(k)\n\npr2\nk\n\n S\n\nout\n\nyn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n(a)\n\npr1\n\n k\n\nS\n(b)\n\nFigure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of", "utions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.\n\n3.4\n\nSheaves modeling autoregressive timeseries\n\nAutoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1", "variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.\n\n3.4\n\nSheaves modeling autoregressive timeseries\n\nAutoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for s", ", . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure", "hort in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25\n\n... x1, ... xn\n\ns\n\nout\n\nin\n\nidentity\n\nLCF(k)\n\ncoef\n\na1, ... ak\n\nout\n\nin\n\nid\n\npr2\n\ns\n\n k s\n\nid\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\ns\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive", "7(a). Using Definition 13, we obtain the\n25\n\n... x1, ... xn\n\ns\n\nout\n\nin\n\nidentity\n\nLCF(k)\n\ncoef\n\na1, ... ak\n\nout\n\nin\n\nid\n\npr2\n\ns\n\n k s\n\nid\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\ns\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms", "timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the value", "are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.", "s of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.\nWhen the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a", "When the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.\n\n26\n\nx1, ... xn\n n\nin\n\nin\n\ncrop\n\nLCF(k)\n\na1, ... ak\n\ncoef\n\nout", "slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.\n\n26\n\nx1, ... xn\n n\nin\n\nin\n\ncrop\n\nLCF(k)\n\na1, ... ak\n\ncoef\n\nout\n\nout\n\nid\n\npr2\n\n n\n\n k n\n\nprk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n n-k\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\n\n n\n\nin\n\n k n\n\nDietCope_lag\n\nCopepods\nin\n\npr2\n\nout\n\nid\n\ncrop\n\n n\nh\n\nDietCope_block\nout\n\nLCF(k)\nprk..n\n\n n - k\n\n n\n\nDiet_Cop\n(a)\n\n(b)\n\nFigure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for th", "out\n\nid\n\npr2\n\n n\n\n k n\n\nprk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n n-k\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\n\n n\n\nin\n\n k n\n\nDietCope_lag\n\nCopepods\nin\n\npr2\n\nout\n\nid\n\ncrop\n\n n\nh\n\nDietCope_block\nout\n\nLCF(k)\nprk..n\n\n n - k\n\n n\n\nDiet_Cop\n(a)\n\n(b)\n\nFigure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.\n\n27\n\nAutoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specificall", "e Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.\n\n27\n\nAutoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropp", "y, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.\n\n4\n\nSheaf encoding of the Bering Sea\n\nWe now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to Figure 1. The reader is directed to [36", "ed to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.\n\n4\n\nSheaf encoding of the Bering Sea\n\nWe now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to Figure 1. The reader is directed to [36] for the software that\ngenerates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each", "] for the software that\ngenerates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the res", "part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model d", "ulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\ns", "oes not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight", "olution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is\n\n28\n\nSeaIce\n\nColdPool\nln (ColdPool ) [ln(km2 )]\n\nln (SeaIce) [ln(km2 )]\n\n1\n0.5\n0.0\n 0.5\n 1.0\n\n0\n 1\n 2\n\n2018 ColdPool\n\n 3\n\nDietCopepods\nln (DietCopepods) [ ]\n\n3\n\nln (Copepods) [ln(count/m )]\n\nCopepods\n2.5\n0.0\n 2.5\n 5.0\n\n1\n0\n 1\n 2\n\nDietKrill\n\n3\n\nln (Krill ) [ln(count/m )]\n\nKrill\nln (DietKrill ) [ ]\n\n0", "the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is\n\n28\n\nSeaIce\n\nColdPool\nln (ColdPool ) [ln(km2 )]\n\nln (SeaIce) [ln(km2 )]\n\n1\n0.5\n0.0\n 0.5\n 1.0\n\n0\n 1\n 2\n\n2018 ColdPool\n\n 3\n\nDietCopepods\nln (DietCopepods) [ ]\n\n3\n\nln (Copepods) [ln(count/m )]\n\nCopepods\n2.5\n0.0\n 2.5\n 5.0\n\n1\n0\n 1\n 2\n\nDietKrill\n\n3\n\nln (Krill ) [ln(count/m )]\n\nKrill\nln (DietKrill ) [ ]\n\n0.5\n0.0\n 0.5\n\n2016 Krill\n\n 1.0\n\n0.5\n0.0\n 0.5\n 1.0\n\nSpawners\n\nSurvival\nln (Survival ) [103 count/kg]\n\nSpawners[106 kg]\n\n4\n3\n2\n1\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nmeasurement\n\n2\n1\n0\n 1\n 2\n1960\n\nDSEM\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nsheaf\n\nFigure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas con", ".5\n0.0\n 0.5\n\n2016 Krill\n\n 1.0\n\n0.5\n0.0\n 0.5\n 1.0\n\nSpawners\n\nSurvival\nln (Survival ) [103 count/kg]\n\nSpawners[106 kg]\n\n4\n3\n2\n1\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nmeasurement\n\n2\n1\n0\n 1\n 2\n1960\n\nDSEM\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nsheaf\n\nFigure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.\n\n29\n\nCopepods_pc\n\nCopepods\n\n n\npr2\n\npc\n\nin\n\nDietCope_block\n\npr1\n\nout\n\nDiet_Cop\n\n n\ng~1\n n\n\n(a)\n\n(b)\n\nFigure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the", "strained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.\n\n29\n\nCopepods_pc\n\nCopepods\n\n n\npr2\n\npc\n\nin\n\nDietCope_block\n\npr1\n\nout\n\nDiet_Cop\n\n n\ng~1\n n\n\n(a)\n\n(b)\n\nFigure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown", "DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that", "in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in", "correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency", "Section 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of", "is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30\n\nSeaIce\nSeaIce_lag\nSeaIce_lagvar\n\nColdPool_block\n\nColdPool_lagvar\n\nSeaIce_pc\n\nColdPool_lag\nColdPool\n\n2018_ColdPool\n\nColdPool_Copepods_pc\n\nColdPool_Krill_pc\nKrill_block\n\nCopepods_block\n\n2016_Krill\nCopepods\n\nK", "ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30\n\nSeaIce\nSeaIce_lag\nSeaIce_lagvar\n\nColdPool_block\n\nColdPool_lagvar\n\nSeaIce_pc\n\nColdPool_lag\nColdPool\n\n2018_ColdPool\n\nColdPool_Copepods_pc\n\nColdPool_Krill_pc\nKrill_block\n\nCopepods_block\n\n2016_Krill\nCopepods\n\nKrill\nCopepods_lag\n\nCopepods_pc\nDietCopepods_block\n\nCopepods_lagvar\n\nKrill_lag\n\nKrill_pc\nDietKrill_block\n\nKrill_lagvar\n\nDietCopepods\n\nSpawners\n\nDietKrill\nDietCopepods_pc DietKrill_pc\n\nDietCopepods_lag\n\nDietKrill_lag\nSpawners_pc\n\nDietCopepods_lagvar\n\nSpawners_lag\n\nDietKrill_lagvar\nSpawners_lagvar\n\nSurvival_block\n\nSurvival\n\ncells\n\nrestrictions\nprojection map\nother function (see text)\n\ninferred variab", "rill\nCopepods_lag\n\nCopepods_pc\nDietCopepods_block\n\nCopepods_lagvar\n\nKrill_lag\n\nKrill_pc\nDietKrill_block\n\nKrill_lagvar\n\nDietCopepods\n\nSpawners\n\nDietKrill\nDietCopepods_pc DietKrill_pc\n\nDietCopepods_lag\n\nDietKrill_lag\nSpawners_pc\n\nDietCopepods_lagvar\n\nSpawners_lag\n\nDietKrill_lagvar\nSpawners_lagvar\n\nSurvival_block\n\nSurvival\n\ncells\n\nrestrictions\nprojection map\nother function (see text)\n\ninferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present\n\n0\n2\n4\nconsistency radius contribution\n\nFigure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that a", "le (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present\n\n0\n2\n4\nconsistency radius contribution\n\nFigure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31\n\nSource\n\nTarget\n\nSeaIce\nCo", "re also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31\n\nSource\n\nTarget\n\nSeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)\n\nDSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2\n\nnone\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848\n\nSheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03", "ldPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)\n\nDSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2\n\nnone\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848\n\nSheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03\n2637\n2679\n\nAR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907\n\nTable 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should", "2637\n2679\n\nAR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907\n\nTable 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by", "interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) T", "an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregre", "he DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coeffi", "ssive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table\n\n32\n\nVariable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods", "cients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table\n\n32\n\nVariable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill\n\nAR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060\n\nAR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445\n\nTable 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the four", "DietKrill\n\nAR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060\n\nAR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445\n\nTable 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were appli", "th column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representativ", "ed. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linea", "e when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.\n\n5\n\nThe topology of subsystems\n\nClassically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a dual", "r, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.\n\n5\n\nThe topology of subsystems\n\nClassically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n3", "ity pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33\n\nbase space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether accordin", "3\n\nbase space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as", "g to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)\n\n5.1\n\nDynamical systems\n\nDefinition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a cl", "a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)\n\n5.1\n\nDynamical systems\n\nDefinition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f :", "assical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram", "S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU\n\nf\n\ni\n\nV\n\nf\n\n/U\n\n/V\n\ni \n\ncommutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34\n\nProof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative dia", "U\n\nf\n\ni\n\nV\n\nf\n\n/U\n\n/V\n\ni \n\ncommutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34\n\nProof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng\n\ng\n\nS2\n\nf2\n\n/ S2\n\nComposition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.\n\n5.2\n\nThe cosheaf endomorphism of invariant sets\n\nThe state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical", "gram of the\nform\nf1\n/ S1\nS1\ng\n\ng\n\nS2\n\nf2\n\n/ S2\n\nComposition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.\n\n5.2\n\nThe cosheaf endomorphism of invariant sets\n\nThe state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance", "system is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2.", "of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V\n\nW", "CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V\n\nW V\n\nW V\n\nsince the equivalence identifies points that agree on overlaps.\n35\n\nThe above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S i", "V\n\nW V\n\nsince the equivalence identifies points that agree on overlaps.\n35\n\nThe above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))\n\nmV\n\n/ S(V )\nS(U V )\n\nR(f 1 (U )) mU / S(U )\n\nDually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphi", "s a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))\n\nmV\n\n/ S(V )\nS(U V )\n\nR(f 1 (U )) mU / S(U )\n\nDually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO\n\nmV\n\n/ S(V )\nO\n\nR(f 1 (U ) f 1 (V ))\n\nS(U V )\n\nR(f 1 (U ))\n\nmU\n\n/ S(U )\n\nWith the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological spa", "sm m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO\n\nmV\n\n/ S(V )\nO\n\nR(f 1 (U ) f 1 (V ))\n\nS(U V )\n\nR(f 1 (U ))\n\nmU\n\n/ S(U )\n\nWith the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U", "ce\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )\n\nFU :=f |U\n\nC(X,TX ) (f 1 (U ) f 1 (V ))\n\nC(X,TX ) (f 1 (V )) = f 1 (V )\n\n/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )\n\nFV :=f |V\n\n/ C(Y,T ) (V ) = V\nY\n\nwhich establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36\n\nNow suppose that we have two continuous m", ") f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )\n\nFU :=f |U\n\nC(X,TX ) (f 1 (U ) f 1 (V ))\n\nC(X,TX ) (f 1 (V )) = f 1 (V )\n\n/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )\n\nFV :=f |V\n\n/ C(Y,T ) (V ) = V\nY\n\nwhich establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36\n\nNow suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subs", "aps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to", "ets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU\n\nmU =f\n\n/U\n\ni\n\nV\n\nmV =f\n\n/V\n\ni\n\ncommutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.\n\n5.3\n\nSubsystem decomposition sheaf\n\nRather than carving up the state space into", "U . If U V are two invariant sets, then Proposition 8\nimplies that\nU\n\nmU =f\n\n/U\n\ni\n\nV\n\nmV =f\n\n/V\n\ni\n\ncommutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.\n\n5.3\n\nSubsystem decomposition sheaf\n\nRather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose th", "different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni\n\nU\n\np\n\ni\n\nf |U\n\nB\n\n/U\n\np\n\ng\n\n/B\n\nDually, the diagram at right above captures the situation where B is a subsystem\nof f .\n\n37\n\nDefinition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g :", "at f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni\n\nU\n\np\n\ni\n\nf |U\n\nB\n\n/U\n\np\n\ng\n\n/B\n\nDually, the diagram at right above captures the situation where B is a subsystem\nof f .\n\n37\n\nDefinition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can b", "B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explici", "e read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an", "tly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subs", ", an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n\n=", "ystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n\n= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )\n\n= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38\n\nExample 3.\n?B\nA\n\nC\n\nFollowing the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B,", "p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )\n\n= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38\n\nExample 3.\n?B\nA\n\nC\n\nFollowing the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA\n\n?C\n\nB\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. C", "and C given by\nthe graph\nA\n\n?C\n\nB\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)\n\nAh\n\nB\n\n(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next", "onsider the DSEM on variables A and B given by the graph\n)\n\nAh\n\nB\n\n(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv\n\n39\n\nfor some λ. Then the subspace spanned by v is an", "example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv\n\n39\n\nfor some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomp", "invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))\n\n= prW (f (u) + f (w))\n\n= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)\n\n= (f |W ) (prW (u + w))\n= (f |W prW )(v).\n\nLemma 13. The relation is a subsystem of is a preorder, or in o", "osition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))\n\n= prW (f (u) + f (w))\n\n= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)\n\n= (f |W ) (prW (u + w))\n= (f |W prW )(v).\n\nLemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf\n\nS\np2\n\np1 p2\n\nB2\n\np2\ng2\n\np1\n\nB1\n\n/S\n\n/ B2\n\np1 p2\n\np1\n\ng1\n\n/ B1\n\nso that (g1 , (p1 p2", "ther words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf\n\nS\np2\n\np1 p2\n\nB2\n\np2\ng2\n\np1\n\nB1\n\n/S\n\n/ B2\n\np1 p2\n\np1\n\ng1\n\n/ B1\n\nso that (g1 , (p1 p2 )) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).\n\n40\n\nTh", ")) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).\n\n40\n\nThis has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1\n\nZ\n\nf\n\n/ Z3\npr1\n\ng\n\n/Z\n\nIn this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R", "is has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1\n\nZ\n\nf\n\n/ Z3\npr1\n\ng\n\n/Z\n\nIn this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2\n\nf\n\np\n\nB\n\n/ R2\np\n\nid\n\n/B\n\nwhere p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system w", "2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2\n\nf\n\np\n\nB\n\n/ R2\np\n\nid\n\n/B\n\nwhere p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS\n\nf\n\np\n\nB\n\n/S\np\n\ng\n\n/B\n\nSuppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means", "ith g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS\n\nf\n\np\n\nB\n\n/S\np\n\ng\n\n/B\n\nSuppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .\n\n41\n\nLemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram o", "that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .\n\n41\n\nLemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\n\n/S\n\nf\n\nS\np\n\np\n\nB\n\n/B\n\ng\n\nThe conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this c", "f solid arrows below commutes:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\n\n/S\n\nf\n\nS\np\n\np\n\nB\n\n/B\n\ng\n\nThe conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\nf\n\nS\n\nB\n\ng\n\n/S\n\n/B\n\nTherefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected", "onsists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\nf\n\nS\n\nB\n\ng\n\n/S\n\n/B\n\nTherefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user", "behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42\n\nbut rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well.", "loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42\n\nbut rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU\n\nf |U\n\nX\n\n/U\n\nf\n\n/X\n\nAs an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program", "Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU\n\nf |U\n\nX\n\n/U\n\nf\n\n/X\n\nAs an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-le", "will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU\n\nf |U\n\nq\n\nY\n\ng\n\n/U", "vel program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU\n\nf |U\n\nq\n\nY\n\ng\n\n/U\n\n/Y\n\nq\n\nwhere the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np\n\nY\n\ng\n\n43\n\n/Y\n\np\n\nValues of X for which", "/Y\n\nq\n\nwhere the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np\n\nY\n\ng\n\n43\n\n/Y\n\np\n\nValues of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutat", "the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO\n\nf |U\n\ni\n\nY\n\ng\n\n/U\n\n/Y\n\nf\n\nXO\nq\n\n i\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nFor instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,\n\nR1 = x,\nR2 = y,\n\nR[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO\n\nf |U\n\n/U\nO\n\ni\n\nY\n\ni\ng\n\n/Y\n\nInspired by Example 9, su", "ive diagrams\nUO\n\nf |U\n\ni\n\nY\n\ng\n\n/U\n\n/Y\n\nf\n\nXO\nq\n\n i\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nFor instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,\n\nR1 = x,\nR2 = y,\n\nR[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO\n\nf |U\n\n/U\nO\n\ni\n\nY\n\ni\ng\n\n/Y\n\nInspired by Example 9, suppose that we have a commutative diagram\nXO\n\nf\n\ni\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nwhere i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX\n\nf\n\np\n\nY\n\ng\n\n44\n\n/X\n\n/Y\n\np\n\ncommute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case", "ppose that we have a commutative diagram\nXO\n\nf\n\ni\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nwhere i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX\n\nf\n\np\n\nY\n\ng\n\n44\n\n/X\n\n/Y\n\np\n\ncommute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon su", "that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1\n\nf1\n\n/ S1\nO\n\np1\n\np1\n\nS\n\n/S\n\nf\n\np2\n\nS2\n\np2\n\nf2\n\n/ S2\n\nWe want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition", "bsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1\n\nf1\n\n/ S1\nO\n\np1\n\np1\n\nS\n\n/S\n\nf\n\np2\n\nS2\n\np2\n\nf2\n\n/ S2\n\nWe want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\n/ S3\n\nsuch that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below", "for the dashed arrows in\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\n/ S3\n\nsuch that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\nf1\n\n/ S3\n\nS1\nf3\n\nf2\n\nS2\n\n45\n\np \n3\n\np 3\n\n/ S3\n\nProposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.", "S\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\nf1\n\n/ S3\n\nS1\nf3\n\nf2\n\nS2\n\n45\n\np \n3\n\np 3\n\n/ S3\n\nProposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.\nAs a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1\n\ng1\n\np2\n\nB2\n\np2\n\ng2\n\n/ B2\n\ng1\n\n/ B1\n\np1\n\nB1\n\n/ B1\n\np1\n\nSince p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since", "As a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1\n\ng1\n\np2\n\nB2\n\np2\n\ng2\n\n/ B2\n\ng1\n\n/ B1\n\np1\n\nB1\n\n/ B1\n\np1\n\nSince p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid\n\nZO 2\n\nf\n\n/ Z2\nid\n\nh\n\n/ Z2\nO\n\ni", "it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid\n\nZO 2\n\nf\n\n/ Z2\nid\n\nh\n\n/ Z2\nO\n\nid\n\nid\n\nZ2\n\ng\n\n/ Z2\n\nwhere\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).\n\n46\n\nThere is also no function h that will make the diagram below commute\nZ2\npr1\n\nZO\n\nf\n\n/Z\nid\n\nh\n\n/Z\nO\n\npr2\n\nZ2\n\nid\ng\n\n/Z\n\nwhere\nf (x, y) = 1 x,\n\nand\n\ng(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (", "d\n\nid\n\nZ2\n\ng\n\n/ Z2\n\nwhere\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).\n\n46\n\nThere is also no function h that will make the diagram below commute\nZ2\npr1\n\nZO\n\nf\n\n/Z\nid\n\nh\n\n/Z\nO\n\npr2\n\nZ2\n\nid\ng\n\n/Z\n\nwhere\nf (x, y) = 1 x,\n\nand\n\ng(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f acco", "B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since", "rding to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf", "there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB.\np:S B subsystem\n\np:S B subsystem\n\nSuppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47\n\nTherefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems", "(p) =\nB.\np:S B subsystem\n\np:S B subsystem\n\nSuppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47\n\nTherefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S v", ", and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because", "ia self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of inte", "we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.\n\nLemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v", "rest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.\n\nLemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynam", "I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nth", "ical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48\n\nf\n\n k\npr1\n\n k\npr1\n\n k \n\n k \n\npr1\n\npr1\n\n( k ) \n\n( k ) \npr1\n\npr1\n\ng\n\n( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4\n\npr1\n( k ) \npr1\n\npr1\n( k ) \npr1\n\n( k ) \n( k ) \npr1,2,5,6\n\n k\n\npr1,2,3,4\n\npr7\n\n k k \n\n k\n\npr7\n\n k k \n\nFigure 14: Sheaf of subsystems", "at are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48\n\nf\n\n k\npr1\n\n k\npr1\n\n k \n\n k \n\npr1\n\npr1\n\n( k ) \n\n( k ) \npr1\n\npr1\n\ng\n\n( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4\n\npr1\n( k ) \npr1\n\npr1\n( k ) \npr1\n\n( k ) \n( k ) \npr1,2,5,6\n\n k\n\npr1,2,3,4\n\npr7\n\n k k \n\n k\n\npr7\n\n k k \n\nFigure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.\n\n6\n\nSubsystems of the Bering Sea system\n\nFigure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\na", "for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.\n\n6\n\nSubsystems of the Bering Sea system\n\nFigure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0\n\nwhile the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0\n\nNotice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with mod", "i xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0\n\nwhile the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0\n\nNotice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynami", "ifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49\n\nDefinition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those res", "cally related\nvariables, each represented by sliding windows of timeseries,\n49\n\nDefinition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.\n\n7\n\nConclusion\n\nIn this chapter, we have demonstrated how the", "triction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.\n\n7\n\nConclusion\n\nIn this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling a", "general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that whe", "re that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system model", "n a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50\n\nhybrid versions, for instance performing simultaneous f", "er can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50\n\nhybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding she", "orward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard st", "af. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.\n\nAcknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Indepen", "atistical hypothesis testing framework.\n\nAcknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "sentence", "num_chunks": 508, "avg_chunk_len": 204.43503937007873, "std_chunk_len": 206.5435020063847, "max_chunk_len": 2033, "min_chunk_len": 5, "total_chars": 103853, "compression_ratio": 1.0035434700971566, "chunks": ["arXiv:2511. 04603v1 [math. AT] 6 Nov 2025\n\nAnalyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L.", "Szulczewski\nJames T. Thorson\nSeptember 2025\n\nAbstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea.", "The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.", "Contents\n1 Introduction\n1. 1 Related work . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", "1. 2 Contributions . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", "1. 3 Chapter outline . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . 2\n3\n4\n5\n\n2 Dynamical modeling of ecosystems\n2.", "1 DSEM background and motivation . . .", ". . .", ". . .", ". . .", ". . .", ". . 2.", "2 Ecological background and the DSEM system for the Bering Sea\n\n5\n5\n7\n\n Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor.", "2025 The MITRE Corporation. ALL RIGHTS RESERVED. 1\n\n3 Sheaf encodings of composite systems\n3.", "1 Netlists . . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . 3.", "2 Sheaves and cosheaves . . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", "3. 3 The netlist sheaf . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". 3. 4 Sheaves modeling autoregressive timeseries .", ". . .", ". . .", ". . .", ". . 8\n11\n14\n18\n25\n\n4 Sheaf encoding of the Bering Sea\n\n28\n\n5 The topology of subsystems\n33\n5.", "1 Dynamical systems . . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . 34\n5.", "2 The cosheaf endomorphism of invariant sets . . .", ". . .", ". . .", ". . .", "35\n5. 3 Subsystem decomposition sheaf . .", ". . .", ". . .", ". . .", ". . .", ". . .", ". . 37\n6 Subsystems of the Bering Sea system\n\n49\n\n7 Conclusion\n\n50\n\n1\n\nIntroduction\n\nEcologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48].", "These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time. Understanding the dynamics of these kind of large composite models is much\neasier reductively.", "Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20). Subsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems.", "This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5). This chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6).", "The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries. The translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5).", "One real-world example\n2\n\ninvolves part of the food web in the Bering Sea (Figure 1; Sections 2. 2, 4, and\n6). Sheaves provide many advantages to a modeler.", "They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).", "Sheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.", "1. 1\n\nRelated work\n\nThe challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5].", "Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47]. The key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions.", "Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18]. Although algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al.", "[27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. One challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27].", "For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4]. We argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught.", "Aspects of the formalism we introduce in this chapter are not entirely novel. For instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20.", "Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software\n\n3\n\n[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7). Roughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality).", "Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.", "Finally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22].", "In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure. 1.", "2\n\nContributions\n\nThis chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.", "This chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3.", "the netlist sheaf with additional stalks for individual observations (Definition 14). 4\n\n1. 3\n\nChapter outline\n\nSection 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves.", "This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs.", "Sheaves can be nonlinear,\nwhereas DSEMs are linear. Section 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed.", "Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well. In Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail.", "Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail. Section 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21).", "This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems. Section 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.", "Finally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work. 2\n\nDynamical modeling of ecosystems\n\nThis section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter. 2.", "1\n\nDSEM background and motivation\n\nDefinition 1. Given a set of variables X = {x1 , . .", ". , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5\n\nLinearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge. The absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0.", "For brevity, we write a vertex (xj , tk ) simply as xj,k . The variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j .", "Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ). dτ\ni=1 j=1\n\n(1)\n\nIn what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM. In the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.", "Exogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.", "Exogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ). =\ndτ\ni=1 j=1\n\n(2)\n\nWe can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h\n\nJ X\nT\nX\ni=1 j=1\n\nγk, ,i,j xi (τ tj ) + hϵk, (τ ). (3)\n\nIf we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,\n\n(4)\n\nwhere the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6\n\nthe xk (τ t h) terms.", "In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P. To obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj . Equation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT\n\nQ = (id P )V\n\n 1\n\n(5)\n(id P),\n\n(6)\n\nwhere id is the identity matrix.", "The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects. Now we address measurement errors.", "Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j . ) Let us write yk,j\nfor the observation of the variable xk,j .", "We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via\n\nyk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean. The clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj .", "With this in hand, V takes the form\nV = id T T GGT ,\n\n(7)\n\nwhere is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert. 2.", "2\n\nEcological background and the DSEM system for the\nBering Sea\n\nTo demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7\n\nby specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt\n\n(8)\n\nα\n\nwhere e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42].", "The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:\n\nRt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.", "The DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock.", "See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.", "3\n\nSheaf encodings of composite systems\n\nIn this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.", "2 provides the necessary background. With the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM.", "Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations. 8\n\nTable 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.", "e. , subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].", "Name\nSeaIce\n\nDescription\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct. 15 to Dec. 15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]\n\nColdPool\n\nSpatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]\n\nSpawners\n\nFemale spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]\n\nSurvival\n\nAge-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners\n\nCopepods\n\nDensity of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.", "to early Oct. [12] (calculated by Dave Kimmel, pers. comm.", ")\n\nKrill\n\nIndex of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]\n\nDietCopepods\n\nBiomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers.", "comm. ). DietKrill\n\nSame as DietCopepods, but for euphausiids (krill)\n9\n\nTable 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study.", "We also include a first-order autoregressive term for\neach variable (i. e. , 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.", "2. Path\nSeaIce ColdP ool\n\nEcological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )\n\nSign\n+\n\nColdP ool Copepods\n\nWarmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]\n\n+\n\nColdP ool Krill\n\nWater temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )\n\n? Copepods DietCopepods\n\nIncreased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator\n\n+\n\nKrill DietKrill\n\nSame as Copepods DietCopepods\nbut for krill\n\n+\n\nDietCopepods Survival\n\nIncreased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]\n\n+\n\nDietKrill Survival\n\nSame as DietCopepods Survival,\nbut for krill\n\n+\n\nSpawners Survival\n\nIncreased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]\n\n10\n\nbiomass\ndensitysurvival\n\nSeaIce\n\nout\n\nColdPool\n\nf\n n\n\nColdPool\n\nin\n\nCopepods_block\nout\n\nKrill_block\nout\n\nCopepods\n\nKrill\n\nKrill\n\nin\n\nCopepods_block\n\nKrill_block\n\nin\n\nDietCope_block\nDiet_Cop\n\nDiet_Krill\n\nSpawners\n\nout\n\nDiet_Cop\nSurvival\n\nin_copepods\n\nout\n\nSpawners\n\nout\n\nDiet_Cop\nin_copepods\n\nin_spawners\n\nDiet_Krill\n\nSpawners\n\nin_krill\n\nSurvival_block\n\nin_spawners\n\ng2\n\ng1\n\n n\n\nid\n n\nh\n\nid\n n\nk\n n\n\n n\npr1\n\npr2\nn\n\nn\n\nSurvival\n\nm\n n\n\n(b)\n\n(c)\n\n(d)\n\nout\n\n n\n\n n\n\nSurvival\n\nout\n\n(a)\n\nin\n\nDietKrill_block\n\nDietCope_block\n\nout\n\nDiet_Krill\nin_krill\n\nSurvival_block\n\nKrill\n\nin\n\nin\n\nDietKrill_block\n\n n\n\nout\n\nCopepods\n\nid\n\nid\n\nin\nin\n\nout\n\nCopepods\n\n n\n\nColdPool_block\n\nout\n\nColdPool\n\nid\n\nin\n\nColdPool_block\nSeaIce\n\n n\n\nSeaIce\n\nin\n\nn\n\n n\npr3\n\nFigure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram.", "The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2. 1); in (c), they point from netlist parts to nets (Sec.", "3. 1);\nand in (d), they denote restriction functions (Sec. 3.", "2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.", "4. Throughout this section, we refer to Figure 1 for intuition. Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea.", "The DSEM-to-netlist\ntranslation, described in Section 3. 1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf.", "Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.", "4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf. 3. 1\n\nNetlists\n\nThe term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3].", "Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables). Each variable consists of the specification of a set of possible values for a\nnet.", "In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.", "4, we show how to handle missing values in such a timeseries. Each part has a number of ports, to which connections can be made. Each\nport is either an output, which means that it determines the value of the variable\n11\n\nPart 2 (capacitor)\nNet 1\n\nin\n\nout\n\nNet 2\nin\n\nout\n\nPart 1\n(Battery)\n\nPart 3 (resistor)\nin\n\nout\n\nNet 3\n\nFigure 2: A netlist for an electric circuit, described in Example 1.", "of a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it. Each net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port.", "The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port. This formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.", "Example 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1.", "Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.", "The values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit.", "This circuit contains no external inputs nor external\noutputs. A DSEM graph can be translated into a netlist via the following construction. Definition 2.", "Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12\n\n each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables. There are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph. Definition 3.", "The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net. The label on each vertex is simply the name of the part corresponding to\nthat vertex. The vertices within a hyperedge correspond to the parts connected to the\ncorresponding net.", "The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty. Because the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics.", "Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure.", "The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly. Figure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM.", "Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports. Proposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.", "Proof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist. Assume we have a set of variables for all nets that are consistent with the\ninput-output functions.", "As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13\n\nsomewhere in the netlist.", "This means that Equation (4) is satisfied by construction. Assume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.", "The wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation. Definition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net.", "The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net.", "The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net. Figure 1(c) shows the netlist graph for the Bering Sea example. Corollary 2.", "The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected. Proposition 3.", "The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully. As we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient.", "Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6). 3. 2\n\nSheaves and cosheaves\n\nSheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model.", "In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related. This nearness can be most easily formalized by using the netlist graph defined\nin the previous section. Since the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices.", "That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.", "A topological space is a mathematical formalism that captures the notion of\n neighborhoods. 14\n\nDefinition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .", "The ordered pair (X, T ) is called a topological space. Often, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U. The following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.", "Additionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below. Definition 6.", "Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }. The idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).", "Definition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1.", "For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4.", "S(U U ) is the identity function. 15\n\nDually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2.", "For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.", "If for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces. As Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.", "Definition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).", "The direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n! G\nC(U ) .", "U U\n\nIf U = T , we usually say that the (co)assignment is global. (Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.", "Definition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ). Dually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,\n\nG\nC(X) = \nC(U ) / ,\nU open\n\nwhere is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .", "Local (co)sections are defined similarly, but refers to some collection U of\nopen sets. 16\n\nIntuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf. The set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X).", "It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves. Definition 10.", "Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U. Dually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .", "For the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5. Given that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section.", "When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius. Definition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p\n\ncS (a) := \n\nX\n\nX\n\nU T , V T :V U\n\np\n\n(dV (a(V ), S(V U )a(U ))) \n\n,\n\n(10)\n\nwhere p 1.", "In all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV\n\nn\nX\nk=1\n\n!", "1/p\np\n\n|xk yk |\n\n,\n\nwhere αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.", "Corollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0. 17\n\nConsistency radius is stable under perturbations, which means that it can\nbe reliably estimated.", "Theorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.", "We will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one. Definition 12.", "[35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U . cS (a; U) := min cS (b) : b \nU T\n\nWe will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .", "U T\n\nAs the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference. 3. 3\n\nThe netlist sheaf\n\nThe key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization.", "In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6). In order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.", "This progression is summarized in Figure 3. Before delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings.", "In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs. In the sheaf diagram the arrows are functions between the stalks.", "Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18\n\nConstraints\n\nx\n\nx\n\nm b x\n\nx\n\nm b\n\npr1\n\nx\n\n n\n\npr2\n\npr3\n\ny = mx + b\n\ny = mx + b\n\ny\n\ny\n\ny\n\ny\n\ny\n\n n\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nn\n\nf\n\nAssignment support\n\nFigure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf.", "To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings. data within the part it is attached to.", "However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be.", "If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net. Beginning the example in earnest, suppose that (x1 , y1 ), . .", ". , (xn , yn ) are n\npoints in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable.", "If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a). The netlist for the problem represents the same information as in the SEM.", "As shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x). The prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c).", "The sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value.", "On the other hand,\n19\n\nthe stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs.", "Explicitly,\npr1 (m, b, (x1 , . . .", ", xn )) = m,\npr2 (m, b, (x1 , . . .", ", xn )) = b,\nand\npr3 (m, b, (x1 , . . .", ", xn )) = (x1 , . . .", ", xn ). The remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . .", ". , yn ) = f (m, b, (x1 , . .", ". , xn )) = (mx1 + b, . .", ". , mxn + b). (11)\n\nThe function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .", "The space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form\n\na := m, b, (x1 , .", ". . , xn ), (y1 , .", ". . , yn ), m,\ne eb, (f\nx1 , .", ". . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part.", "The consistency radius of this assignment is\nc(a) =\n\np\n\np\n\n|m\ne m| + |eb b| +\n\nn\nX\nk=1\n\np\n\n|f\nxk xk | +\n\nn\nX\nk=1\n\n! 1/p\np\n\n|b + mf\nx k yk |\n\n(13)\n\nfor a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.", "The problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).", "To follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.", "Explicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . .", ", xn ), (y1 , . . .", ", yn ), ) ,\n20\n\n(14)\n\nwhere the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . .", ". , x\nfn ). Minimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,.", ". . ,xn )\n\n|m\ne m|2 + |eb b|2 +\n\nn\nX\nk=1\n\n|f\nxk xk |2 +\n\nn\nX\nk=1\n\n!", "1/2\n|b + mf\nx k y k |2\n\nBut since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n! 1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,. .", ". ,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n. k=1\n\nk=1\n\nThis permits the values of the variables x and y to differ from their copies,\nsubject to a penalty.", "Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given. To obtain classical least squares regression, we must constrain x\nfk = xk for\nall k.", "The global assignment we seek is of the form\ng = (m, b, (x1 , . . .", ", xn ), (y1 , . . .", ", yn ), (m, b, (x1 , . . .", ", xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n! 1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n. k=1\n\nConsistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , .", ". . , xn ), , ) .", "Consistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . .", ". , xn ), (b + mx1 , . .", ". , b + mxn ), (m, b, (x1 , . .", ". , xn ))) ,\nconsistency radius minimization does not require constraints in this case. 21\n\n.", "y\n\ny1\n\ny = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius\n\nb + mx1\nconstrained\nconsistency\nradius\n\nx\nx1\n\nx~1\n\nFigure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13. Backward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . .", ". , yn ), ) . Consistency radius minimization will infer the values for x.", "If m = 0, this\nalways results in a global section,\n\n(m, b, ((y1 b)/m, . . .", ", (yn b)/m, (y1 , . . .", ", yn ), (m, b, ((y1 b)/m, . . .", ", (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . .", ". , xn , (y1 , . .", ". , yn ), (0, b, (x1 , . .", ". , xn ))) . Noting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.", "Regression (model fitting) (Details above, included for completeness here. )\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . .", ". , xn ), (y1 , . .", ". , yn ), ) . Consistency radius minimization will infer the values for b and m.", "As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression. 22\n\nConstraints\n\npr1\n\n. .", ". pr3\n\npr2\n\nprn+2\n\nf1 f2\n\nn\n\nAssignment support\n\nfn\n\n. .", ". Figure 5: Modification to the sheaf in Figure 3(d) to allow for missing data. Hybrid versions of the above problems can also be addressed.", "Assignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk. Figure 5 shows the resulting construction.", "The fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . .", ", xn )) = mxk + b. The set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by\n\na := m, b, x1 , .", ". . , xn , y1 , .", ". . , yn , m,\ne eb, x\nf1 , .", ". . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses.", "The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.", "Feedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9). 23\n\nX\n\nx\n\nx\nout\n\nf\n\ng\n\ng\n\nid\n\nX\n\nX\n\nid\n\nf\n\nin\n\ng\n\nf\n\nin\n\nout\n\ny\n\ny\n\nX\n\n(a)\n\n(b)\n\n(c)\n\nFigure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.", "Consider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value. X = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0. X = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.", "X = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist. Feedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3. 4.", "With the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm. Definition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N .", "The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set.", "The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables. It is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The following modification to Definition 13 allows for missing\ndata in general.", "Definition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder.", "The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries. 24\n\nx1, . .", ". xn\nS\nin\n\na1, . .", ". ak\n\ncoef\n\nLCF(k)\n\npr2\nk\n\n S\n\nout\n\nyn = a1 xn-1 + a2 xn-2 + . .", ". ak xn-k\n\n(a)\n\npr1\n\n k\n\nS\n(b)\n\nFigure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf. Theorem 6.", "Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections. Proof. (see also [34][Prop.", "6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps. Moreover, according to [35, Thm.", "1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y. 3.", "4\n\nSheaves modeling autoregressive timeseries\n\nAutoregressive timeseries are sequences . . .", ", x0 , x1 , . . .", "that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . .", ", ak . We say that such a sequence is AR(k) autoregressive. Autoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.", "It is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , .", ". . ak are constants.", "We can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25\n\n. .", ". x1, . .", ". xn\n\ns\n\nout\n\nin\n\nidentity\n\nLCF(k)\n\ncoef\n\na1, . .", ". ak\n\nout\n\nin\n\nid\n\npr2\n\ns\n\n k s\n\nid\nxn = a1 xn-1 + a2 xn-2 + . .", ". ak xn-k\n\ns\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries. netlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.", "To handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn . This is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above.", "Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)). The associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.", "Following a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts. Once again, if we consider an assignment supported on the two variables (with\nthe same values on each!", "), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x. When the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps.", "Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence.", "Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n. 26\n\nx1, . .", ". xn\n n\nin\n\nin\n\ncrop\n\nLCF(k)\n\na1, . .", ". ak\n\ncoef\n\nout\n\nout\n\nid\n\npr2\n\n n\n\n k n\n\nprk. .", "n\nxn = a1 xn-1 + a2 xn-2 + . . .", "ak xn-k\n\n n-k\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries. n\n\nin\n\n k n\n\nDietCope_lag\n\nCopepods\nin\n\npr2\n\nout\n\nid\n\ncrop\n\n n\nh\n\nDietCope_block\nout\n\nLCF(k)\nprk. .", "n\n\n n - k\n\n n\n\nDiet_Cop\n(a)\n\n(b)\n\nFigure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13. 27\n\nAutoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example.", "All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part.", "The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10. 4\n\nSheaf encoding of the Bering Sea\n\nWe now return to the ecological DSEM example introduced in Section 2.", "2, and\nrefer the reader to Figure 1. The reader is directed to [36] for the software that\ngenerates the sheaf results presented in this section. The DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).", "The netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11.", "The DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error.", "Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout.", "This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).", "As noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is\n\n28\n\nSeaIce\n\nColdPool\nln (ColdPool ) [ln(km2 )]\n\nln (SeaIce) [ln(km2 )]\n\n1\n0.", "5\n0. 0\n 0. 5\n 1.", "0\n\n0\n 1\n 2\n\n2018 ColdPool\n\n 3\n\nDietCopepods\nln (DietCopepods) [ ]\n\n3\n\nln (Copepods) [ln(count/m )]\n\nCopepods\n2. 5\n0. 0\n 2.", "5\n 5. 0\n\n1\n0\n 1\n 2\n\nDietKrill\n\n3\n\nln (Krill ) [ln(count/m )]\n\nKrill\nln (DietKrill ) [ ]\n\n0. 5\n0.", "0\n 0. 5\n\n2016 Krill\n\n 1. 0\n\n0.", "5\n0. 0\n 0. 5\n 1.", "0\n\nSpawners\n\nSurvival\nln (Survival ) [103 count/kg]\n\nSpawners[106 kg]\n\n4\n3\n2\n1\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nmeasurement\n\n2\n1\n0\n 1\n 2\n1960\n\nDSEM\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nsheaf\n\nFigure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied. 29\n\nCopepods_pc\n\nCopepods\n\n n\npr2\n\npc\n\nin\n\nDietCope_block\n\npr1\n\nout\n\nDiet_Cop\n\n n\ng~1\n n\n\n(a)\n\n(b)\n\nFigure 12: Modification to the netlist to include path coefficients and constants\nas an input.", "11. 9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect.", "While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM. Taking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.", "Figure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13. Using the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization.", "Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients.", "Because the approach explained in\nSection 2. 1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf. There are some differences between the sheaf and the measurement data.", "The contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1.", "the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ). The second of these is easier to interpret.", "We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30\n\nSeaIce\nSeaIce_lag\nSeaIce_lagvar\n\nColdPool_block\n\nColdPool_lagvar\n\nSeaIce_pc\n\nColdPool_lag\nColdPool\n\n2018_ColdPool\n\nColdPool_Copepods_pc\n\nColdPool_Krill_pc\nKrill_block\n\nCopepods_block\n\n2016_Krill\nCopepods\n\nKrill\nCopepods_lag\n\nCopepods_pc\nDietCopepods_block\n\nCopepods_lagvar\n\nKrill_lag\n\nKrill_pc\nDietKrill_block\n\nKrill_lagvar\n\nDietCopepods\n\nSpawners\n\nDietKrill\nDietCopepods_pc DietKrill_pc\n\nDietCopepods_lag\n\nDietKrill_lag\nSpawners_pc\n\nDietCopepods_lagvar\n\nSpawners_lag\n\nDietKrill_lagvar\nSpawners_lagvar\n\nSurvival_block\n\nSurvival\n\ncells\n\nrestrictions\nprojection map\nother function (see text)\n\ninferred variable (shown in Fig. 11)\nobserved variable highlighted in Fig. 11\npseudometric not present\npseudometric present\n\n0\n2\n4\nconsistency radius contribution\n\nFigure 13: The full sheaf for the DSEM described in Section 2.", "2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1.", "The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11.", "White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds.", "These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11. 31\n\nSource\n\nTarget\n\nSeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)\n\nDSEM [46]\nAR(1)\n0.", "6\n1. 79\n0. 18\n0.", "29\n0. 06\n0. 15\n0.", "13\n 0. 59\n11. 9\n2\n\nnone\n1.", "68\n4. 45\n0. 44\n0.", "32\n0. 52\n 0. 50\n7.", "56\n 0. 82\n6. 60\n2848\n\nSheaf\nAR(1) AR(2)\n1.", "81\n1. 78\n4. 38\n4.", "47\n0. 38\n0. 41\n0.", "35\n0. 36\n0. 70\n0.", "65\n 0. 12 0. 05\n5.", "29\n7. 19\n 0. 65 0.", "55\n9. 48\n9. 03\n2637\n2679\n\nAR(10)\n1.", "74\n4. 17\n0. 39\n0.", "34\n0. 56\n 0. 32\n5.", "63\n 0. 74\n7. 93\n2907\n\nTable 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model.", "This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11. We should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries. Notice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size.", "This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation. Table 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2. 2) and by the sheaf (using minimum consistency radius).", "Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations. ) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries.", "Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable. There is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves.", "Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown. Because the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case. Some caution in comparing consistency radius across the columns of Table\n\n32\n\nVariable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill\n\nAR(1)\nlag 1\n0.", "582\n0. 361\n0. 828\n0.", "692\n1. 01\n0. 886\n0.", "060\n\nAR(2)\nlag 1\nlag 2\n0. 480\n0. 202\n0.", "287\n0. 190\n1. 16\n-0.", "442\n0. 308\n0. 411\n1.", "78\n-0. 768\n1. 68\n-0.", "924\n0. 0596 0. 0445\n\nTable 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.", "3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9.", "Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller.", "Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data. Runtimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1. 4 GHz with 32 GB RAM.", "The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems.", "The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem. 5\n\nThe topology of subsystems\n\nClassically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system.", "This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves). We will establish that a dynamical system induces a cosheaf of invariant\nsets.", "The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33\n\nbase space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently. We will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6).", "Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems.", "This is not entirely compatible with all system decompositions in the literature, so caution is advised! (The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph.", "In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.", ")\n\n5. 1\n\nDynamical systems\n\nDefinition 15. A dynamical system is a continuous bijection f : S S.", "The\nset S in this case is called the set of states of the dynamical system. It is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.", "Definition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V. Corollary 7.", "If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V . Definition 17. Suppose that A B.", "The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i. Dually, a projection is a function p : B A such that p p = p and\np|A = id A .", "Proposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU\n\nf\n\ni\n\nV\n\nf\n\n/U\n\n/V\n\ni \n\ncommutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.", "34\n\nProof. Suppose that x U . Since U is an invariant set, f (U ) U .", "However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set. Definition 18.", "The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng\n\ng\n\nS2\n\nf2\n\n/ S2\n\nComposition of morphisms is given by composing the g functions. Proposition 9.", "Isomorphisms in Dyn are conjugacy classes of dynamical systems. 5. 2\n\nThe cosheaf endomorphism of invariant sets\n\nThe state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets.", "This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.", "We begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally. Lemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX.", "Consider the inclusion partial order on U, given by U V whenever U V . Define the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2.", "CU (U V ) = CU (U V ) : U V via the inclusion map. Then CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ). Proof.", "Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!", "! G\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V\n\nW V\n\nW V\n\nsince the equivalence identifies points that agree on overlaps. 35\n\nThe above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets.", "In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves. Definition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function.", "A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))\n\nmV\n\n/ S(V )\nS(U V )\n\nR(f 1 (U )) mU / S(U )\n\nDually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO\n\nmV\n\n/ S(V )\nO\n\nR(f 1 (U ) f 1 (V ))\n\nS(U V )\n\nR(f 1 (U ))\n\nmU\n\n/ S(U )\n\nWith the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial. Lemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .", "Proof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ). Suppose that f : (X, TX ) (Y, TY ) is a continuous map.", "This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.", "Therefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )\n\nFU :=f |U\n\nC(X,TX ) (f 1 (U ) f 1 (V ))\n\nC(X,TX ) (f 1 (V )) = f 1 (V )\n\n/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )\n\nFV :=f |V\n\n/ C(Y,T ) (V ) = V\nY\n\nwhich establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism. 36\n\nNow suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ).", "This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ). Suppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion.", "Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f . Proposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .", "Proof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U .", "If U V are two invariant sets, then Proposition 8\nimplies that\nU\n\nmU =f\n\n/U\n\ni\n\nV\n\nmV =f\n\n/V\n\ni\n\ncommutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism. Moreover, since S is itself an invariant set, the proof is complete.", "5. 3\n\nSubsystem decomposition sheaf\n\nRather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf.", "The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together. Dualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f .", "If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni\n\nU\n\np\n\ni\n\nf |U\n\nB\n\n/U\n\np\n\ng\n\n/B\n\nDually, the diagram at right above captures the situation where B is a subsystem\nof f . 37\n\nDefinition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p.", "We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f . We can think of the function g as a dynamical system in its own right.", "The idea of a subsystem is neatly compatible with the DSEM construction. As will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.", "Example 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone.", "As a result, there are two nested subsystems: {A} and {A B}. To see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . .", ". ). The dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep.", "Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . .", ",an , an 1 , . . .", ", . . .", ", bn , bn 1 , . . .", ")\n= (. . .", ", an+1 , an , . . .", ", . . .", ", β(an , an 1 , . . .", "), β(an 1 , an 2 , . . .", "), . . .", "). Because of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.", "The subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . .", ", an , an 1 , . . .", ", . . .", ", bn , bn 1 , . . .", ") = (. . .", ", an+1 , an , . . .", "). The subsystem dynamical map g : A A is simply\ng(. .", ". , an , an 1 , . .", ". ) = (. .", ". , an+1 , an , . .", ". ). Verification that (g, p) is a subsystem is then simply a calculation,\n(p f )(.", ". . , an , an 1 , .", ". . , .", ". . , bn , bn 1 , .", ". . )\n\n= p(.", ". . , an+1 , an , .", ". . , .", ". . , β(an , an 1 , .", ". . ), β(an 1 , an 2 , .", ". . ), .", ". . )\n= (.", ". . , an+1 , an , .", ". . )\n\n= g(.", ". . , an , an 1 , .", ". . )\n= (g p)(.", ". . , an , an 1 , .", ". . , .", ". . , bn , bn 1 , .", ". . ).", "38\n\nExample 3. ? B\nA\n\nC\n\nFollowing the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.", "Example 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA\n\n? C\n\nB\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem.", "Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B. When a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation. Example 5.", "Consider the DSEM on variables A and B given by the graph\n)\n\nAh\n\nB\n\n(See also Figure 6 for the sheaf model. ) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A. Linear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.", "Example 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system.", "Subsystems and invariant subspaces of f are\nin bijective correspondence. To see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv\n\n39\n\nfor some λ. Then the subspace spanned by v is an invariant set.", "Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues). Since V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem.", "To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0. Because f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))\n\n= prW (f (u) + f (w))\n\n= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)\n\n= (f |W ) (prW (u + w))\n= (f |W prW )(v).", "Lemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation. Proof.", "Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 .", "That is, we\nhave the commutative diagram\nf\n\nS\np2\n\np1 p2\n\nB2\n\np2\ng2\n\np1\n\nB1\n\n/S\n\n/ B2\n\np1 p2\n\np1\n\ng1\n\n/ B1\n\nso that (g1 , (p1 p2 )) is a subsystem of f . Intuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ).", "As a result, the state of g1 can influence the state of\ng2 . Example 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).", "40\n\nThis has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1\n\nZ\n\nf\n\n/ Z3\npr1\n\ng\n\n/Z\n\nIn this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system. It is not necessarily the case that subsystems are invariant sets. Example 8.", "Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2\n\nf\n\np\n\nB\n\n/ R2\np\n\nid\n\n/B\n\nwhere p(x, y) = (x, 0), even though the set B is not an invariant set.", "However, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system. Lemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B.", "If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f . Proof. The hypotheses posit a commutative diagram of the form\nS\n\nf\n\np\n\nB\n\n/S\np\n\ng\n\n/B\n\nSuppose that x p 1 (V ) S.", "We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .", "41\n\nLemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .", "Proof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\n\n/S\n\nf\n\nS\np\n\np\n\nB\n\n/B\n\ng\n\nThe conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .", "A related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\nf\n\nS\n\nB\n\ng\n\n/S\n\n/B\n\nTherefore, when studying a dynamical system, one will often encounter problems of the following form. Question 1.", "When do lifts to the dashed arrows in the diagram above exist? Answers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.", "Example 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X.", "For this example, f is not bijective. The way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X.", "The user does not have control over the entire state of the machine,\n42\n\nbut rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU\n\nf |U\n\nX\n\n/U\n\nf\n\n/X\n\nAs an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U .", "If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt. Evidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions.", "This ensures that f (U ) U is indeed an\ninvariant set. We might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2.", "For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2. The compilation process essentially ensures that we have the following commutative diagram\nU\n\nf |U\n\nq\n\nY\n\ng\n\n/U\n\n/Y\n\nq\n\nwhere the q maps select the two registers R1 and R2 from the entirety of the\nmachine state. Notice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.", "Since the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np\n\nY\n\ng\n\n43\n\n/Y\n\np\n\nValues of X for which the commutativity fails egregiously are instances of weird\nmachine states [13]. However, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states.", "We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO\n\nf |U\n\ni\n\nY\n\ng\n\n/U\n\n/Y\n\nf\n\nXO\nq\n\n i\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nFor instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,\n\nR1 = x,\nR2 = y,\n\nR[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO\n\nf |U\n\n/U\nO\n\ni\n\nY\n\ni\ng\n\n/Y\n\nInspired by Example 9, suppose that we have a commutative diagram\nXO\n\nf\n\ni\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nwhere i is injective, p is surjective, and f , g are bijective. This leads to another question that is often of interest when studying system\nbehaviors.", "Question 2. Under what conditions does\nX\n\nf\n\np\n\nY\n\ng\n\n44\n\n/X\n\n/Y\n\np\n\ncommute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f .", "It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X . Lemma 16. The subsystem preorder is a meet-semilattice.", "That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 . Proof.", "We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1\n\nf1\n\n/ S1\nO\n\np1\n\np1\n\nS\n\n/S\n\nf\n\np2\n\nS2\n\np2\n\nf2\n\n/ S2\n\nWe want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\n/ S3\n\nsuch that this diagram is a colimit. Since each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y.", "The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\nf1\n\n/ S3\n\nS1\nf3\n\nf2\n\nS2\n\n45\n\np \n3\n\np 3\n\n/ S3\n\nProposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.", "As a result, data feedback loops are confined to happen within a given subsystem. Proof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1\n\ng1\n\np2\n\nB2\n\np2\n\ng2\n\n/ B2\n\ng1\n\n/ B1\n\np1\n\nB1\n\n/ B1\n\np1\n\nSince p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection.", "Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.", "Example 10. There is no function h that will make the diagram below commute\nZ2\nid\n\nZO 2\n\nf\n\n/ Z2\nid\n\nh\n\n/ Z2\nO\n\nid\n\nid\n\nZ2\n\ng\n\n/ Z2\n\nwhere\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y). 46\n\nThere is also no function h that will make the diagram below commute\nZ2\npr1\n\nZO\n\nf\n\n/Z\nid\n\nh\n\n/Z\nO\n\npr2\n\nZ2\n\nid\ng\n\n/Z\n\nwhere\nf (x, y) = 1 x,\n\nand\n\ng(x, y) = y.", "Suppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B.", "For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ). Definition 21. Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .", "Even if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces. Lemma 18.", "For a dynamical system f : S S, the space of global sections of\nFf is precisely S. Proof. First of all, notice that id S : S S meets the criteria for a subsystem.", "We merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB. p:S B subsystem\n\np:S B subsystem\n\nSuppose that we have a global section s.", "On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B). 47\n\nTherefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem. Proposition 19.", "A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf. Proof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.", "In short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems. We are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation.", "As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality. The key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value.", "Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem. Definition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.", "Lemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem. Proof.", "Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.", "As a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea. Corollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.", "Corollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.", "48\n\nf\n\n k\npr1\n\n k\npr1\n\n k \n\n k \n\npr1\n\npr1\n\n( k ) \n\n( k ) \npr1\n\npr1\n\ng\n\n( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4\n\npr1\n( k ) \npr1\n\npr1\n( k ) \npr1\n\n( k ) \n( k ) \npr1,2,5,6\n\n k\n\npr1,2,3,4\n\npr7\n\n k k \n\n k\n\npr7\n\n k k \n\nFigure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.", "6\n\nSubsystems of the Bering Sea system\n\nFigure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13. The function f performs an AR (k) update:\n! k 1\nX\nai xk i ,\nf (x1 , .", ". . , xk ) = x2 , .", ". . , xk ,\ni=0\n\nwhile the function g performs the dynamical update for the subsystem containing the Krill variables:\n!", "k 1\nX\ng(x1 , . . .", ", xk , y, z) = x2 , . . .", ", xk ,\nai xk i , y + cxk , z + dy . i=0\n\nNotice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14. Although Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system.", "Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49\n\nDefinition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk. With this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries.", "The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut. 7\n\nConclusion\n\nIn this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.", "Sheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis. This chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1.", "the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).", "With these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e. g.", "forward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50\n\nhybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.", "It remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM. Indeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model.", "If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework. Acknowledgments\nThe linear regression example in Section 3. 3 is due to Donna Dietz.", "This article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "paragraph", "num_chunks": 835, "avg_chunk_len": 122.75928143712575, "std_chunk_len": 358.16424596058386, "max_chunk_len": 2784, "min_chunk_len": 1, "total_chars": 102504, "compression_ratio": 1.0167505658315774, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025", "Analyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025", "Abstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.", "Contents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "2\n3\n4\n5", "2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea", "5\n5\n7", "Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.", "1", "3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .", "8\n11\n14\n18\n25", "4 Sheaf encoding of the Bering Sea", "28", "5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35\n5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system", "49", "7 Conclusion", "50", "1", "Introduction", "Ecologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2", "involves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.", "1.1", "Related work", "The challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software", "3", "[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.", "1.2", "Contributions", "This chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).", "4", "1.3", "Chapter outline", "Section 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.", "2", "Dynamical modeling of ecosystems", "This section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter.", "2.1", "DSEM background and motivation", "Definition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5", "Linearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1", "(1)", "In what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1", "(2)", "We can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h", "J X\nT\nX\ni=1 j=1", "γk, ,i,j xi (τ tj ) + hϵk, (τ ).", "(3)", "If we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,", "(4)", "where the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6", "the xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT", "Q = (id P )V", "1", "(5)\n(id P),", "(6)", "where id is the identity matrix. The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via", "yk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,", "(7)", "where is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.", "2.2", "Ecological background and the DSEM system for the\nBering Sea", "To demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7", "by specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt", "(8)", "α", "where e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:", "Rt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.", "3", "Sheaf encodings of composite systems", "In this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8", "Table 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].\nName\nSeaIce", "Description\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]", "ColdPool", "Spatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]", "Spawners", "Female spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]", "Survival", "Age-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners", "Copepods", "Density of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)", "Krill", "Index of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]", "DietCopepods", "Biomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).", "DietKrill", "Same as DietCopepods, but for euphausiids (krill)\n9", "Table 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool", "Ecological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )", "Sign\n+", "ColdP ool Copepods", "Warmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]", "+", "ColdP ool Krill", "Water temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )", "?", "Copepods DietCopepods", "Increased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator", "+", "Krill DietKrill", "Same as Copepods DietCopepods\nbut for krill", "+", "DietCopepods Survival", "Increased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]", "+", "DietKrill Survival", "Same as DietCopepods Survival,\nbut for krill", "+", "Spawners Survival", "Increased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]", "10", "biomass\ndensitysurvival", "SeaIce", "out", "ColdPool", "f\n n", "ColdPool", "in", "Copepods_block\nout", "Krill_block\nout", "Copepods", "Krill", "Krill", "in", "Copepods_block", "Krill_block", "in", "DietCope_block\nDiet_Cop", "Diet_Krill", "Spawners", "out", "Diet_Cop\nSurvival", "in_copepods", "out", "Spawners", "out", "Diet_Cop\nin_copepods", "in_spawners", "Diet_Krill", "Spawners", "in_krill", "Survival_block", "in_spawners", "g2", "g1", "n", "id\n n\nh", "id\n n\nk\n n", "n\npr1", "pr2\nn", "n", "Survival", "m\n n", "(b)", "(c)", "(d)", "out", "n", "n", "Survival", "out", "(a)", "in", "DietKrill_block", "DietCope_block", "out", "Diet_Krill\nin_krill", "Survival_block", "Krill", "in", "in", "DietKrill_block", "n", "out", "Copepods", "id", "id", "in\nin", "out", "Copepods", "n", "ColdPool_block", "out", "ColdPool", "id", "in", "ColdPool_block\nSeaIce", "n", "SeaIce", "in", "n", "n\npr3", "Figure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.", "3.1", "Netlists", "The term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made. Each\nport is either an output, which means that it determines the value of the variable\n11", "Part 2 (capacitor)\nNet 1", "in", "out", "Net 2\nin", "out", "Part 1\n(Battery)", "Part 3 (resistor)\nin", "out", "Net 3", "Figure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12", "each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13", "somewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).", "3.2", "Sheaves and cosheaves", "Sheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborhoods. \n14", "Definition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U ) is the identity function.\n15", "Dually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U", "If U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,", "G\nC(X) = \nC(U ) / ,\nU open", "where is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16", "Intuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p", "cS (a) :=", "X", "X", "U T , V T :V U", "p", "(dV (a(V ), S(V U )a(U )))", ",", "(10)", "where p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV", "n\nX\nk=1", "!1/p\np", "|xk yk |", ",", "where αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.", "17", "Consistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T", "We will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T", "As the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.", "3.3", "The netlist sheaf", "The key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18", "Constraints", "x", "x", "m b x", "x", "m b", "pr1", "x", "n", "pr2", "pr3", "y = mx + b", "y = mx + b", "y", "y", "y", "y", "y", "n", "(a)", "(b)", "(c)", "(d)", "n", "f", "Assignment support", "Figure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n\npoints in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19", "the stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).", "(11)", "The function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form", "a := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =", "p", "p", "|m\ne m| + |eb b| +", "n\nX\nk=1", "p", "|f\nxk xk | +", "n\nX\nk=1", "!1/p\np", "|b + mf\nx k yk |", "(13)", "for a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20", "(14)", "where the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )", "|m\ne m|2 + |eb b|2 +", "n\nX\nk=1", "|f\nxk xk |2 +", "n\nX\nk=1", "!1/2\n|b + mf\nx k y k |2", "But since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1", "k=1", "This permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1", "Consistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21", ".", "y", "y1", "y = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius", "b + mx1\nconstrained\nconsistency\nradius", "x\nx1", "x~1", "Figure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,", "(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for completeness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22", "Constraints", "pr1", "...", "pr3", "pr2", "prn+2", "f1 f2", "n", "Assignment support", "fn", "...", "Figure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by", "a := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9).\n23", "X", "x", "x\nout", "f", "g", "g", "id", "X", "X", "id", "f", "in", "g", "f", "in", "out", "y", "y", "X", "(a)", "(b)", "(c)", "Figure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries.\n24", "x1, ... xn\nS\nin", "a1, ... ak", "coef", "LCF(k)", "pr2\nk", "S", "out", "yn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "(a)", "pr1", "k", "S\n(b)", "Figure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.", "3.4", "Sheaves modeling autoregressive timeseries", "Autoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25", "... x1, ... xn", "s", "out", "in", "identity", "LCF(k)", "coef", "a1, ... ak", "out", "in", "id", "pr2", "s", "k s", "id\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "s", "(a)", "(b)", "pr1", "k", "Figure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.\nWhen the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.", "26", "x1, ... xn\n n\nin", "in", "crop", "LCF(k)", "a1, ... ak", "coef", "out", "out", "id", "pr2", "n", "k n", "prk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "n-k", "(a)", "(b)", "pr1", "k", "Figure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.", "n", "in", "k n", "DietCope_lag", "Copepods\nin", "pr2", "out", "id", "crop", "n\nh", "DietCope_block\nout", "LCF(k)\nprk..n", "n - k", "n", "Diet_Cop\n(a)", "(b)", "Figure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.", "27", "Autoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.", "4", "Sheaf encoding of the Bering Sea", "We now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to Figure 1. The reader is directed to [36] for the software that\ngenerates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is", "28", "SeaIce", "ColdPool\nln (ColdPool ) [ln(km2 )]", "ln (SeaIce) [ln(km2 )]", "1\n0.5\n0.0\n 0.5\n 1.0", "0\n 1\n 2", "2018 ColdPool", "3", "DietCopepods\nln (DietCopepods) [ ]", "3", "ln (Copepods) [ln(count/m )]", "Copepods\n2.5\n0.0\n 2.5\n 5.0", "1\n0\n 1\n 2", "DietKrill", "3", "ln (Krill ) [ln(count/m )]", "Krill\nln (DietKrill ) [ ]", "0.5\n0.0\n 0.5", "2016 Krill", "1.0", "0.5\n0.0\n 0.5\n 1.0", "Spawners", "Survival\nln (Survival ) [103 count/kg]", "Spawners[106 kg]", "4\n3\n2\n1\n1960", "1970", "1980", "1990", "2000", "2010", "2020", "measurement", "2\n1\n0\n 1\n 2\n1960", "DSEM", "1970", "1980", "1990", "2000", "2010", "2020", "sheaf", "Figure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.", "29", "Copepods_pc", "Copepods", "n\npr2", "pc", "in", "DietCope_block", "pr1", "out", "Diet_Cop", "n\ng~1\n n", "(a)", "(b)", "Figure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30", "SeaIce\nSeaIce_lag\nSeaIce_lagvar", "ColdPool_block", "ColdPool_lagvar", "SeaIce_pc", "ColdPool_lag\nColdPool", "2018_ColdPool", "ColdPool_Copepods_pc", "ColdPool_Krill_pc\nKrill_block", "Copepods_block", "2016_Krill\nCopepods", "Krill\nCopepods_lag", "Copepods_pc\nDietCopepods_block", "Copepods_lagvar", "Krill_lag", "Krill_pc\nDietKrill_block", "Krill_lagvar", "DietCopepods", "Spawners", "DietKrill\nDietCopepods_pc DietKrill_pc", "DietCopepods_lag", "DietKrill_lag\nSpawners_pc", "DietCopepods_lagvar", "Spawners_lag", "DietKrill_lagvar\nSpawners_lagvar", "Survival_block", "Survival", "cells", "restrictions\nprojection map\nother function (see text)", "inferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present", "0\n2\n4\nconsistency radius contribution", "Figure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31", "Source", "Target", "SeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)", "DSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2", "none\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848", "Sheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03\n2637\n2679", "AR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907", "Table 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table", "32", "Variable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill", "AR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060", "AR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445", "Table 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.", "5", "The topology of subsystems", "Classically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33", "base space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)", "5.1", "Dynamical systems", "Definition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU", "f", "i", "V", "f", "/U", "/V", "i", "commutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34", "Proof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng", "g", "S2", "f2", "/ S2", "Composition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.", "5.2", "The cosheaf endomorphism of invariant sets", "The state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V", "W V", "W V", "since the equivalence identifies points that agree on overlaps.\n35", "The above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))", "mV", "/ S(V )\nS(U V )", "R(f 1 (U )) mU / S(U )", "Dually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO", "mV", "/ S(V )\nO", "R(f 1 (U ) f 1 (V ))", "S(U V )", "R(f 1 (U ))", "mU", "/ S(U )", "With the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )", "FU :=f |U", "C(X,TX ) (f 1 (U ) f 1 (V ))", "C(X,TX ) (f 1 (V )) = f 1 (V )", "/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )", "FV :=f |V", "/ C(Y,T ) (V ) = V\nY", "which establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36", "Now suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU", "mU =f", "/U", "i", "V", "mV =f", "/V", "i", "commutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.", "5.3", "Subsystem decomposition sheaf", "Rather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni", "U", "p", "i", "f |U", "B", "/U", "p", "g", "/B", "Dually, the diagram at right above captures the situation where B is a subsystem\nof f .", "37", "Definition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )", "= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )", "= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38", "Example 3.\n?B\nA", "C", "Following the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA", "?C", "B\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)", "Ah", "B", "(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv", "39", "for some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))", "= prW (f (u) + f (w))", "= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)", "= (f |W ) (prW (u + w))\n= (f |W prW )(v).", "Lemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf", "S\np2", "p1 p2", "B2", "p2\ng2", "p1", "B1", "/S", "/ B2", "p1 p2", "p1", "g1", "/ B1", "so that (g1 , (p1 p2 )) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).", "40", "This has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1", "Z", "f", "/ Z3\npr1", "g", "/Z", "In this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2", "f", "p", "B", "/ R2\np", "id", "/B", "where p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS", "f", "p", "B", "/S\np", "g", "/B", "Suppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .", "41", "Lemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )", "Y", "/Y", "i", "i", "/S", "f", "S\np", "p", "B", "/B", "g", "The conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )", "Y", "/Y", "i", "i\nf", "S", "B", "g", "/S", "/B", "Therefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42", "but rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU", "f |U", "X", "/U", "f", "/X", "As an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU", "f |U", "q", "Y", "g", "/U", "/Y", "q", "where the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np", "Y", "g", "43", "/Y", "p", "Values of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO", "f |U", "i", "Y", "g", "/U", "/Y", "f", "XO\nq", "i", "Y", "g", "/X", "/Y", "p", "For instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,", "R1 = x,\nR2 = y,", "R[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO", "f |U", "/U\nO", "i", "Y", "i\ng", "/Y", "Inspired by Example 9, suppose that we have a commutative diagram\nXO", "f", "i", "Y", "g", "/X", "/Y", "p", "where i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX", "f", "p", "Y", "g", "44", "/X", "/Y", "p", "commute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1", "f1", "/ S1\nO", "p1", "p1", "S", "/S", "f", "p2", "S2", "p2", "f2", "/ S2", "We want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "/ S3", "such that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "f1", "/ S3", "S1\nf3", "f2", "S2", "45", "p \n3", "p 3", "/ S3", "Proposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.\nAs a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1", "g1", "p2", "B2", "p2", "g2", "/ B2", "g1", "/ B1", "p1", "B1", "/ B1", "p1", "Since p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid", "ZO 2", "f", "/ Z2\nid", "h", "/ Z2\nO", "id", "id", "Z2", "g", "/ Z2", "where\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).", "46", "There is also no function h that will make the diagram below commute\nZ2\npr1", "ZO", "f", "/Z\nid", "h", "/Z\nO", "pr2", "Z2", "id\ng", "/Z", "where\nf (x, y) = 1 x,", "and", "g(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB.\np:S B subsystem", "p:S B subsystem", "Suppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47", "Therefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.", "Lemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48", "f", "k\npr1", "k\npr1", "k", "k", "pr1", "pr1", "( k )", "( k ) \npr1", "pr1", "g", "( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4", "pr1\n( k ) \npr1", "pr1\n( k ) \npr1", "( k ) \n( k ) \npr1,2,5,6", "k", "pr1,2,3,4", "pr7", "k k", "k", "pr7", "k k", "Figure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.", "6", "Subsystems of the Bering Sea system", "Figure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0", "while the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0", "Notice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49", "Definition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.", "7", "Conclusion", "In this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50", "hybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.", "Acknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "recursive", "num_chunks": 1050, "avg_chunk_len": 97.43619047619048, "std_chunk_len": 162.8183686395362, "max_chunk_len": 800, "min_chunk_len": 1, "total_chars": 102308, "compression_ratio": 1.0186984400046917, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025", "Analyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025", "Abstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.", "Contents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "2\n3\n4\n5", "2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea", "5\n5\n7", "Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.", "1", "3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .", "8\n11\n14\n18\n25", "4 Sheaf encoding of the Bering Sea", "28", "5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35\n5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system", "49", "7 Conclusion", "50", "1", "Introduction", "Ecologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.", "Understanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20). Subsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation.", "Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5). This chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5).", "Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries. The translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem.", "Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2", "involves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.", "1.1", "Related work", "The challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47].", "The key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].", "Although algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find.", "One challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].", "We argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught. Aspects of the formalism we introduce in this chapter are not entirely novel.", "For instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software", "3", "[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7). Roughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality).", "Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.", "Finally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22].", "In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.", "1.2", "Contributions", "This chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations an", "d the hypothesis.", "This chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3.", "the netlist sheaf with additional stalks for individual observations (Definition 14).", "4", "1.3", "Chapter outline", "Section 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models.", "As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear. Section 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization.", "Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well. In Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3.", "The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail. Section 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair.", "We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems. Section 6 revisits the ecological model from Section 2 once again.", "Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model. Finally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.", "2", "Dynamical modeling of ecosystems", "This section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter.", "2.1", "DSEM background and motivation", "Definition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5", "Linearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge. The absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .", "The variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).", "dτ\ni=1 j=1", "(1)", "In what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1", "(2)", "We can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h", "J X\nT\nX\ni=1 j=1", "γk, ,i,j xi (τ tj ) + hϵk, (τ ).", "(3)", "If we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,", "(4)", "where the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6", "the xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT", "Q = (id P )V", "1", "(5)\n(id P),", "(6)", "where id is the identity matrix. The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.", "Now we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .", ") Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via", "yk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,", "(7)", "where is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.", "2.2", "Ecological background and the DSEM system for the\nBering Sea", "To demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7", "by specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt", "(8)", "α", "where e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15].", "Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:", "Rt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses. The DSEM we translate into a sheaf was previously developed by Thorson\net al. [47].", "It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]).", "This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.", "3", "Sheaf encodings of composite systems", "In this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8", "Table 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].\nName\nSeaIce", "Description\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]", "ColdPool", "Spatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]", "Spawners", "Female spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]", "Survival", "Age-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners", "Copepods", "Density of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)", "Krill", "Index of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]", "DietCopepods", "Biomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).", "DietKrill", "Same as DietCopepods, but for euphausiids (krill)\n9", "Table 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool", "Ecological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )", "Sign\n+", "ColdP ool Copepods", "Warmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]", "+", "ColdP ool Krill", "Water temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )", "?", "Copepods DietCopepods", "Increased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator", "+", "Krill DietKrill", "Same as Copepods DietCopepods\nbut for krill", "+", "DietCopepods Survival", "Increased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]", "+", "DietKrill Survival", "Same as DietCopepods Survival,\nbut for krill", "+", "Spawners Survival", "Increased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]", "10", "biomass\ndensitysurvival", "SeaIce", "out", "ColdPool", "f\n n", "ColdPool", "in", "Copepods_block\nout", "Krill_block\nout", "Copepods", "Krill", "Krill", "in", "Copepods_block", "Krill_block", "in", "DietCope_block\nDiet_Cop", "Diet_Krill", "Spawners", "out", "Diet_Cop\nSurvival", "in_copepods", "out", "Spawners", "out", "Diet_Cop\nin_copepods", "in_spawners", "Diet_Krill", "Spawners", "in_krill", "Survival_block", "in_spawners", "g2", "g1", "n", "id\n n\nh", "id\n n\nk\n n", "n\npr1", "pr2\nn", "n", "Survival", "m\n n", "(b)", "(c)", "(d)", "out", "n", "n", "Survival", "out", "(a)", "in", "DietKrill_block", "DietCope_block", "out", "Diet_Krill\nin_krill", "Survival_block", "Krill", "in", "in", "DietKrill_block", "n", "out", "Copepods", "id", "id", "in\nin", "out", "Copepods", "n", "ColdPool_block", "out", "ColdPool", "id", "in", "ColdPool_block\nSeaIce", "n", "SeaIce", "in", "n", "n\npr3", "Figure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.", "1); in (c), they point from netlist parts to nets (Sec. 3. 1);\nand in (d), they denote restriction functions (Sec.", "3. 2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model.", "This remedied in Section 3. 4. Throughout this section, we refer to Figure 1 for intuition.", "Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3. 1, results in Figure 1(b).", "Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d).", "Section 3. 4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.", "3.1", "Netlists", "The term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).", "Each variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries.", "In Section\n3. 4, we show how to handle missing values in such a timeseries. Each part has a number of ports, to which connections can be made.", "Each\nport is either an output, which means that it determines the value of the variable\n11", "Part 2 (capacitor)\nNet 1", "in", "out", "Net 2\nin", "out", "Part 1\n(Battery)", "Part 3 (resistor)\nin", "out", "Net 3", "Figure 2: A netlist for an electric circuit, described in Example 1. of a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it. Each net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output.", "Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port. This formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs.", "Clearly each\nexternal output must attach to exactly one port, which must be an output port. Example 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor.", "These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3.", "Connecting the output port of the resistor to the input port of the battery. The values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net.", "The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs. A DSEM graph can be translated into a netlist via the following construction.", "Definition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12", "each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables. There are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph. Definition 3.", "The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net. The label on each vertex is simply the name of the part corresponding to\nthat vertex. The vertices within a hyperedge correspond to the parts connected to the\ncorresponding net.", "The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty. Because the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics.", "Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure.", "The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly. Figure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM.", "Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports. Proposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.", "Proof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist. Assume we have a set of variables for all nets that are consistent with the\ninput-output functions.", "As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13", "somewhere in the netlist. This means that Equation (4) is satisfied by construction. Assume that we have a solution to Equation (4).", "Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove. The wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation. Definition 4.", "The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows.", "The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net. Figure 1(c) shows the netlist graph for the Bering Sea example.", "Corollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.", "Proposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully.", "As we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).", "3.2", "Sheaves and cosheaves", "Sheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related. This nearness can be most easily formalized by using the netlist graph defined\nin the previous section.", "Since the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive.", "Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology. A topological space is a mathematical formalism that captures the notion of\n neighborhoods. 14", "Definition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T . The ordered pair (X, T ) is called a topological space.", "Often, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U. The following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R. Additionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7].", "For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below. Definition 6. Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation.", "The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }. The idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves). Definition 7.", "Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2.", "For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U ) is the identity function.", "15", "Dually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3.", "For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function. If for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.", "As Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take. Definition 8.", "Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ). The direct product is in general not the direct sum, since the topology\nmay be infinite!", "For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n! G\nC(U ) . U U", "If U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,", "G\nC(X) = \nC(U ) / ,\nU open", "where is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16", "Intuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf. The set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction.", "A similar axiom applies for cosheaves. Definition 10. Let P be a presheaf on the topological space (X, T ).", "We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U. Dually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U . For the time being, we will focus on sheaves.", "Cosheaves will reappear in\nSection 5. Given that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius.", "Definition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p", "cS (a) :=", "X", "X", "U T , V T :V U", "p", "(dV (a(V ), S(V U )a(U )))", ",", "(10)", "where p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV", "n\nX\nk=1", "!1/p\np", "|xk yk |", ",", "where αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.", "17", "Consistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T", "We will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T", "As the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.", "3.3", "The netlist sheaf", "The key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6). In order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples.", "The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf. This progression is summarized in Figure 3. Before delving into the details, let us consider the meaning of the arrows\nshown in Figure 3.", "The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.", "In the sheaf diagram the arrows are functions between the stalks. Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs.", "An output variable is determined by the\n18", "Constraints", "x", "x", "m b x", "x", "m b", "pr1", "x", "n", "pr2", "pr3", "y = mx + b", "y = mx + b", "y", "y", "y", "y", "y", "n", "(a)", "(b)", "(c)", "(d)", "n", "f", "Assignment support", "Figure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.", "data within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical!", "The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net. Beginning the example in earnest, suppose that (x1 , y1 ), .", ". . , (xn , yn ) are n\npoints in the plane R2 .", "As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).", "The netlist for the problem represents the same information as in the SEM. As shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x). The prediction process depends on the two parameters b and m, which can\nalso be considered as inputs.", "This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c). The sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information.", "The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19", "the stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).", "(11)", "The function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form", "a := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =", "p", "p", "|m\ne m| + |eb b| +", "n\nX\nk=1", "p", "|f\nxk xk | +", "n\nX\nk=1", "!1/p\np", "|b + mf\nx k yk |", "(13)", "for a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression. The problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13).", "Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d). To follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none.", "The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}. Explicitly, we start with a non-global assignment supported on U,\n( , , (x1 , .", ". . , xn ), (y1 , .", ". . , yn ), ) ,\n20", "(14)", "where the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )", "|m\ne m|2 + |eb b|2 +", "n\nX\nk=1", "|f\nxk xk |2 +", "n\nX\nk=1", "!1/2\n|b + mf\nx k y k |2", "But since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1", "k=1", "This permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1", "Consistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21", ".", "y", "y1", "y = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius", "b + mx1\nconstrained\nconsistency\nradius", "x\nx1", "x~1", "Figure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,", "(m, b, ((y1 b)/m, . . .", ", (yn b)/m, (y1 , . . .", ", yn ), (m, b, ((y1 b)/m, . . .", ", (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . .", ". , xn , (y1 , . .", ". , yn ), (0, b, (x1 , . .", ". , xn ))) . Noting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.", "Regression (model fitting) (Details above, included for completeness here. )\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . .", ". , xn ), (y1 , . .", ". , yn ), ) . Consistency radius minimization will infer the values for b and m.", "As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression. 22", "Constraints", "pr1", "...", "pr3", "pr2", "prn+2", "f1 f2", "n", "Assignment support", "fn", "...", "Figure 5: Modification to the sheaf in Figure 3(d) to allow for missing data. Hybrid versions of the above problems can also be addressed. Assignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn .", "If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk. Figure 5 shows the resulting construction. The fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , .", ". . , xn )) = mxk + b.", "The set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by", "a := m, b, x1 , . . .", ", xn , y1 , . . .", ", yn , m,\ne eb, x\nf1 , . . .", ", x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5).", "But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged. Feedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9).", "23", "X", "x", "x\nout", "f", "g", "g", "id", "X", "X", "id", "f", "in", "g", "f", "in", "out", "y", "y", "X", "(a)", "(b)", "(c)", "Figure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation. Consider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value. X = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.", "X = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2. X = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist. Feedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.", "4. With the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm. Definition 13.", "If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports.", "The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables. It is often useful to have individual observations on their own stalks, like we\ndid in Figure 5.", "The following modification to Definition 13 allows for missing\ndata in general. Definition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable.", "These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries. 24", "x1, ... xn\nS\nin", "a1, ... ak", "coef", "LCF(k)", "pr2\nk", "S", "out", "yn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "(a)", "pr1", "k", "S\n(b)", "Figure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.", "3.4", "Sheaves modeling autoregressive timeseries", "Autoregressive timeseries are sequences . . .", ", x0 , x1 , . . .", "that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . .", ", ak . We say that such a sequence is AR(k) autoregressive. Autoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.", "It is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , .", ". . ak are constants.", "We can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25", "... x1, ... xn", "s", "out", "in", "identity", "LCF(k)", "coef", "a1, ... ak", "out", "in", "id", "pr2", "s", "k s", "id\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "s", "(a)", "(b)", "pr1", "k", "Figure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries. netlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers. To handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .", "This is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).", "The associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a. Following a theme already present in the linear regression example, there is\nduplication of data in the sheaf model.", "Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts. Once again, if we consider an assignment supported on the two variables (with\nthe same values on each! ), minimizing consistency radius will infer the values\nof the a coefficients.", "Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x. When the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k.", "The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.", "26", "x1, ... xn\n n\nin", "in", "crop", "LCF(k)", "a1, ... ak", "coef", "out", "out", "id", "pr2", "n", "k n", "prk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "n-k", "(a)", "(b)", "pr1", "k", "Figure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.", "n", "in", "k n", "DietCope_lag", "Copepods\nin", "pr2", "out", "id", "crop", "n\nh", "DietCope_block\nout", "LCF(k)\nprk..n", "n - k", "n", "Diet_Cop\n(a)", "(b)", "Figure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.", "27", "Autoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself.", "As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.", "4", "Sheaf encoding of the Bering Sea", "We now return to the ecological DSEM example introduced in Section 2. 2, and\nrefer the reader to Figure 1. The reader is directed to [36] for the software that\ngenerates the sheaf results presented in this section.", "The DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d). The netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners.", "If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11. The DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time.", "This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values.", "Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there.", "In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable). As noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally.", "The consistency radius of the assignment after minimization is", "28", "SeaIce", "ColdPool\nln (ColdPool ) [ln(km2 )]", "ln (SeaIce) [ln(km2 )]", "1\n0.5\n0.0\n 0.5\n 1.0", "0\n 1\n 2", "2018 ColdPool", "3", "DietCopepods\nln (DietCopepods) [ ]", "3", "ln (Copepods) [ln(count/m )]", "Copepods\n2.5\n0.0\n 2.5\n 5.0", "1\n0\n 1\n 2", "DietKrill", "3", "ln (Krill ) [ln(count/m )]", "Krill\nln (DietKrill ) [ ]", "0.5\n0.0\n 0.5", "2016 Krill", "1.0", "0.5\n0.0\n 0.5\n 1.0", "Spawners", "Survival\nln (Survival ) [103 count/kg]", "Spawners[106 kg]", "4\n3\n2\n1\n1960", "1970", "1980", "1990", "2000", "2010", "2020", "measurement", "2\n1\n0\n 1\n 2\n1960", "DSEM", "1970", "1980", "1990", "2000", "2010", "2020", "sheaf", "Figure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.", "29", "Copepods_pc", "Copepods", "n\npr2", "pc", "in", "DietCope_block", "pr1", "out", "Diet_Cop", "n\ng~1\n n", "(a)", "(b)", "Figure 12: Modification to the netlist to include path coefficients and constants\nas an input. 11. 9.", "Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.", "Taking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated. Figure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.", "Using the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred.", "The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2. 1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.", "There are some differences between the sheaf and the measurement data. The contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries.", "This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).", "The second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30", "SeaIce\nSeaIce_lag\nSeaIce_lagvar", "ColdPool_block", "ColdPool_lagvar", "SeaIce_pc", "ColdPool_lag\nColdPool", "2018_ColdPool", "ColdPool_Copepods_pc", "ColdPool_Krill_pc\nKrill_block", "Copepods_block", "2016_Krill\nCopepods", "Krill\nCopepods_lag", "Copepods_pc\nDietCopepods_block", "Copepods_lagvar", "Krill_lag", "Krill_pc\nDietKrill_block", "Krill_lagvar", "DietCopepods", "Spawners", "DietKrill\nDietCopepods_pc DietKrill_pc", "DietCopepods_lag", "DietKrill_lag\nSpawners_pc", "DietCopepods_lagvar", "Spawners_lag", "DietKrill_lagvar\nSpawners_lagvar", "Survival_block", "Survival", "cells", "restrictions\nprojection map\nother function (see text)", "inferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present", "0\n2\n4\nconsistency radius contribution", "Figure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31", "Source", "Target", "SeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)", "DSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2", "none\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848", "Sheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03\n2637\n2679", "AR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907", "Table 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11. We should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.", "Notice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation. Table 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.", "2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.", ") The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable.", "There is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown. Because the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.", "Some caution in comparing consistency radius across the columns of Table", "32", "Variable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill", "AR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060", "AR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445", "Table 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels. 3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left).", "This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied.", "Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data. Runtimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.", "4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM.", "This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.", "5", "The topology of subsystems", "Classically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33", "base space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently. We will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6).", "Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems.", "This is not entirely compatible with all system decompositions in the literature, so caution is advised! (The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph.", "In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.", ")", "5.1", "Dynamical systems", "Definition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.", "It is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems. Definition 16.", "For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V. Corollary 7. If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V .", "Definition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A.", "Notice that (i|A) i = i. Dually, a projection is a function p : B A such that p p = p and\np|A = id A . Proposition 8.", "Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU", "f", "i", "V", "f", "/U", "/V", "i", "commutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34", "Proof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng", "g", "S2", "f2", "/ S2", "Composition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.", "5.2", "The cosheaf endomorphism of invariant sets", "The state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it.", "To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other. We begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally. Lemma 10.", "Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V . Define the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1.", "CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map. Then CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).", "Proof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V .", "The space of global cosections on V is\n! ! G\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V", "W V", "W V", "since the equivalence identifies points that agree on overlaps.\n35", "The above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))", "mV", "/ S(V )\nS(U V )", "R(f 1 (U )) mU / S(U )", "Dually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO", "mV", "/ S(V )\nO", "R(f 1 (U ) f 1 (V ))", "S(U V )", "R(f 1 (U ))", "mU", "/ S(U )", "With the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )", "FU :=f |U", "C(X,TX ) (f 1 (U ) f 1 (V ))", "C(X,TX ) (f 1 (V )) = f 1 (V )", "/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )", "FV :=f |V", "/ C(Y,T ) (V ) = V\nY", "which establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36", "Now suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).", "Suppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .", "Proposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f . Proof.", "Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU", "mU =f", "/U", "i", "V", "mV =f", "/V", "i", "commutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.", "5.3", "Subsystem decomposition sheaf", "Rather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni", "U", "p", "i", "f |U", "B", "/U", "p", "g", "/B", "Dually, the diagram at right above captures the situation where B is a subsystem\nof f .", "37", "Definition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection.", "When p is clear from\ncontext, we will often say g is a subsystem of f . We can think of the function g as a dynamical system in its own right. The idea of a subsystem is neatly compatible with the DSEM construction.", "As will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition. Example 2.", "Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.", "To see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . .", "). The dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (.", ". . ,an , an 1 , .", ". . , .", ". . , bn , bn 1 , .", ". . )\n= (.", ". . , an+1 , an , .", ". . , .", ". . , β(an , an 1 , .", ". . ), β(an 1 , an 2 , .", ". . ), .", ". . ).", "Because of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined. The subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(.", ". . , an , an 1 , .", ". . , .", ". . , bn , bn 1 , .", ". . ) = (.", ". . , an+1 , an , .", ". . ).", "The subsystem dynamical map g : A A is simply\ng(. . .", ", an , an 1 , . . .", ") = (. . .", ", an+1 , an , . . .", "). Verification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. .", ". , an , an 1 , . .", ". , . .", ". , bn , bn 1 , . .", ". )", "= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )", "= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38", "Example 3.\n?B\nA", "C", "Following the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA", "?C", "B\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)", "Ah", "B", "(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv", "39", "for some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))", "= prW (f (u) + f (w))", "= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)", "= (f |W ) (prW (u + w))\n= (f |W prW )(v).", "Lemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf", "S\np2", "p1 p2", "B2", "p2\ng2", "p1", "B1", "/S", "/ B2", "p1 p2", "p1", "g1", "/ B1", "so that (g1 , (p1 p2 )) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).", "40", "This has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1", "Z", "f", "/ Z3\npr1", "g", "/Z", "In this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2", "f", "p", "B", "/ R2\np", "id", "/B", "where p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS", "f", "p", "B", "/S\np", "g", "/B", "Suppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .", "41", "Lemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )", "Y", "/Y", "i", "i", "/S", "f", "S\np", "p", "B", "/B", "g", "The conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )", "Y", "/Y", "i", "i\nf", "S", "B", "g", "/S", "/B", "Therefore, when studying a dynamical system, one will often encounter problems of the following form. Question 1. When do lifts to the dashed arrows in the diagram above exist?", "Answers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows. Example 9.", "Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.", "The way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42", "but rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU", "f |U", "X", "/U", "f", "/X", "As an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt. Evidently, if PC = 1, then the program halts immediately.", "No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set. We might instead imagine that the executable specified by U was the result\nof a compiled, high-level program.", "Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2. The compilation process essentially ensures that we have the following commutative diagram\nU", "f |U", "q", "Y", "g", "/U", "/Y", "q", "where the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np", "Y", "g", "43", "/Y", "p", "Values of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO", "f |U", "i", "Y", "g", "/U", "/Y", "f", "XO\nq", "i", "Y", "g", "/X", "/Y", "p", "For instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,", "R1 = x,\nR2 = y,", "R[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO", "f |U", "/U\nO", "i", "Y", "i\ng", "/Y", "Inspired by Example 9, suppose that we have a commutative diagram\nXO", "f", "i", "Y", "g", "/X", "/Y", "p", "where i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX", "f", "p", "Y", "g", "44", "/X", "/Y", "p", "commute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1", "f1", "/ S1\nO", "p1", "p1", "S", "/S", "f", "p2", "S2", "p2", "f2", "/ S2", "We want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "/ S3", "such that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "f1", "/ S3", "S1\nf3", "f2", "S2", "45", "p \n3", "p 3", "/ S3", "Proposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.\nAs a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1", "g1", "p2", "B2", "p2", "g2", "/ B2", "g1", "/ B1", "p1", "B1", "/ B1", "p1", "Since p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid", "ZO 2", "f", "/ Z2\nid", "h", "/ Z2\nO", "id", "id", "Z2", "g", "/ Z2", "where\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).", "46", "There is also no function h that will make the diagram below commute\nZ2\npr1", "ZO", "f", "/Z\nid", "h", "/Z\nO", "pr2", "Z2", "id\ng", "/Z", "where\nf (x, y) = 1 x,", "and", "g(x, y) = y. Suppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17.", "Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ). Definition 21.", "Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 . Even if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.", "Lemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S. Proof.", "First of all, notice that id S : S S meets the criteria for a subsystem. We merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB.", "p:S B subsystem", "p:S B subsystem", "Suppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47", "Therefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem. Proposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.", "Proof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism. In short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x).", "One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems. We are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem.", "Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality. The key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem.", "Definition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.", "Lemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem. Proof.", "Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.", "As a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea. Corollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.", "Corollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.", "48", "f", "k\npr1", "k\npr1", "k", "k", "pr1", "pr1", "( k )", "( k ) \npr1", "pr1", "g", "( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4", "pr1\n( k ) \npr1", "pr1\n( k ) \npr1", "( k ) \n( k ) \npr1,2,5,6", "k", "pr1,2,3,4", "pr7", "k k", "k", "pr7", "k k", "Figure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.", "6", "Subsystems of the Bering Sea system", "Figure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0", "while the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0", "Notice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49", "Definition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.", "7", "Conclusion", "In this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software. Sheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations a", "nd the hypothesis.", "This chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3.", "the netlist sheaf with additional stalks for individual observations (Definition 14). With these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.", "g. forward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50", "hybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks. It remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf.", "In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM. Indeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.", "Acknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "semantic", "num_chunks": 23, "avg_chunk_len": 4489.478260869565, "std_chunk_len": 4651.886796910103, "max_chunk_len": 19049, "min_chunk_len": 179, "total_chars": 103258, "compression_ratio": 1.0093261539057505, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025\n\nAnalyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025", "Abstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.", "Contents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n3\n4\n5\n\n2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea\n\n5\n5\n7\n\n Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.\n\n1\n\n3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .\n\n8\n11\n14\n18\n25\n\n4 Sheaf encoding of the Bering Sea\n\n28\n\n5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35\n5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system\n\n49\n\n7 Conclusion\n\n50", "1\n\nIntroduction\n\nEcologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2\ninvolves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.", "1.1\n\nRelated work\n\nThe challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software\n\n3\n\n[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.", "1.2\n\nContributions\n\nThis chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\n\n4", "1.3\n\nChapter outline\n\nSection 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.", "2\n\nDynamical modeling of ecosystems\n\nThis section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter.", "2.1\n\nDSEM background and motivation\n\nDefinition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5\n\nLinearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n= \nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1\n\n(1)\n\nIn what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n= \ndτ\ni=1 j=1\n\n(2)\n\nWe can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h\n\nJ X\nT\nX\ni=1 j=1\n\nγk, ,i,j xi (τ tj ) + hϵk, (τ ).\n\n(3)\n\nIf we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,\n\n(4)\n\nwhere the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6\nthe xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT\n\nQ = (id P )V\n\n 1\n\n(5)\n(id P),\n\n(6)\n\nwhere id is the identity matrix. The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via\n\nyk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,\n\n(7)\n\nwhere is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.", "2.2\n\nEcological background and the DSEM system for the\nBering Sea\n\nTo demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7\n\nby specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt\n\n(8)\n\nα\nwhere e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:\n\nRt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.\n8\n\nTable 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].\nName\nSeaIce\n\nDescription\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]\n\nColdPool\n\nSpatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]\n\nSpawners\n\nFemale spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]\n\nSurvival\n\nAge-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners\n\nCopepods\n\nDensity of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)\n\nKrill\n\nIndex of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]\n\nDietCopepods\n\nBiomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).\n\nDietKrill\n\nSame as DietCopepods, but for euphausiids (krill)\n9\n\nTable 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in the DSEM case study. We also include a first-order autoregressive term for each variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in Section 2.2.\nPath\nSeaIce ColdP ool\n\nEcological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )\n\nSign\n+\n\nColdP ool Copepods\n\nWarmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]\n\n+\n\nColdP ool Krill\n\nWater temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )\n\n?\n\nCopepods DietCopepods\n\nIncreased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator\n\n+\n\nKrill DietKrill\n\nSame as Copepods DietCopepods\nbut for krill\n\n+\n\nDietCopepods Survival\n\nIncreased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]\n\n+\n\nDietKrill Survival\n\nSame as DietCopepods Survival,\nbut for krill\n\n+\n\nSpawners Survival\n\nIncreased\nspawning\nbiomass\n(Spawners) will cause a densitydependent decrease in survival\n(Survival ) [15]\n\n10", "3\n\nSheaf encodings of composite systems\n\nIn this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\nSeaIce\n\nout\n\nColdPool\n\nf\n n\n\nColdPool\n\nin\n\nCopepods_block\nout\n\nKrill_block\nout\n\nCopepods\n\nKrill\n\nKrill\n\nin\n\nCopepods_block\n\nKrill_block\n\nin\n\nDietCope_block\nDiet_Cop\n\nDiet_Krill\n\nSpawners\n\nout\n\nDiet_Cop\nSurvival\n\nin_copepods\n\nout\n\nSpawners\n\nout\n\nDiet_Cop\nin_copepods\n\nin_spawners\n\nDiet_Krill\n\nSpawners\n\nin_krill\n\nSurvival_block\n\nin_spawners\n\ng2\n\ng1\n\n n\n\nid\n n\nh\n\nid\n n\nk\n n\n\n n\npr1\n\npr2\nn\n\nn\n\nSurvival\n\nm\n n\n\n(b)\n\n(c)\n\n(d)\n\nout\n\n n\n\n n\n\nSurvival\n\nout\n\n(a)\n\nin\n\nDietKrill_block\n\nDietCope_block\n\nout\n\nDiet_Krill\nin_krill\n\nSurvival_block\n\nKrill\n\nin\n\nin\n\nDietKrill_block\n\n n\n\nout\n\nCopepods\n\nid\n\nid\n\nin\nin\n\nout\n\nCopepods\n\n n\n\nColdPool_block\n\nout\n\nColdPool\n\nid\n\nin\n\nColdPool_block\nSeaIce\n\n n\n\nSeaIce\n\nin\n\nn\n\nn\npr3\n\nFigure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.", "3.1\n\nNetlists\n\nThe term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made. Each\nport is either an output, which means that it determines the value of the variable\n11\n\nPart 2 (capacitor)\nNet 1\n\nin\n\nout\n\nNet 2\nin\n\nout\n\nPart 1\n(Battery)\n\nPart 3 (resistor)\nin\n\nout\n\nNet 3\n\nFigure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12\n\n each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13\nsomewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).", "3.2\n\nSheaves and cosheaves\n\nSheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborhoods. \n14\n\nDefinition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U ) is the identity function.\n15\nDually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U\n\nIf U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,\n\nG\nC(X) = \nC(U ) / ,\nU open\n\nwhere is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16\n\nIntuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p\n\ncS (a) := \n\nX\n\nX\n\nU T , V T :V U\n\np\n\n(dV (a(V ), S(V U )a(U ))) \n\n,\n\n(10)\n\nwhere p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV\n\nn\nX\nk=1\n\n!1/p\np\n\n|xk yk |\n\n,\n\nwhere αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.\n\n17\n\nConsistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T\n\nWe will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T\n\nAs the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.", "3.3\n\nThe netlist sheaf\n\nThe key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n\npoints in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19\nthe stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).\n\n(11)\n\nThe function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form\n\na := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n\n(12)\n\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =\n\np\n\np\n\n|m\ne m| + |eb b| +\n\nn\nX\nk=1\n\np\n\n|f\nxk xk | +\n\nn\nX\nk=1\n\n!1/p\np\n\n|b + mf\nx k yk |\n\n(13)\n\nfor a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20\n\n(14)\n\nwhere the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )\n\n|m\ne m|2 + |eb b|2 +\n\nn\nX\nk=1\n\n|f\nxk xk |2 +\n\nn\nX\nk=1\n\n!1/2\n|b + mf\nx k y k |2\n\nBut since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1\n\nk=1\n\nThis permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1\n\nConsistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21\n\n.\n\ny\n\ny1\n\ny = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius\n\nb + mx1\nconstrained\nconsistency\nradius\n\nx\nx1\n\nx~1\n\nFigure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,\n\n(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for completeness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22\n\nConstraints\n\npr1\n\n...\n\npr3\n\npr2\n\nprn+2\n\nf1 f2\n\nn\n\nAssignment support\n\nfn\n\n... \n\nFigure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by\n\na := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\n\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9).\n23\n\nX\n\nx\n\nx\nout\n\nf\n\ng\n\ng\n\nid\n\nX\n\nX\n\nid\n\nf\n\nin\n\ng\n\nf\n\nin\n\nout\n\ny\n\ny\n\nX\n\n(a)\n\n(b)\n\n(c)\n\nFigure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries.\n24\n\nx1, ... xn\nS\n\nin\n\na1, ... ak\n\ncoef\n\nLCF(k)\n\npr2\nk\n\n S\n\nout\n\nyn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n(a)\n\npr1\n\n k\n\nS\n(b)\n\nFigure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.", "3.4\n\nSheaves modeling autoregressive timeseries\n\nAutoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25\n\n... x1, ... xn\n\ns\n\nout\n\nin\n\nidentity\n\nLCF(k)\n\ncoef\n\na1, ... ak\n\nout\n\nin\n\nid\n\npr2\n\ns\n\n k s\n\nid\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\ns\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.\nWhen the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.\n\n26\n\n n\n\nin\n\n k n\n\nDietCope_lag\n\nCopepods\nin\n\npr2\n\nout\n\nid\n\ncrop\n\n n\nh\n\nDietCope_block\nout\n\nLCF(k)\nprk..n\n\n n - k\n\n n\n\nDiet_Cop\n(a)\n\n(b)\n\nFigure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.\n\nAutoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.", "4\n\nSheaf encoding of the Bering Sea\n\nWe now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to [36] for the software that generates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is\n\n28\n\nSeaIce\n\nColdPool\nln (ColdPool ) [ln(km2 )]\n\nln (SeaIce) [ln(km2 )]\n\n1\n0.5\n0.0\n 0.5\n 1.0\n\n0\n 1\n 2\n\n2018 ColdPool\n\n 3\n\nDietCopepods\nln (DietCopepods) [ ]\n\n3\n\nln (Copepods) [ln(count/m )]\n\nCopepods\n2.5\n0.0\n 2.5\n 5.0\n\n1\n0\n 1\n 2\n\nDietKrill\n\n3\n\nln (Krill ) [ln(count/m )]\n\nKrill\nln (DietKrill ) [ ]\n\n0.5\n0.0\n 0.5\n\n2016 Krill\n\n 1.0\n\n0.5\n0.0\n 0.5\n 1.0\n\nSpawners\n\nSurvival\nln (Survival ) [103 count/kg]\n\nSpawners[106 kg]\n\n4\n3\n2\n1\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nmeasurement\n\n2\n1\n0\n 1\n 2\n1960\n\nDSEM\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nsheaf\n\nFigure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.\n\n29\n\nCopepods_pc\n\nCopepods\n\n n\npr2\n\npc\n\nin\n\nDietCope_block\n\npr1\n\nout\n\nDiet_Cop\n\n n\ng~1\n n\n\n(a)\n\n(b)\n\nFigure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30\n\nSeaIce\nSeaIce_lag\nSeaIce_lagvar\n\nColdPool_block\n\nColdPool_lagvar\n\nSeaIce_pc\n\nColdPool_lag\nColdPool\n\n2018_ColdPool\n\nColdPool_Copepods_pc\n\nColdPool_Krill_pc\nKrill_block\n\nCopepods_block\n\n2016_Krill\nCopepods\n\nKrill\nCopepods_lag\n\nCopepods_pc\nDietCopepods_block\n\nCopepods_lagvar\n\nKrill_lag\n\nKrill_pc\nDietKrill_block\n\nKrill_lagvar\n\nDietCopepods\n\nSpawners\n\nDietKrill\nDietCopepods_pc DietKrill_pc\n\nDietCopepods_lag\n\nDietKrill_lag\nSpawners_pc\n\nDietCopepods_lagvar\n\nSpawners_lag\n\nDietKrill_lagvar\nSpawners_lagvar\n\nSurvival_block\n\nSurvival\n\ncells\n\nrestrictions\nprojection map\nother function (see text)\n\ninferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present\n\n0\n2\n4\nconsistency radius contribution\n\nFigure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31\n\nSource\n\nTarget\n\nSeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)\n\nDSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2\n\nnone\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848\n\nSheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03\n2637\n2679\n\nAR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907\n\nTable 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table\n\n32\n\nVariable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill\n\nAR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060\n\nAR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445\n\nTable 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.", "5\n\nThe topology of subsystems\n\nClassically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33\n\nbase space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)", "5.1\n\nDynamical systems\n\nDefinition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU\n\nf\n\ni\n\nV\n\nf\n\n/U\n\n/V\n\ni \n\ncommutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34\n\nProof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng\n\ng\n\nS2\n\nf2\n\n/ S2\n\nComposition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.", "5.2\n\nThe cosheaf endomorphism of invariant sets\n\nThe state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V\n\nW V\n\nW V\n\nsince the equivalence identifies points that agree on overlaps.\n35\n\nThe above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))\n\nmV\n\n/ S(V )\nS(U V )\n\nR(f 1 (U )) mU / S(U )\n\nDually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO\n\nmV\n\n/ S(V )\nO\n\nR(f 1 (U ) f 1 (V ))\n\nS(U V )\n\nR(f 1 (U ))\n\nmU\n\n/ S(U )\n\nWith the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )\n\nFU :=f |U\n\nC(X,TX ) (f 1 (U ) f 1 (V ))\n\nC(X,TX ) (f 1 (V )) = f 1 (V )\n\n/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )\n\nFV :=f |V\n\n/ C(Y,T ) (V ) = V\nY\n\nwhich establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36\n\nNow suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU\n\nmU =f\n\n/U\n\ni\n\nV\n\nmV =f\n\n/V\n\ni\n\ncommutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.", "5.3\n\nSubsystem decomposition sheaf\n\nRather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni\n\nU\n\np\n\ni\n\nf |U\n\nB\n\n/U\n\np\n\ng\n\n/B\n\nDually, the diagram at right above captures the situation where B is a subsystem\nof f .\n\n37\n\nDefinition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n\n= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )\n\n= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38\n\nExample 3.\n?B\nA\n\nC\n\nFollowing the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA\n\n?C\n\nB\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)\n\nAh\n\nB\n\n(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv\n\n39\n\nfor some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))\n\n= prW (f (u) + f (w))\n\n= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)\n\n= (f |W ) (prW (u + w))\n= (f |W prW )(v).\n\nLemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf\n\nS\n\np2\n\np1 p2\n\nB2\n\np2\ng2\n\np1\n\nB1\n\n/S\n\n/ B2\n\np1 p2\n\np1\n\ng1\n\n/ B1\n\nso that (g1 , (p1 p2 )) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).\n\n40\n\nThis has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1\n\nZ\n\nf\n\n/ Z3\npr1\n\ng\n\n/Z\n\nIn this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2\n\nf\n\np\n\nB\n\n/ R2\n\np\n\nid\n\n/B\n\nwhere p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS\n\nf\n\np\n\nB\n\n/S\n\np\n\ng\n\n/B\n\nSuppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .\n\n41\n\nLemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\n\n/S\n\nf\n\nS\n\np\n\np\n\nB\n\n/B\n\ng\n\nThe conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\nf\n\nS\n\nB\n\ng\n\n/S\n\n/B\n\nTherefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42\n\nbut rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU\n\nf |U\n\nX\n\n/U\n\nf\n\n/X\n\nAs an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU\n\nf |U\n\nq\n\nY\n\ng\n\n/U\n\n/Y\n\nq\n\nwhere the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np\n\nY\n\ng\n\n43\n\n/Y\n\np\n\nValues of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO\n\nf |U\n\ni\n\nY\n\ng\n\n/U\n\n/Y\n\nf\n\nXO\nq\n\n i\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nFor instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,\n\nR1 = x,\nR2 = y,\n\nR[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO\n\nf |U\n\n/U\nO\n\ni\n\nY\n\ni\ng\n\n/Y\n\nInspired by Example 9, suppose that we have a commutative diagram\nXO\n\nf\n\ni\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nwhere i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX\n\nf\n\np\n\nY\n\ng\n\n44\n\n/X\n\n/Y\n\np\n\ncommute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1\n\nf1\n\n/ S1\nO\n\np1\n\np1\n\nS\n\n/S\n\nf\n\np2\n\nS2\n\np2\n\nf2\n\n/ S2\n\nWe want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\n/ S3\n\nsuch that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\nf1\n\n/ S3\n\nS1\nf3\n\nf2\n\nS2\n\n45\n\np \n3\n\np 3\n\n/ S3\n\nProposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.\nAs a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1\n\ng1\n\np2\n\nB2\n\np2\ng2\n\n/ B2\n\ng1\n\n/ B1\n\np1\n\nB1\n\n/ B1\n\np1\n\nSince p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid\n\nZO 2\n\nf\n\n/ Z2\nid\n\nh\n\n/ Z2\nO\n\nid\n\nid\n\nZ2\n\ng\n\n/ Z2\n\nwhere\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).\n\n46\n\nThere is also no function h that will make the diagram below commute\nZ2\npr1\n\nZO\n\nf\n\n/Z\nid\n\nh\n\n/Z\nO\n\npr2\n\nZ2\n\nid\ng\n\n/Z\n\nwhere\nf (x, y) = 1 x,\n\nand\n\ng(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB.\np:S B subsystem\n\np:S B subsystem\n\nSuppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47\n\nTherefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.\n\nLemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48", "6\n\nSubsystems of the Bering Sea system\n\nFigure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0\n\nwhile the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0\n\nNotice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49\n\nDefinition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.\nf\n\n k\npr1\n\n k\npr1\n\n k \n\n k \n\npr1\n\npr1\n\n( k ) \n\n( k ) \npr1\n\npr1\n\ng\n\n( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4\n\npr1\n( k ) \npr1\n\npr1\n( k ) \npr1\n\n( k ) \n( k ) \npr1,2,5,6\n\n k\n\npr1,2,3,4\n\npr7\n\n k k \n\n k\n\npr7\n\n k k \n\nFigure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.", "7\n\nConclusion\n\nIn this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50\nhybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.", "Acknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nDevelopment (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "delimiter", "num_chunks": 835, "avg_chunk_len": 122.75928143712575, "std_chunk_len": 358.16424596058386, "max_chunk_len": 2784, "min_chunk_len": 1, "total_chars": 102504, "compression_ratio": 1.0167505658315774, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025", "Analyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025", "Abstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.", "Contents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "2\n3\n4\n5", "2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea", "5\n5\n7", "Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.", "1", "3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .", "8\n11\n14\n18\n25", "4 Sheaf encoding of the Bering Sea", "28", "5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35\n5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system", "49", "7 Conclusion", "50", "1", "Introduction", "Ecologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2", "involves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.", "1.1", "Related work", "The challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software", "3", "[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.", "1.2", "Contributions", "This chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).", "4", "1.3", "Chapter outline", "Section 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.", "2", "Dynamical modeling of ecosystems", "This section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter.", "2.1", "DSEM background and motivation", "Definition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5", "Linearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1", "(1)", "In what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1", "(2)", "We can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h", "J X\nT\nX\ni=1 j=1", "γk, ,i,j xi (τ tj ) + hϵk, (τ ).", "(3)", "If we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,", "(4)", "where the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6", "the xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT", "Q = (id P )V", "1", "(5)\n(id P),", "(6)", "where id is the identity matrix. The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via", "yk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,", "(7)", "where is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.", "2.2", "Ecological background and the DSEM system for the\nBering Sea", "To demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7", "by specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt", "(8)", "α", "where e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:", "Rt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.", "3", "Sheaf encodings of composite systems", "In this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8", "Table 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].\nName\nSeaIce", "Description\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]", "ColdPool", "Spatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]", "Spawners", "Female spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]", "Survival", "Age-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners", "Copepods", "Density of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)", "Krill", "Index of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]", "DietCopepods", "Biomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).", "DietKrill", "Same as DietCopepods, but for euphausiids (krill)\n9", "Table 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool", "Ecological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )", "Sign\n+", "ColdP ool Copepods", "Warmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]", "+", "ColdP ool Krill", "Water temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )", "?", "Copepods DietCopepods", "Increased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator", "+", "Krill DietKrill", "Same as Copepods DietCopepods\nbut for krill", "+", "DietCopepods Survival", "Increased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]", "+", "DietKrill Survival", "Same as DietCopepods Survival,\nbut for krill", "+", "Spawners Survival", "Increased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]", "10", "biomass\ndensitysurvival", "SeaIce", "out", "ColdPool", "f\n n", "ColdPool", "in", "Copepods_block\nout", "Krill_block\nout", "Copepods", "Krill", "Krill", "in", "Copepods_block", "Krill_block", "in", "DietCope_block\nDiet_Cop", "Diet_Krill", "Spawners", "out", "Diet_Cop\nSurvival", "in_copepods", "out", "Spawners", "out", "Diet_Cop\nin_copepods", "in_spawners", "Diet_Krill", "Spawners", "in_krill", "Survival_block", "in_spawners", "g2", "g1", "n", "id\n n\nh", "id\n n\nk\n n", "n\npr1", "pr2\nn", "n", "Survival", "m\n n", "(b)", "(c)", "(d)", "out", "n", "n", "Survival", "out", "(a)", "in", "DietKrill_block", "DietCope_block", "out", "Diet_Krill\nin_krill", "Survival_block", "Krill", "in", "in", "DietKrill_block", "n", "out", "Copepods", "id", "id", "in\nin", "out", "Copepods", "n", "ColdPool_block", "out", "ColdPool", "id", "in", "ColdPool_block\nSeaIce", "n", "SeaIce", "in", "n", "n\npr3", "Figure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.", "3.1", "Netlists", "The term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made. Each\nport is either an output, which means that it determines the value of the variable\n11", "Part 2 (capacitor)\nNet 1", "in", "out", "Net 2\nin", "out", "Part 1\n(Battery)", "Part 3 (resistor)\nin", "out", "Net 3", "Figure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12", "each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13", "somewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).", "3.2", "Sheaves and cosheaves", "Sheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborhoods. \n14", "Definition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U ) is the identity function.\n15", "Dually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U", "If U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,", "G\nC(X) = \nC(U ) / ,\nU open", "where is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16", "Intuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p", "cS (a) :=", "X", "X", "U T , V T :V U", "p", "(dV (a(V ), S(V U )a(U )))", ",", "(10)", "where p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV", "n\nX\nk=1", "!1/p\np", "|xk yk |", ",", "where αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.", "17", "Consistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T", "We will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T", "As the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.", "3.3", "The netlist sheaf", "The key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18", "Constraints", "x", "x", "m b x", "x", "m b", "pr1", "x", "n", "pr2", "pr3", "y = mx + b", "y = mx + b", "y", "y", "y", "y", "y", "n", "(a)", "(b)", "(c)", "(d)", "n", "f", "Assignment support", "Figure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n\npoints in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19", "the stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).", "(11)", "The function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form", "a := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =", "p", "p", "|m\ne m| + |eb b| +", "n\nX\nk=1", "p", "|f\nxk xk | +", "n\nX\nk=1", "!1/p\np", "|b + mf\nx k yk |", "(13)", "for a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20", "(14)", "where the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )", "|m\ne m|2 + |eb b|2 +", "n\nX\nk=1", "|f\nxk xk |2 +", "n\nX\nk=1", "!1/2\n|b + mf\nx k y k |2", "But since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1", "k=1", "This permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1", "Consistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21", ".", "y", "y1", "y = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius", "b + mx1\nconstrained\nconsistency\nradius", "x\nx1", "x~1", "Figure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,", "(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for completeness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22", "Constraints", "pr1", "...", "pr3", "pr2", "prn+2", "f1 f2", "n", "Assignment support", "fn", "...", "Figure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by", "a := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9).\n23", "X", "x", "x\nout", "f", "g", "g", "id", "X", "X", "id", "f", "in", "g", "f", "in", "out", "y", "y", "X", "(a)", "(b)", "(c)", "Figure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries.\n24", "x1, ... xn\nS\nin", "a1, ... ak", "coef", "LCF(k)", "pr2\nk", "S", "out", "yn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "(a)", "pr1", "k", "S\n(b)", "Figure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.", "3.4", "Sheaves modeling autoregressive timeseries", "Autoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25", "... x1, ... xn", "s", "out", "in", "identity", "LCF(k)", "coef", "a1, ... ak", "out", "in", "id", "pr2", "s", "k s", "id\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "s", "(a)", "(b)", "pr1", "k", "Figure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.\nWhen the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.", "26", "x1, ... xn\n n\nin", "in", "crop", "LCF(k)", "a1, ... ak", "coef", "out", "out", "id", "pr2", "n", "k n", "prk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "n-k", "(a)", "(b)", "pr1", "k", "Figure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.", "n", "in", "k n", "DietCope_lag", "Copepods\nin", "pr2", "out", "id", "crop", "n\nh", "DietCope_block\nout", "LCF(k)\nprk..n", "n - k", "n", "Diet_Cop\n(a)", "(b)", "Figure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.", "27", "Autoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.", "4", "Sheaf encoding of the Bering Sea", "We now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to Figure 1. The reader is directed to [36] for the software that\ngenerates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is", "28", "SeaIce", "ColdPool\nln (ColdPool ) [ln(km2 )]", "ln (SeaIce) [ln(km2 )]", "1\n0.5\n0.0\n 0.5\n 1.0", "0\n 1\n 2", "2018 ColdPool", "3", "DietCopepods\nln (DietCopepods) [ ]", "3", "ln (Copepods) [ln(count/m )]", "Copepods\n2.5\n0.0\n 2.5\n 5.0", "1\n0\n 1\n 2", "DietKrill", "3", "ln (Krill ) [ln(count/m )]", "Krill\nln (DietKrill ) [ ]", "0.5\n0.0\n 0.5", "2016 Krill", "1.0", "0.5\n0.0\n 0.5\n 1.0", "Spawners", "Survival\nln (Survival ) [103 count/kg]", "Spawners[106 kg]", "4\n3\n2\n1\n1960", "1970", "1980", "1990", "2000", "2010", "2020", "measurement", "2\n1\n0\n 1\n 2\n1960", "DSEM", "1970", "1980", "1990", "2000", "2010", "2020", "sheaf", "Figure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.", "29", "Copepods_pc", "Copepods", "n\npr2", "pc", "in", "DietCope_block", "pr1", "out", "Diet_Cop", "n\ng~1\n n", "(a)", "(b)", "Figure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30", "SeaIce\nSeaIce_lag\nSeaIce_lagvar", "ColdPool_block", "ColdPool_lagvar", "SeaIce_pc", "ColdPool_lag\nColdPool", "2018_ColdPool", "ColdPool_Copepods_pc", "ColdPool_Krill_pc\nKrill_block", "Copepods_block", "2016_Krill\nCopepods", "Krill\nCopepods_lag", "Copepods_pc\nDietCopepods_block", "Copepods_lagvar", "Krill_lag", "Krill_pc\nDietKrill_block", "Krill_lagvar", "DietCopepods", "Spawners", "DietKrill\nDietCopepods_pc DietKrill_pc", "DietCopepods_lag", "DietKrill_lag\nSpawners_pc", "DietCopepods_lagvar", "Spawners_lag", "DietKrill_lagvar\nSpawners_lagvar", "Survival_block", "Survival", "cells", "restrictions\nprojection map\nother function (see text)", "inferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present", "0\n2\n4\nconsistency radius contribution", "Figure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31", "Source", "Target", "SeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)", "DSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2", "none\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848", "Sheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03\n2637\n2679", "AR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907", "Table 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table", "32", "Variable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill", "AR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060", "AR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445", "Table 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.", "5", "The topology of subsystems", "Classically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33", "base space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)", "5.1", "Dynamical systems", "Definition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU", "f", "i", "V", "f", "/U", "/V", "i", "commutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34", "Proof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng", "g", "S2", "f2", "/ S2", "Composition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.", "5.2", "The cosheaf endomorphism of invariant sets", "The state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V", "W V", "W V", "since the equivalence identifies points that agree on overlaps.\n35", "The above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))", "mV", "/ S(V )\nS(U V )", "R(f 1 (U )) mU / S(U )", "Dually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO", "mV", "/ S(V )\nO", "R(f 1 (U ) f 1 (V ))", "S(U V )", "R(f 1 (U ))", "mU", "/ S(U )", "With the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )", "FU :=f |U", "C(X,TX ) (f 1 (U ) f 1 (V ))", "C(X,TX ) (f 1 (V )) = f 1 (V )", "/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )", "FV :=f |V", "/ C(Y,T ) (V ) = V\nY", "which establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36", "Now suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU", "mU =f", "/U", "i", "V", "mV =f", "/V", "i", "commutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.", "5.3", "Subsystem decomposition sheaf", "Rather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni", "U", "p", "i", "f |U", "B", "/U", "p", "g", "/B", "Dually, the diagram at right above captures the situation where B is a subsystem\nof f .", "37", "Definition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )", "= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )", "= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38", "Example 3.\n?B\nA", "C", "Following the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA", "?C", "B\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)", "Ah", "B", "(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv", "39", "for some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))", "= prW (f (u) + f (w))", "= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)", "= (f |W ) (prW (u + w))\n= (f |W prW )(v).", "Lemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf", "S\np2", "p1 p2", "B2", "p2\ng2", "p1", "B1", "/S", "/ B2", "p1 p2", "p1", "g1", "/ B1", "so that (g1 , (p1 p2 )) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).", "40", "This has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1", "Z", "f", "/ Z3\npr1", "g", "/Z", "In this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2", "f", "p", "B", "/ R2\np", "id", "/B", "where p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS", "f", "p", "B", "/S\np", "g", "/B", "Suppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .", "41", "Lemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )", "Y", "/Y", "i", "i", "/S", "f", "S\np", "p", "B", "/B", "g", "The conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )", "Y", "/Y", "i", "i\nf", "S", "B", "g", "/S", "/B", "Therefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42", "but rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU", "f |U", "X", "/U", "f", "/X", "As an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU", "f |U", "q", "Y", "g", "/U", "/Y", "q", "where the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np", "Y", "g", "43", "/Y", "p", "Values of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO", "f |U", "i", "Y", "g", "/U", "/Y", "f", "XO\nq", "i", "Y", "g", "/X", "/Y", "p", "For instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,", "R1 = x,\nR2 = y,", "R[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO", "f |U", "/U\nO", "i", "Y", "i\ng", "/Y", "Inspired by Example 9, suppose that we have a commutative diagram\nXO", "f", "i", "Y", "g", "/X", "/Y", "p", "where i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX", "f", "p", "Y", "g", "44", "/X", "/Y", "p", "commute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1", "f1", "/ S1\nO", "p1", "p1", "S", "/S", "f", "p2", "S2", "p2", "f2", "/ S2", "We want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "/ S3", "such that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "f1", "/ S3", "S1\nf3", "f2", "S2", "45", "p \n3", "p 3", "/ S3", "Proposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.\nAs a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1", "g1", "p2", "B2", "p2", "g2", "/ B2", "g1", "/ B1", "p1", "B1", "/ B1", "p1", "Since p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid", "ZO 2", "f", "/ Z2\nid", "h", "/ Z2\nO", "id", "id", "Z2", "g", "/ Z2", "where\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).", "46", "There is also no function h that will make the diagram below commute\nZ2\npr1", "ZO", "f", "/Z\nid", "h", "/Z\nO", "pr2", "Z2", "id\ng", "/Z", "where\nf (x, y) = 1 x,", "and", "g(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB.\np:S B subsystem", "p:S B subsystem", "Suppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47", "Therefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.", "Lemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48", "f", "k\npr1", "k\npr1", "k", "k", "pr1", "pr1", "( k )", "( k ) \npr1", "pr1", "g", "( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4", "pr1\n( k ) \npr1", "pr1\n( k ) \npr1", "( k ) \n( k ) \npr1,2,5,6", "k", "pr1,2,3,4", "pr7", "k k", "k", "pr7", "k k", "Figure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.", "6", "Subsystems of the Bering Sea system", "Figure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0", "while the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0", "Notice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49", "Definition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.", "7", "Conclusion", "In this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50", "hybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.", "Acknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "token_limit", "num_chunks": 131, "avg_chunk_len": 795.1068702290077, "std_chunk_len": 50.44386978217948, "max_chunk_len": 800, "min_chunk_len": 220, "total_chars": 104159, "compression_ratio": 1.0005952438099444, "avg_chunk_tokens": 198.4885496183206, "max_chunk_tokens": 200, "min_chunk_tokens": 55, "tokenizer": "", "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025\n\nAnalyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025\n\nAbstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can", "be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.\n\nContents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n3\n4\n5\n\n2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea\n\n5\n5\n7\n\n Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for ide", "ntification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.\n\n1\n\n3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .\n\n8\n11\n14\n18\n25\n\n4 Sheaf encoding of the Bering Sea\n\n28\n\n5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35", "5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system\n\n49\n\n7 Conclusion\n\n50\n\n1\n\nIntroduction\n\nEcologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking,", "a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sh", "eaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2\n\ninvolves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system para", "meters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.\n\n1.1\n\nRelated work\n\nThe challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time", "series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Ev", "en though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint", ", unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software\n\n3\n\n[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between inv", "ariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.\n\n1.2\n\nContributions\n\nThis chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studyi", "ng the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Def", "inition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\n\n4\n\n1.3\n\nChapter outline\n\nSection 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs", "sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof i", "nvariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.\n\n2\n\nDynamical modeling of ecosystems\n\nThis section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then", "presents a representative\necosystem model that will be revisited several times in the chapter.\n\n2.1\n\nDSEM background and motivation\n\nDefinition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5\n\nLinearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be i", "nterpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1\n\n(1)\n\nIn what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a varia", "ble xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1\n\n(2)\n\nWe can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h\n\nJ X\nT\nX\ni=1 j=1\n\nγk, ,i,j xi (τ tj ) + hϵk, (τ ).\n\n(3)\n\nIf we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equati", "on (3) can be compactly written in matrix form as\nX PX + E,\n\n(4)\n\nwhere the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6\n\nthe xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT\n\nQ = (id P )V\n\n 1\n\n(5)\n(id P),\n\n(6)\n\nwhere id is the identity matrix. The path coefficient matrix P can", "be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nt", "he distribution of yk,j through a link function gj , via\n\nyk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,\n\n(7)\n\nwhere is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.\n\n2.2\n\nEcological background and the DSEM system for the\nBering Sea\n\nTo demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-wint", "er survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7\n\nby specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt\n\n(8)\n\nα\n\nwhere e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of", "fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:\n\nRt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabun", "dance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.\n\n3\n\nSheaf encodings of composite systems\n\nIn this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist,", "and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8\n\nTable 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries", "of\nthe variables are taken from [47].\nName\nSeaIce\n\nDescription\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]\n\nColdPool\n\nSpatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]\n\nSpawners\n\nFemale spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]\n\nSurvival\n\nAge-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessmen", "t model [20]\ndivided by Spawners\n\nCopepods\n\nDensity of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)\n\nKrill\n\nIndex of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]\n\nDietCopepods\n\nBiomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for sub", "sampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).\n\nDietKrill\n\nSame as DietCopepods, but for euphausiids (krill)\n9\n\nTable 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool\n\nEcological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )\n\nSign\n+\n\nColdP", "ool Copepods\n\nWarmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]\n\n+\n\nColdP ool Krill\n\nWater temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )\n\n?\n\nCopepods DietCopepods\n\nIncreased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator\n\n+\n\nKrill DietKrill\n\nSame as Copepods DietCopepods\nbut for krill\n\n+\n\nDietCopepods Survival\n\nIncreased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survi", "val ) [19]\n\n+\n\nDietKrill Survival\n\nSame as DietCopepods Survival,\nbut for krill\n\n+\n\nSpawners Survival\n\nIncreased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]\n\n10\n\nbiomass\ndensitysurvival\n\nSeaIce\n\nout\n\nColdPool\n\nf\n n\n\nColdPool\n\nin\n\nCopepods_block\nout\n\nKrill_block\nout\n\nCopepods\n\nKrill\n\nKrill\n\nin\n\nCopepods_block\n\nKrill_block\n\nin\n\nDietCope_block\nDiet_Cop\n\nDiet_Krill\n\nSpawners\n\nout\n\nDiet_Cop\nSurvival\n\nin_copepods\n\nout\n\nSpawners\n\nout\n\nDiet_Cop\nin_copepods\n\nin_spawners\n\nDiet_Krill\n\nSpawners\n\nin_krill\n\nSurvival_block\n\nin_spawners\n\ng2\n\ng1\n\n n\n\nid\n n\nh\n\nid\n n\nk\n n\n\n n\npr1\n\npr2\nn\n\nn\n\nSurvival\n\nm\n n\n\n(b)\n\n(c)\n\n(d)\n\nout\n\n n\n\n n\n\nSurvival\n\nout\n\n(a)\n\nin\n\nDietKrill_block\n\nDietCope_block\n\nout\n\nDiet_Krill\nin_krill\n\nSurvival_block\n\nKrill\n\nin\n\nin\n\nDietKrill_block\n\n n\n\nout\n\nCopepods", "id\n\nid\n\nin\nin\n\nout\n\nCopepods\n\n n\n\nColdPool_block\n\nout\n\nColdPool\n\nid\n\nin\n\nColdPool_block\nSeaIce\n\n n\n\nSeaIce\n\nin\n\nn\n\n n\npr3\n\nFigure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a", ") shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.\n\n3.1\n\nNetlists\n\nThe term netlist appears to have entered the technical lexicon in the early\ndays of", "computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made.", "Each\nport is either an output, which means that it determines the value of the variable\n11\n\nPart 2 (capacitor)\nNet 1\n\nin\n\nout\n\nNet 2\nin\n\nout\n\nPart 1\n(Battery)\n\nPart 3 (resistor)\nin\n\nout\n\nNet 3\n\nFigure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output", "variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets sp", "ecify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12\n\n each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the sam", "e name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. Th", "e partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergrap", "h for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was const", "ructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13\n\nsomewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply th", "e name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist", "graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).\n\n3.2\n\nSheaves and cosheaves\n\nSheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally", "induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborhoods. \n14\n\nDefinition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological", "space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P,", ") is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U", ") is the identity function.\n15\n\nDually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to th", "e points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U\n\nIf U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S o", "n a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,\n\nG\nC(X) = \nC(U ) / ,\nU open\n\nwhere is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16\n\nIntuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional", "gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the", "stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p\n\ncS (a) := \n\nX\n\nX\n\nU T , V T :V U\n\np\n\n(dV (a(V ), S(V U )a(U ))) \n\n,\n\n(10)\n\nwhere p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV\n\nn\nX\nk=1\n\n!1/p\np\n\n|xk yk |\n\n,\n\nwhere αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if d", "ifferent\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.\n\n17\n\nConsistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an as", "signment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T\n\nWe will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T\n\nAs the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.\n\n3.3\n\nThe netlist sheaf\n\nThe key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (T", "heorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalk", "s represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18\n\nConstraints\n\nx\n\nx\n\nm b x\n\nx\n\nm b\n\npr1\n\nx\n\n n\n\npr2\n\npr3\n\ny = mx + b\n\ny = mx + b\n\ny\n\ny\n\ny\n\ny\n\ny\n\n n\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nn\n\nf\n\nAssignment support\n\nFigure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the", "remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n", "points in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) a", "nd the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19\n\nthe stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the", "prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).\n\n(11)\n\nThe function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form\n\na := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =\n\np\n\np\n\n|m\ne m| + |eb", "b| +\n\nn\nX\nk=1\n\np\n\n|f\nxk xk | +\n\nn\nX\nk=1\n\n!1/p\np\n\n|b + mf\nx k yk |\n\n(13)\n\nfor a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shade", "d\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20\n\n(14)\n\nwhere the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )\n\n|m\ne m|2 + |eb b|2 +\n\nn\nX\nk=1\n\n|f\nxk xk |2 +\n\nn\nX\nk=1\n\n!1/2\n|b + mf\nx k y k", "|2\n\nBut since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1\n\nk=1\n\nThis permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimiza", "tion problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1\n\nConsistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21\n\n.\n\ny\n\ny1\n\ny = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius\n\nb + mx1\nconstrained\nconsistency\nradius\n\nx\nx1\n\nx~1\n\nFigure 4: Geometric meanings of the terms contribut", "ing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,\n\n(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for c", "ompleteness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22\n\nConstraints\n\npr1\n\n...\n\npr3\n\npr2\n\nprn+2\n\nf1 f2\n\nn\n\nAssignment support\n\nfn\n\n... \n\nFigure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf con", "struction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by\n\na := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by ex", "actly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible", ",\nthe resulting sheaf will or will not have global sections (Definition 9).\n23\n\nX\n\nx\n\nx\nout\n\nf\n\ng\n\ng\n\nid\n\nX\n\nX\n\nid\n\nf\n\nin\n\ng\n\nf\n\nin\n\nout\n\ny\n\ny\n\nX\n\n(a)\n\n(b)\n\n(c)\n\nFigure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model auto", "regressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The", "following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries.\n24\n\nx1, ... xn\nS\nin\n\na1, ... ak\n\ncoef\n\nLCF(k)\n\npr2\nk\n\n S\n\nout\n\nyn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n(a)\n\npr1\n\n k\n\nS\n(b)\n\nFigure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsol", "utions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.\n\n3.4\n\nSheaves modeling autoregressive timeseries\n\nAutoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1", ", . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure", "7(a). Using Definition 13, we obtain the\n25\n\n... x1, ... xn\n\ns\n\nout\n\nin\n\nidentity\n\nLCF(k)\n\ncoef\n\na1, ... ak\n\nout\n\nin\n\nid\n\npr2\n\ns\n\n k s\n\nid\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\ns\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms", "are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.", "When the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.\n\n26\n\nx1, ... xn\n n\nin\n\nin\n\ncrop\n\nLCF(k)\n\na1, ... ak\n\ncoef\n\nout", "out\n\nid\n\npr2\n\n n\n\n k n\n\nprk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n n-k\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\n\n n\n\nin\n\n k n\n\nDietCope_lag\n\nCopepods\nin\n\npr2\n\nout\n\nid\n\ncrop\n\n n\nh\n\nDietCope_block\nout\n\nLCF(k)\nprk..n\n\n n - k\n\n n\n\nDiet_Cop\n(a)\n\n(b)\n\nFigure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.\n\n27\n\nAutoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specificall", "y, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.\n\n4\n\nSheaf encoding of the Bering Sea\n\nWe now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to Figure 1. The reader is directed to [36", "] for the software that\ngenerates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the res", "ulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\ns", "olution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is\n\n28\n\nSeaIce\n\nColdPool\nln (ColdPool ) [ln(km2 )]\n\nln (SeaIce) [ln(km2 )]\n\n1\n0.5\n0.0\n 0.5\n 1.0\n\n0\n 1\n 2\n\n2018 ColdPool\n\n 3\n\nDietCopepods\nln (DietCopepods) [ ]\n\n3\n\nln (Copepods) [ln(count/m )]\n\nCopepods\n2.5\n0.0\n 2.5\n 5.0\n\n1\n0\n 1\n 2\n\nDietKrill\n\n3\n\nln (Krill ) [ln(count/m )]\n\nKrill\nln (DietKrill ) [ ]\n\n0", ".5\n0.0\n 0.5\n\n2016 Krill\n\n 1.0\n\n0.5\n0.0\n 0.5\n 1.0\n\nSpawners\n\nSurvival\nln (Survival ) [103 count/kg]\n\nSpawners[106 kg]\n\n4\n3\n2\n1\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nmeasurement\n\n2\n1\n0\n 1\n 2\n1960\n\nDSEM\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nsheaf\n\nFigure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.\n\n29\n\nCopepods_pc\n\nCopepods\n\n n\npr2\n\npc\n\nin\n\nDietCope_block\n\npr1\n\nout\n\nDiet_Cop\n\n n\ng~1\n n\n\n(a)\n\n(b)\n\nFigure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the", "DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that", "correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency", "is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30\n\nSeaIce\nSeaIce_lag\nSeaIce_lagvar\n\nColdPool_block\n\nColdPool_lagvar\n\nSeaIce_pc\n\nColdPool_lag\nColdPool\n\n2018_ColdPool\n\nColdPool_Copepods_pc\n\nColdPool_Krill_pc\nKrill_block\n\nCopepods_block\n\n2016_Krill\nCopepods\n\nK", "rill\nCopepods_lag\n\nCopepods_pc\nDietCopepods_block\n\nCopepods_lagvar\n\nKrill_lag\n\nKrill_pc\nDietKrill_block\n\nKrill_lagvar\n\nDietCopepods\n\nSpawners\n\nDietKrill\nDietCopepods_pc DietKrill_pc\n\nDietCopepods_lag\n\nDietKrill_lag\nSpawners_pc\n\nDietCopepods_lagvar\n\nSpawners_lag\n\nDietKrill_lagvar\nSpawners_lagvar\n\nSurvival_block\n\nSurvival\n\ncells\n\nrestrictions\nprojection map\nother function (see text)\n\ninferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present\n\n0\n2\n4\nconsistency radius contribution\n\nFigure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that a", "re also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31\n\nSource\n\nTarget\n\nSeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)\n\nDSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2\n\nnone\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848\n\nSheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03", "2637\n2679\n\nAR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907\n\nTable 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by", "an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregre", "ssive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table\n\n32\n\nVariable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods", "DietKrill\n\nAR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060\n\nAR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445\n\nTable 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were appli", "ed. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linea", "r, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.\n\n5\n\nThe topology of subsystems\n\nClassically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n3", "3\n\nbase space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as", "a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)\n\n5.1\n\nDynamical systems\n\nDefinition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f :", "S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU\n\nf\n\ni\n\nV\n\nf\n\n/U\n\n/V\n\ni \n\ncommutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34\n\nProof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative dia", "gram of the\nform\nf1\n/ S1\nS1\ng\n\ng\n\nS2\n\nf2\n\n/ S2\n\nComposition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.\n\n5.2\n\nThe cosheaf endomorphism of invariant sets\n\nThe state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance", "of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V\n\nW", "V\n\nW V\n\nsince the equivalence identifies points that agree on overlaps.\n35\n\nThe above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))\n\nmV\n\n/ S(V )\nS(U V )\n\nR(f 1 (U )) mU / S(U )\n\nDually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphi", "sm m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO\n\nmV\n\n/ S(V )\nO\n\nR(f 1 (U ) f 1 (V ))\n\nS(U V )\n\nR(f 1 (U ))\n\nmU\n\n/ S(U )\n\nWith the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U", ") f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )\n\nFU :=f |U\n\nC(X,TX ) (f 1 (U ) f 1 (V ))\n\nC(X,TX ) (f 1 (V )) = f 1 (V )\n\n/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )\n\nFV :=f |V\n\n/ C(Y,T ) (V ) = V\nY\n\nwhich establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36\n\nNow suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subs", "ets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU\n\nmU =f\n\n/U\n\ni\n\nV\n\nmV =f\n\n/V\n\ni\n\ncommutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.\n\n5.3\n\nSubsystem decomposition sheaf\n\nRather than carving up the state space into", "different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni\n\nU\n\np\n\ni\n\nf |U\n\nB\n\n/U\n\np\n\ng\n\n/B\n\nDually, the diagram at right above captures the situation where B is a subsystem\nof f .\n\n37\n\nDefinition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g :", "B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explici", "tly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subs", "ystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n\n= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )\n\n= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38\n\nExample 3.\n?B\nA\n\nC\n\nFollowing the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B,", "and C given by\nthe graph\nA\n\n?C\n\nB\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)\n\nAh\n\nB\n\n(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next", "example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv\n\n39\n\nfor some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomp", "osition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))\n\n= prW (f (u) + f (w))\n\n= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)\n\n= (f |W ) (prW (u + w))\n= (f |W prW )(v).\n\nLemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf\n\nS\np2\n\np1 p2\n\nB2\n\np2\ng2\n\np1\n\nB1\n\n/S\n\n/ B2\n\np1 p2\n\np1\n\ng1\n\n/ B1\n\nso that (g1 , (p1 p2", ")) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).\n\n40\n\nThis has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1\n\nZ\n\nf\n\n/ Z3\npr1\n\ng\n\n/Z\n\nIn this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R", "2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2\n\nf\n\np\n\nB\n\n/ R2\np\n\nid\n\n/B\n\nwhere p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS\n\nf\n\np\n\nB\n\n/S\np\n\ng\n\n/B\n\nSuppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means", "that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .\n\n41\n\nLemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\n\n/S\n\nf\n\nS\np\n\np\n\nB\n\n/B\n\ng\n\nThe conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this c", "onsists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\nf\n\nS\n\nB\n\ng\n\n/S\n\n/B\n\nTherefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user", "loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42\n\nbut rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU\n\nf |U\n\nX\n\n/U\n\nf\n\n/X\n\nAs an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program", "will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU\n\nf |U\n\nq\n\nY\n\ng\n\n/U", "/Y\n\nq\n\nwhere the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np\n\nY\n\ng\n\n43\n\n/Y\n\np\n\nValues of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutat", "ive diagrams\nUO\n\nf |U\n\ni\n\nY\n\ng\n\n/U\n\n/Y\n\nf\n\nXO\nq\n\n i\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nFor instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,\n\nR1 = x,\nR2 = y,\n\nR[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO\n\nf |U\n\n/U\nO\n\ni\n\nY\n\ni\ng\n\n/Y\n\nInspired by Example 9, suppose that we have a commutative diagram\nXO\n\nf\n\ni\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nwhere i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX\n\nf\n\np\n\nY\n\ng\n\n44\n\n/X\n\n/Y\n\np\n\ncommute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case", "that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1\n\nf1\n\n/ S1\nO\n\np1\n\np1\n\nS\n\n/S\n\nf\n\np2\n\nS2\n\np2\n\nf2\n\n/ S2\n\nWe want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition", "for the dashed arrows in\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\n/ S3\n\nsuch that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\nf1\n\n/ S3\n\nS1\nf3\n\nf2\n\nS2\n\n45\n\np \n3\n\np 3\n\n/ S3\n\nProposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.", "As a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1\n\ng1\n\np2\n\nB2\n\np2\n\ng2\n\n/ B2\n\ng1\n\n/ B1\n\np1\n\nB1\n\n/ B1\n\np1\n\nSince p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid\n\nZO 2\n\nf\n\n/ Z2\nid\n\nh\n\n/ Z2\nO\n\ni", "d\n\nid\n\nZ2\n\ng\n\n/ Z2\n\nwhere\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).\n\n46\n\nThere is also no function h that will make the diagram below commute\nZ2\npr1\n\nZO\n\nf\n\n/Z\nid\n\nh\n\n/Z\nO\n\npr2\n\nZ2\n\nid\ng\n\n/Z\n\nwhere\nf (x, y) = 1 x,\n\nand\n\ng(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f acco", "rding to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf", "(p) =\nB.\np:S B subsystem\n\np:S B subsystem\n\nSuppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47\n\nTherefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S v", "ia self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of inte", "rest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.\n\nLemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynam", "ical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48\n\nf\n\n k\npr1\n\n k\npr1\n\n k \n\n k \n\npr1\n\npr1\n\n( k ) \n\n( k ) \npr1\n\npr1\n\ng\n\n( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4\n\npr1\n( k ) \npr1\n\npr1\n( k ) \npr1\n\n( k ) \n( k ) \npr1,2,5,6\n\n k\n\npr1,2,3,4\n\npr7\n\n k k \n\n k\n\npr7\n\n k k \n\nFigure 14: Sheaf of subsystems", "for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.\n\n6\n\nSubsystems of the Bering Sea system\n\nFigure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0\n\nwhile the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0\n\nNotice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with mod", "ifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49\n\nDefinition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those res", "triction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.\n\n7\n\nConclusion\n\nIn this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling a", "re that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system model", "er can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50\n\nhybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding she", "af. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.\n\nAcknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Indepen", "dent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "format_aware", "num_chunks": 1, "avg_chunk_len": 104220.0, "std_chunk_len": 0.0, "max_chunk_len": 104220, "min_chunk_len": 104220, "total_chars": 104220, "compression_ratio": 1.0000095950873154, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025\n\nAnalyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025\n\nAbstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.\n\nContents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n3\n4\n5\n\n2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea\n\n5\n5\n7\n\n Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.\n\n1\n\n3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .\n\n8\n11\n14\n18\n25\n\n4 Sheaf encoding of the Bering Sea\n\n28\n\n5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35\n5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system\n\n49\n\n7 Conclusion\n\n50\n\n1\n\nIntroduction\n\nEcologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.\nUnderstanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20).\nSubsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation. Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5).\nThis chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5). Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries.\nThe translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem. Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2\n\ninvolves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.\n\n1.1\n\nRelated work\n\nThe challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47].\nThe key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].\nAlthough algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find. \nOne challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].\nWe argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught.\nAspects of the formalism we introduce in this chapter are not entirely novel.\nFor instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software\n\n3\n\n[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7).\nRoughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality). Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.\nFinally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22]. In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.\n\n1.2\n\nContributions\n\nThis chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\n\n4\n\n1.3\n\nChapter outline\n\nSection 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models. As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear.\nSection 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization. Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well.\nIn Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3. The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail.\nSection 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair. We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems.\nSection 6 revisits the ecological model from Section 2 once again. Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model.\nFinally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.\n\n2\n\nDynamical modeling of ecosystems\n\nThis section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter.\n\n2.1\n\nDSEM background and motivation\n\nDefinition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5\n\nLinearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge.\nThe absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .\nThe variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).\ndτ\ni=1 j=1\n\n(1)\n\nIn what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1\n\n(2)\n\nWe can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h\n\nJ X\nT\nX\ni=1 j=1\n\nγk, ,i,j xi (τ tj ) + hϵk, (τ ).\n\n(3)\n\nIf we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,\n\n(4)\n\nwhere the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6\n\nthe xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT\n\nQ = (id P )V\n\n 1\n\n(5)\n(id P),\n\n(6)\n\nwhere id is the identity matrix. The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.\nNow we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .) Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via\n\nyk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,\n\n(7)\n\nwhere is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.\n\n2.2\n\nEcological background and the DSEM system for the\nBering Sea\n\nTo demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7\n\nby specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt\n\n(8)\n\nα\n\nwhere e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15]. Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:\n\nRt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses.\nThe DSEM we translate into a sheaf was previously developed by Thorson\net al. [47]. It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]). This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.\n\n3\n\nSheaf encodings of composite systems\n\nIn this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8\n\nTable 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].\nName\nSeaIce\n\nDescription\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]\n\nColdPool\n\nSpatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]\n\nSpawners\n\nFemale spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]\n\nSurvival\n\nAge-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners\n\nCopepods\n\nDensity of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)\n\nKrill\n\nIndex of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]\n\nDietCopepods\n\nBiomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).\n\nDietKrill\n\nSame as DietCopepods, but for euphausiids (krill)\n9\n\nTable 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool\n\nEcological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )\n\nSign\n+\n\nColdP ool Copepods\n\nWarmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]\n\n+\n\nColdP ool Krill\n\nWater temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )\n\n?\n\nCopepods DietCopepods\n\nIncreased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator\n\n+\n\nKrill DietKrill\n\nSame as Copepods DietCopepods\nbut for krill\n\n+\n\nDietCopepods Survival\n\nIncreased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]\n\n+\n\nDietKrill Survival\n\nSame as DietCopepods Survival,\nbut for krill\n\n+\n\nSpawners Survival\n\nIncreased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]\n\n10\n\nbiomass\ndensitysurvival\n\nSeaIce\n\nout\n\nColdPool\n\nf\n n\n\nColdPool\n\nin\n\nCopepods_block\nout\n\nKrill_block\nout\n\nCopepods\n\nKrill\n\nKrill\n\nin\n\nCopepods_block\n\nKrill_block\n\nin\n\nDietCope_block\nDiet_Cop\n\nDiet_Krill\n\nSpawners\n\nout\n\nDiet_Cop\nSurvival\n\nin_copepods\n\nout\n\nSpawners\n\nout\n\nDiet_Cop\nin_copepods\n\nin_spawners\n\nDiet_Krill\n\nSpawners\n\nin_krill\n\nSurvival_block\n\nin_spawners\n\ng2\n\ng1\n\n n\n\nid\n n\nh\n\nid\n n\nk\n n\n\n n\npr1\n\npr2\nn\n\nn\n\nSurvival\n\nm\n n\n\n(b)\n\n(c)\n\n(d)\n\nout\n\n n\n\n n\n\nSurvival\n\nout\n\n(a)\n\nin\n\nDietKrill_block\n\nDietCope_block\n\nout\n\nDiet_Krill\nin_krill\n\nSurvival_block\n\nKrill\n\nin\n\nin\n\nDietKrill_block\n\n n\n\nout\n\nCopepods\n\nid\n\nid\n\nin\nin\n\nout\n\nCopepods\n\n n\n\nColdPool_block\n\nout\n\nColdPool\n\nid\n\nin\n\nColdPool_block\nSeaIce\n\n n\n\nSeaIce\n\nin\n\nn\n\n n\npr3\n\nFigure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.1); in (c), they point from netlist parts to nets (Sec. 3.1);\nand in (d), they denote restriction functions (Sec. 3.2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model. This remedied in Section 3.4.\nThroughout this section, we refer to Figure 1 for intuition. Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3.1, results in Figure 1(b). Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d). Section 3.4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.\n\n3.1\n\nNetlists\n\nThe term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).\nEach variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries. In Section\n3.4, we show how to handle missing values in such a timeseries.\nEach part has a number of ports, to which connections can be made. Each\nport is either an output, which means that it determines the value of the variable\n11\n\nPart 2 (capacitor)\nNet 1\n\nin\n\nout\n\nNet 2\nin\n\nout\n\nPart 1\n(Battery)\n\nPart 3 (resistor)\nin\n\nout\n\nNet 3\n\nFigure 2: A netlist for an electric circuit, described in Example 1.\nof a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it.\nEach net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output. Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port.\nThis formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs. Clearly each\nexternal output must attach to exactly one port, which must be an output port.\nExample 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor. These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3. Connecting the output port of the resistor to the input port of the battery.\nThe values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net. The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs.\nA DSEM graph can be translated into a netlist via the following construction.\nDefinition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12\n\n each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables.\nThere are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph.\nDefinition 3. The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net.\nThe label on each vertex is simply the name of the part corresponding to\nthat vertex.\nThe vertices within a hyperedge correspond to the parts connected to the\ncorresponding net. The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty.\nBecause the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics. Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure. The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly.\nFigure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM. Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports.\nProposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.\nProof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist.\nAssume we have a set of variables for all nets that are consistent with the\ninput-output functions. As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13\n\nsomewhere in the netlist. This means that Equation (4) is satisfied by construction.\nAssume that we have a solution to Equation (4). Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove.\nThe wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation.\nDefinition 4. The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows. The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net.\nFigure 1(c) shows the netlist graph for the Bering Sea example.\nCorollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.\nProposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully.\nAs we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).\n\n3.2\n\nSheaves and cosheaves\n\nSheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related.\nThis nearness can be most easily formalized by using the netlist graph defined\nin the previous section.\nSince the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive. Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology.\nA topological space is a mathematical formalism that captures the notion of\n neighborhoods. \n14\n\nDefinition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T .\nThe ordered pair (X, T ) is called a topological space.\nOften, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U.\nThe following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R.\nAdditionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7]. For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below.\nDefinition 6. Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation. The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }.\nThe idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves).\nDefinition 7. Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2. For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U ) is the identity function.\n15\n\nDually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3. For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function.\nIf for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.\nAs Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take.\nDefinition 8. Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ).\nThe direct product is in general not the direct sum, since the topology\nmay be infinite! For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n!\nG\nC(U ) .\nU U\n\nIf U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,\n\nG\nC(X) = \nC(U ) / ,\nU open\n\nwhere is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16\n\nIntuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf.\nThe set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction. A similar axiom applies for cosheaves.\nDefinition 10. Let P be a presheaf on the topological space (X, T ). We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U.\nDually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U .\nFor the time being, we will focus on sheaves. Cosheaves will reappear in\nSection 5.\nGiven that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius.\nDefinition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p\n\ncS (a) := \n\nX\n\nX\n\nU T , V T :V U\n\np\n\n(dV (a(V ), S(V U )a(U ))) \n\n,\n\n(10)\n\nwhere p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV\n\nn\nX\nk=1\n\n!1/p\np\n\n|xk yk |\n\n,\n\nwhere αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.\n\n17\n\nConsistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T\n\nWe will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T\n\nAs the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.\n\n3.3\n\nThe netlist sheaf\n\nThe key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6).\nIn order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples. The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf.\nThis progression is summarized in Figure 3.\nBefore delving into the details, let us consider the meaning of the arrows\nshown in Figure 3. The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.\nIn the sheaf diagram the arrows are functions between the stalks. Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs. An output variable is determined by the\n18\n\nConstraints\n\nx\n\nx\n\nm b x\n\nx\n\nm b\n\npr1\n\nx\n\n n\n\npr2\n\npr3\n\ny = mx + b\n\ny = mx + b\n\ny\n\ny\n\ny\n\ny\n\ny\n\n n\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nn\n\nf\n\nAssignment support\n\nFigure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.\ndata within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical! The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net.\nBeginning the example in earnest, suppose that (x1 , y1 ), . . . , (xn , yn ) are n\npoints in the plane R2 . As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).\nThe netlist for the problem represents the same information as in the SEM.\nAs shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x).\nThe prediction process depends on the two parameters b and m, which can\nalso be considered as inputs. This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c).\nThe sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information. The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19\n\nthe stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).\n\n(11)\n\nThe function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form\n\na := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =\n\np\n\np\n\n|m\ne m| + |eb b| +\n\nn\nX\nk=1\n\np\n\n|f\nxk xk | +\n\nn\nX\nk=1\n\n!1/p\np\n\n|b + mf\nx k yk |\n\n(13)\n\nfor a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression.\nThe problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13). Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d).\nTo follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none. The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}.\nExplicitly, we start with a non-global assignment supported on U,\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) ,\n20\n\n(14)\n\nwhere the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )\n\n|m\ne m|2 + |eb b|2 +\n\nn\nX\nk=1\n\n|f\nxk xk |2 +\n\nn\nX\nk=1\n\n!1/2\n|b + mf\nx k y k |2\n\nBut since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1\n\nk=1\n\nThis permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1\n\nConsistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21\n\n.\n\ny\n\ny1\n\ny = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius\n\nb + mx1\nconstrained\nconsistency\nradius\n\nx\nx1\n\nx~1\n\nFigure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,\n\n(m, b, ((y1 b)/m, . . . , (yn b)/m, (y1 , . . . , yn ), (m, b, ((y1 b)/m, . . . , (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . . . , xn , (y1 , . . . , yn ), (0, b, (x1 , . . . , xn ))) .\nNoting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.\nRegression (model fitting) (Details above, included for completeness here.)\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . . . , xn ), (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for b and m. As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression.\n22\n\nConstraints\n\npr1\n\n...\n\npr3\n\npr2\n\nprn+2\n\nf1 f2\n\nn\n\nAssignment support\n\nfn\n\n... \n\nFigure 5: Modification to the sheaf in Figure 3(d) to allow for missing data.\nHybrid versions of the above problems can also be addressed.\nAssignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn . If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk.\nFigure 5 shows the resulting construction.\nThe fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , . . . , xn )) = mxk + b.\nThe set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by\n\na := m, b, x1 , . . . , xn , y1 , . . . , yn , m,\ne eb, x\nf1 , . . . , x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5). But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged.\nFeedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9).\n23\n\nX\n\nx\n\nx\nout\n\nf\n\ng\n\ng\n\nid\n\nX\n\nX\n\nid\n\nf\n\nin\n\ng\n\nf\n\nin\n\nout\n\ny\n\ny\n\nX\n\n(a)\n\n(b)\n\n(c)\n\nFigure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation.\nConsider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value.\nX = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.\nX = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2.\nX = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist.\nFeedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.4.\nWith the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm.\nDefinition 13. If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports. The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables.\nIt is often useful to have individual observations on their own stalks, like we\ndid in Figure 5. The following modification to Definition 13 allows for missing\ndata in general.\nDefinition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable. These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries.\n24\n\nx1, ... xn\nS\nin\n\na1, ... ak\n\ncoef\n\nLCF(k)\n\npr2\nk\n\n S\n\nout\n\nyn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n(a)\n\npr1\n\n k\n\nS\n(b)\n\nFigure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.\n\n3.4\n\nSheaves modeling autoregressive timeseries\n\nAutoregressive timeseries are sequences . . . , x0 , x1 , . . . that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . . , ak . We say that such a sequence is AR(k) autoregressive.\nAutoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.\nIt is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , . . . ak are constants.\nWe can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25\n\n... x1, ... xn\n\ns\n\nout\n\nin\n\nidentity\n\nLCF(k)\n\ncoef\n\na1, ... ak\n\nout\n\nin\n\nid\n\npr2\n\ns\n\n k s\n\nid\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\ns\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\nnetlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers.\nTo handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .\nThis is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).\nThe associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a.\nFollowing a theme already present in the linear regression example, there is\nduplication of data in the sheaf model. Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts.\nOnce again, if we consider an assignment supported on the two variables (with\nthe same values on each!), minimizing consistency radius will infer the values\nof the a coefficients. Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x.\nWhen the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k. The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.\n\n26\n\nx1, ... xn\n n\nin\n\nin\n\ncrop\n\nLCF(k)\n\na1, ... ak\n\ncoef\n\nout\n\nout\n\nid\n\npr2\n\n n\n\n k n\n\nprk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k\n\n n-k\n\n(a)\n\n(b)\n\npr1\n\n k\n\nFigure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.\n\n n\n\nin\n\n k n\n\nDietCope_lag\n\nCopepods\nin\n\npr2\n\nout\n\nid\n\ncrop\n\n n\nh\n\nDietCope_block\nout\n\nLCF(k)\nprk..n\n\n n - k\n\n n\n\nDiet_Cop\n(a)\n\n(b)\n\nFigure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.\n\n27\n\nAutoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself. As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.\n\n4\n\nSheaf encoding of the Bering Sea\n\nWe now return to the ecological DSEM example introduced in Section 2.2, and\nrefer the reader to Figure 1. The reader is directed to [36] for the software that\ngenerates the sheaf results presented in this section.\nThe DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d).\nThe netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners. If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11.\nThe DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time. This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values. Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there. In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable).\nAs noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally. The consistency radius of the assignment after minimization is\n\n28\n\nSeaIce\n\nColdPool\nln (ColdPool ) [ln(km2 )]\n\nln (SeaIce) [ln(km2 )]\n\n1\n0.5\n0.0\n 0.5\n 1.0\n\n0\n 1\n 2\n\n2018 ColdPool\n\n 3\n\nDietCopepods\nln (DietCopepods) [ ]\n\n3\n\nln (Copepods) [ln(count/m )]\n\nCopepods\n2.5\n0.0\n 2.5\n 5.0\n\n1\n0\n 1\n 2\n\nDietKrill\n\n3\n\nln (Krill ) [ln(count/m )]\n\nKrill\nln (DietKrill ) [ ]\n\n0.5\n0.0\n 0.5\n\n2016 Krill\n\n 1.0\n\n0.5\n0.0\n 0.5\n 1.0\n\nSpawners\n\nSurvival\nln (Survival ) [103 count/kg]\n\nSpawners[106 kg]\n\n4\n3\n2\n1\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nmeasurement\n\n2\n1\n0\n 1\n 2\n1960\n\nDSEM\n\n1970\n\n1980\n\n1990\n\n2000\n\n2010\n\n2020\n\nsheaf\n\nFigure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.\n\n29\n\nCopepods_pc\n\nCopepods\n\n n\npr2\n\npc\n\nin\n\nDietCope_block\n\npr1\n\nout\n\nDiet_Cop\n\n n\ng~1\n n\n\n(a)\n\n(b)\n\nFigure 12: Modification to the netlist to include path coefficients and constants\nas an input.\n11.9. Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.\nTaking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated.\nFigure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.\nUsing the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred. The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2.1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.\nThere are some differences between the sheaf and the measurement data.\nThe contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries. This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).\nThe second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30\n\nSeaIce\nSeaIce_lag\nSeaIce_lagvar\n\nColdPool_block\n\nColdPool_lagvar\n\nSeaIce_pc\n\nColdPool_lag\nColdPool\n\n2018_ColdPool\n\nColdPool_Copepods_pc\n\nColdPool_Krill_pc\nKrill_block\n\nCopepods_block\n\n2016_Krill\nCopepods\n\nKrill\nCopepods_lag\n\nCopepods_pc\nDietCopepods_block\n\nCopepods_lagvar\n\nKrill_lag\n\nKrill_pc\nDietKrill_block\n\nKrill_lagvar\n\nDietCopepods\n\nSpawners\n\nDietKrill\nDietCopepods_pc DietKrill_pc\n\nDietCopepods_lag\n\nDietKrill_lag\nSpawners_pc\n\nDietCopepods_lagvar\n\nSpawners_lag\n\nDietKrill_lagvar\nSpawners_lagvar\n\nSurvival_block\n\nSurvival\n\ncells\n\nrestrictions\nprojection map\nother function (see text)\n\ninferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present\n\n0\n2\n4\nconsistency radius contribution\n\nFigure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31\n\nSource\n\nTarget\n\nSeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)\n\nDSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2\n\nnone\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848\n\nSheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03\n2637\n2679\n\nAR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907\n\nTable 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11.\nWe should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.\nNotice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation.\nTable 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.) The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable.\nThere is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown.\nBecause the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.\nSome caution in comparing consistency radius across the columns of Table\n\n32\n\nVariable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill\n\nAR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060\n\nAR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445\n\nTable 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels.\n3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left). This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied. Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data.\nRuntimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM. This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.\n\n5\n\nThe topology of subsystems\n\nClassically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33\n\nbase space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently.\nWe will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6). Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems. This is not entirely compatible with all system decompositions in the literature, so caution is advised!\n(The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph. In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.)\n\n5.1\n\nDynamical systems\n\nDefinition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.\nIt is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems.\nDefinition 16. For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V.\nCorollary 7. If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V .\nDefinition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A. Notice that (i|A) i = i.\nDually, a projection is a function p : B A such that p p = p and\np|A = id A .\nProposition 8. Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU\n\nf\n\ni\n\nV\n\nf\n\n/U\n\n/V\n\ni \n\ncommutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34\n\nProof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng\n\ng\n\nS2\n\nf2\n\n/ S2\n\nComposition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.\n\n5.2\n\nThe cosheaf endomorphism of invariant sets\n\nThe state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it. To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other.\nWe begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally.\nLemma 10. Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V .\nDefine the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1. CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map.\nThen CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).\nProof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V . The space of global cosections on V is\n!\n!\nG\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V\n\nW V\n\nW V\n\nsince the equivalence identifies points that agree on overlaps.\n35\n\nThe above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))\n\nmV\n\n/ S(V )\nS(U V )\n\nR(f 1 (U )) mU / S(U )\n\nDually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO\n\nmV\n\n/ S(V )\nO\n\nR(f 1 (U ) f 1 (V ))\n\nS(U V )\n\nR(f 1 (U ))\n\nmU\n\n/ S(U )\n\nWith the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )\n\nFU :=f |U\n\nC(X,TX ) (f 1 (U ) f 1 (V ))\n\nC(X,TX ) (f 1 (V )) = f 1 (V )\n\n/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )\n\nFV :=f |V\n\n/ C(Y,T ) (V ) = V\nY\n\nwhich establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36\n\nNow suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).\nSuppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .\nProposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f .\nProof. Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU\n\nmU =f\n\n/U\n\ni\n\nV\n\nmV =f\n\n/V\n\ni\n\ncommutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.\n\n5.3\n\nSubsystem decomposition sheaf\n\nRather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni\n\nU\n\np\n\ni\n\nf |U\n\nB\n\n/U\n\np\n\ng\n\n/B\n\nDually, the diagram at right above captures the situation where B is a subsystem\nof f .\n\n37\n\nDefinition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection. When p is clear from\ncontext, we will often say g is a subsystem of f .\nWe can think of the function g as a dynamical system in its own right.\nThe idea of a subsystem is neatly compatible with the DSEM construction.\nAs will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition.\nExample 2. Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.\nTo see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . . ).\nThe dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (. . . ,an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n= (. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . ).\nBecause of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined.\nThe subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ) = (. . . , an+1 , an , . . . ).\nThe subsystem dynamical map g : A A is simply\ng(. . . , an , an 1 , . . . ) = (. . . , an+1 , an , . . . ).\nVerification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . )\n\n= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )\n\n= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38\n\nExample 3.\n?B\nA\n\nC\n\nFollowing the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA\n\n?C\n\nB\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)\n\nAh\n\nB\n\n(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv\n\n39\n\nfor some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))\n\n= prW (f (u) + f (w))\n\n= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)\n\n= (f |W ) (prW (u + w))\n= (f |W prW )(v).\n\nLemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf\n\nS\np2\n\np1 p2\n\nB2\n\np2\ng2\n\np1\n\nB1\n\n/S\n\n/ B2\n\np1 p2\n\np1\n\ng1\n\n/ B1\n\nso that (g1 , (p1 p2 )) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).\n\n40\n\nThis has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1\n\nZ\n\nf\n\n/ Z3\npr1\n\ng\n\n/Z\n\nIn this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2\n\nf\n\np\n\nB\n\n/ R2\np\n\nid\n\n/B\n\nwhere p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS\n\nf\n\np\n\nB\n\n/S\np\n\ng\n\n/B\n\nSuppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .\n\n41\n\nLemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\n\n/S\n\nf\n\nS\np\n\np\n\nB\n\n/B\n\ng\n\nThe conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )\n\nY\n\n/Y\n\ni\n\ni\nf\n\nS\n\nB\n\ng\n\n/S\n\n/B\n\nTherefore, when studying a dynamical system, one will often encounter problems of the following form.\nQuestion 1. When do lifts to the dashed arrows in the diagram above exist?\nAnswers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows.\nExample 9. Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.\nThe way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42\n\nbut rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU\n\nf |U\n\nX\n\n/U\n\nf\n\n/X\n\nAs an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt.\nEvidently, if PC = 1, then the program halts immediately. No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set.\nWe might instead imagine that the executable specified by U was the result\nof a compiled, high-level program. Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2.\nThe compilation process essentially ensures that we have the following commutative diagram\nU\n\nf |U\n\nq\n\nY\n\ng\n\n/U\n\n/Y\n\nq\n\nwhere the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np\n\nY\n\ng\n\n43\n\n/Y\n\np\n\nValues of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO\n\nf |U\n\ni\n\nY\n\ng\n\n/U\n\n/Y\n\nf\n\nXO\nq\n\n i\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nFor instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,\n\nR1 = x,\nR2 = y,\n\nR[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO\n\nf |U\n\n/U\nO\n\ni\n\nY\n\ni\ng\n\n/Y\n\nInspired by Example 9, suppose that we have a commutative diagram\nXO\n\nf\n\ni\n\nY\n\ng\n\n/X\n\n/Y\n\np\n\nwhere i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX\n\nf\n\np\n\nY\n\ng\n\n44\n\n/X\n\n/Y\n\np\n\ncommute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1\n\nf1\n\n/ S1\nO\n\np1\n\np1\n\nS\n\n/S\n\nf\n\np2\n\nS2\n\np2\n\nf2\n\n/ S2\n\nWe want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\n/ S3\n\nsuch that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS\n\np1\n\np 3\n\np2\n\nS2\n\n/ S1\n\np \n3\n\nf1\n\n/ S3\n\nS1\nf3\n\nf2\n\nS2\n\n45\n\np \n3\n\np 3\n\n/ S3\n\nProposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.\nAs a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1\n\ng1\n\np2\n\nB2\n\np2\n\ng2\n\n/ B2\n\ng1\n\n/ B1\n\np1\n\nB1\n\n/ B1\n\np1\n\nSince p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid\n\nZO 2\n\nf\n\n/ Z2\nid\n\nh\n\n/ Z2\nO\n\nid\n\nid\n\nZ2\n\ng\n\n/ Z2\n\nwhere\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).\n\n46\n\nThere is also no function h that will make the diagram below commute\nZ2\npr1\n\nZO\n\nf\n\n/Z\nid\n\nh\n\n/Z\nO\n\npr2\n\nZ2\n\nid\ng\n\n/Z\n\nwhere\nf (x, y) = 1 x,\n\nand\n\ng(x, y) = y.\nSuppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17. Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ).\nDefinition 21. Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 .\nEven if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.\nLemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S.\nProof. First of all, notice that id S : S S meets the criteria for a subsystem.\nWe merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB.\np:S B subsystem\n\np:S B subsystem\n\nSuppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47\n\nTherefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem.\nProposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.\nProof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism.\nIn short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x). One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems.\nWe are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem. Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality.\nThe key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem.\nDefinition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.\n\nLemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem.\nProof. Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.\nAs a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea.\nCorollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.\nCorollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.\n48\n\nf\n\n k\npr1\n\n k\npr1\n\n k \n\n k \n\npr1\n\npr1\n\n( k ) \n\n( k ) \npr1\n\npr1\n\ng\n\n( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4\n\npr1\n( k ) \npr1\n\npr1\n( k ) \npr1\n\n( k ) \n( k ) \npr1,2,5,6\n\n k\n\npr1,2,3,4\n\npr7\n\n k k \n\n k\n\npr7\n\n k k \n\nFigure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.\n\n6\n\nSubsystems of the Bering Sea system\n\nFigure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0\n\nwhile the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0\n\nNotice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49\n\nDefinition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.\n\n7\n\nConclusion\n\nIn this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software.\nSheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.\nThis chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3. the netlist sheaf with additional stalks for individual observations (Definition 14).\nWith these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.g.\nforward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50\n\nhybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks.\nIt remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf. In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM.\nIndeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.\n\nAcknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
{"method": "hybrid", "num_chunks": 1048, "avg_chunk_len": 97.62213740458016, "std_chunk_len": 163.08186596344342, "max_chunk_len": 818, "min_chunk_len": 1, "total_chars": 102308, "compression_ratio": 1.0186984400046917, "chunks": ["arXiv:2511.04603v1 [math.AT] 6 Nov 2025", "Analyzing the topological structure of composite\ndynamical systems\nMichael Robinson\nMichael L. Szulczewski\nJames T. Thorson\nSeptember 2025", "Abstract\nThis chapter explores dynamical structural equation models (DSEMs)\nand their nonlinear generalizations into sheaves of dynamical systems. It\ndemonstrates these two disciplines on part of the food web in the Bering\nSea. The translation from DSEMs to sheaves passes through a formal\nconstruction borrowed from electronics called a netlist that specifies how\ndata route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations\ncan be tested for consistency, how missing data can be inferred, and how\nuncertainty about the observations can be quantified. Sheaf modeling\nprovides a coherent mathematical framework for studying the interaction\nof various dynamical subsystems that together determine a larger system.", "Contents\n1 Introduction\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Chapter outline . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "2\n3\n4\n5", "2 Dynamical modeling of ecosystems\n2.1 DSEM background and motivation . . . . . . . . . . . . . . . . .\n2.2 Ecological background and the DSEM system for the Bering Sea", "5\n5\n7", "Approved for Public Release by The MITRE Corporation; Distribution Unlimited. Public\nRelease Case Number 25-2751. The author s affiliation with The MITRE Corporation is\nprovided for identification purposes only, and is not intended to convey or imply MITRE s\nconcurrence with, or support for, the positions, opinions, or viewpoints expressed by the\nauthor. 2025 The MITRE Corporation. ALL RIGHTS RESERVED.", "1", "3 Sheaf encodings of composite systems\n3.1 Netlists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Sheaves and cosheaves . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 The netlist sheaf . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Sheaves modeling autoregressive timeseries . . . . . . . . . . . .", "8\n11\n14\n18\n25", "4 Sheaf encoding of the Bering Sea", "28", "5 The topology of subsystems\n33\n5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 The cosheaf endomorphism of invariant sets . . . . . . . . . . . . 35\n5.3 Subsystem decomposition sheaf . . . . . . . . . . . . . . . . . . . 37\n6 Subsystems of the Bering Sea system", "49", "7 Conclusion", "50", "1", "Introduction", "Ecologists often study systems on spatial and temporal scales that cannot be\nexperimentally manipulated (ecosystem processes are distributed across continents, and arise from evolutionary dynamics over millennia), and for which\nextrapolating the results of experiments at fine space-time scales is challenging\n[48]. These systems are also challenging to study because observational data\ncan be noisy and sporadic. A third challenge is the presence of complex, causal\nrelationships between system variables that can change over time.", "Understanding the dynamics of these kind of large composite models is much\neasier reductively. Roughly speaking, a subsystem is a collection of state variables that makes sense as an independent dynamical system (Definition 20). Subsystems can be isolated for a variety of reasons, in addition to spatial or temporal separation.", "Regardless of the reason for the isolation, there is a canonical\nway to write a dynamical system in terms of its subsystems. This subsystem decomposition is a convenient way to explore dynamical summaries of the original\nmodel (Section 5). This chapter explores dynamical structural equation models (DSEMs) and\ntheir nonlinear generalizations via a topologically motivated translation into\nsheaves of dynamical systems (Sections 3 and 5).", "Sheaves are a strict generalization of DSEMs into nonlinear models, which they losslessly represent (Theorem 6). The translation of DSEMs into sheaves follows a clear graphical recipe,\nwhich allows handling observations in three ways: (1) as individual observations, (2) as individual timeseries, and (3) as collections of dynamically related\ntimeseries. The translation from DSEMs to sheaves passes through a formal construction\nborrowed from electronics called a netlist that specifies how data route through a\nsystem.", "Because the netlist and sheaf methodology is explicit and graphical, we\ninclude several illustrative examples (Figures 3 and 5). One real-world example\n2", "involves part of the food web in the Bering Sea (Figure 1; Sections 2.2, 4, and\n6).\nSheaves provide many advantages to a modeler. They enable exploring the\nimpact of uncertainty in various ways. They support inference of missing or\nerroneous data, including system parameters and coefficients (Section 3). They\nalso enable forecasts and retrocasts through the same interface, namely consistency radius optimization (Section 4).\nSheaves also highlight the importance of the original DSEM in model summarization. Using the sheaf of subsystems, Corollary 21 shows that the subsystems\nof a DSEM can be read off its associated graph. This is applied to the Bering\nSea ecosystem model in Section 6.", "1.1", "Related work", "The challenges in modeling ecological systems have motivated interest in structural causal models (SCMs) [31]. SCMs can be fit to observational data in space\nand time, and can decompose the total effect of one variable on another via a\ncombination of direct and indirect effects [16, 5]. Recently, SCMs have been\nadapted to the analysis of ecological time series via DSEMs [47].", "The key idea behind SCMs is that systems can be understood by decomposing them into coherent subsystems. The idea of reducing systems into subsystems has a long history, with general mathematical descriptions of composite\nsystems given by the field of cybernetics, for which Heylighen and Joslyn [17]\nand Ashby [6] are good introductions. Beyond cybernetics, the study of subsystems of dynamical models [50] has occurred in many fields, including manufacturing and operations research [49, 45, 21], design [2], statistical physics [51],\nmathematical systems [9], biology [26], and chemistry [18].", "Although algorithmic and systematic decomposition of systems into subsystems have become common since the dawn of cybernetics, it remains challenging. Maier et al. [27] laments, Even though abstraction is frequently mentioned\nwith regards to modeling and simulation, formal definitions are harder to find.", "One challenge is that decompositions are often not unique: for example, one may\nchoose to group state variables based on constraints rather than functional units\n[8, 24]. These choices are important because they drive the usefulness of the\ndecomposition [27]. For example, overlapping, rather than disjoint, subsystem\ndecompositions are useful for analyzing stability of an entire system [40, 4].", "We argue that a properly general and formal definition of a subsystem\ndecomposition must support overlappingness, non-uniqueness, and ambiguous\ngranularity. Because the collection of all subsystems forms a mathematical sheaf\n(Definition 21), this implies that seeking disjoint, unambiguous subsystems (as\nis often done) is fraught. Aspects of the formalism we introduce in this chapter are not entirely novel.", "For instance, Hirono et al. [18] defines a CRN morphism that is a special case\nof our Definition 20. Additionally, the sheaf of subsystems is based upon a\nclear graphical representation, which is well known in the analysis of software", "3", "[29, 1]. Moreover, Abadi and Lamport [1] uses the term refinement mapping,\nwhich evokes the analogous term from sheaves (Definition 7). Roughly dual to the notion of a subsystem is that of an invariant set of a\ndynamical system (our Definition 20 makes this a true duality).", "Invariant sets\nare widely used in dynamical systems [44], where they generalize equilibrium\nsets and attractors. For linear systems, duality between invariant sets and\nsubsystems is immediate and useful. For instance, the design structure matrix\n[43] yields invariant sets, giving a clear duality to subsystems.", "Finally, we note that the discipline of modeling a system s state via a decomposition into subsystems of state equations is explained in detail in Robinson\n[34, Sec. 5], and is specialized to subsystem graphs in Kearney et al. [22].", "In\nKearney et al. [22], the dynamics are specified locally and are much easier to\nspecify due to the fact that the system is given a graph structure.", "1.2", "Contributions", "This chapter provides an introduction to the discipline of modeling and analyzing a composite system using the language and tools of topology, centered\naround sheaves. Sheaf modeling provides a coherent mathematical framework\nfor studying the complicated interaction of various dynamical subsystems that\ntogether determine a larger system. The guiding principles of sheaf modeling\nare that\n a sheaf represents a hypothesis about how variables will interact (Definition 10),\n a non-global assignment represents the observations collected on the variables in its support (Definition 8),\n minimizing consistency radius estimates values of the variables and parameters that were not observed (Definition 11), and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.", "This chapter shows that when a dynamical system is described by a linear\nsystem, there are three sheaves that provide increasingly granular data about\nthe interactions between variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3.", "the netlist sheaf with additional stalks for individual observations (Definition 14).", "4", "1.3", "Chapter outline", "Section 2 describes a model of a food web in the Bering Sea, which we use to\nillustrate the use of sheaves. This system is large enough to exhibit interesting\nstructures, and corresponding observational data [47] are available. Additionally, we present a graphical causal modeling discipline called dynamical structural equation modeling that serves as an entry point into the more sophisticated\n(but admittedly less familiar) topological sheaf models.", "As is later shown in Section 3, sheaves are a strict generalization of DSEMs. Sheaves can be nonlinear,\nwhereas DSEMs are linear. Section 3 constructs sheaves that model composite systems, and develops\nthe main inferential tool, consistency radius minimization.", "Section 3 is selfcontained, as all of the mathematical background necessary to understand the\nconstructions is introduced as it is needed. Small concrete examples of the\nconstruction and use of sheaf models are presented to build intuition as well. In Section 4, we revisit the ecological model from Section 2 using the sheaf\ntools from Section 3.", "The interface between observational data, sheaves, and\ntheir inference tools is explored in detail. Moreover, we compare differences\nbetween the DSEM and sheaf approaches in detail. Section 5 introduces the idea of a general topological dynamical system, and\nshows that every dynamical system induces a sheaf of subsystems and a cosheaf\nof invariant sets, which form a dual pair.", "We prove that under appropriate conditions, the subsystems of a DSEM can be read off rather directly (Corollary\n21). This provides theoretical justification for why DSEMs are a useful way to\ndescribe a composite linear system by way of its subsystems. Section 6 revisits the ecological model from Section 2 once again.", "Because\nthe model satisfies the hypothesis of Corollary 21, we are able to present a clear\nrepresentation of all the subsystems present in the model. Finally, Section 7 concludes the chapter with practical advice for modelers\nand a brief discussion of future research work.", "2", "Dynamical modeling of ecosystems", "This section begins with a brief recount of modeling linear dynamical systems\naccording to an underlying graph structure, and then presents a representative\necosystem model that will be revisited several times in the chapter.", "2.1", "DSEM background and motivation", "Definition 1. Given a set of variables X = {x1 , . . . , xJ }, and a set Y = {t1 <\n < tT } of real valued time lags, a dynamic structural equation model (DSEM)\nconsists of an edge-labeled directed graph G with vertices X Y and edges E\nsuch that\nCausality The presence of an edge (xj1 , tk1 ) (xj2 , tk2 ) implies that tk1 tk2 ,\nand\n5", "Linearity Each edge (xj1 , tk1 ) (xj2 , tk2 ) is labeled with a real number γj1 ,k1 ,j2 ,k2\ncalled the path coefficient for that edge. The absence of an edge in the graph is assumed to be equivalent to assigning a\npath coefficient of 0. For brevity, we write a vertex (xj , tk ) simply as xj,k .", "The variables in a DSEM are to be interpreted as C 1 (R) functions, which\nare continuous timeseries. A directed edge xi,j xi ,j is to be interpreted as\nspecifying that a change in xi causes a proportional (linear) change in xi after\na lag of (tj tj ), with magnitude controlled by the associated path coefficient\nγi,j,i ,j . Under this interpretation, a DSEM implies that a first order system of\nlinear differential equations governs the values of the variables:\nJ\nT\ndxk (τ t ) X X\n=\nγk, ,i,j xi (τ tj ).", "dτ\ni=1 j=1", "(1)", "In what follows, we will refer to solutions of Equation 1 as solutions to the\nDSEM.\nIn the use of Equation (1) with observational data, there are two kinds of\nerrors that need to be considered: exogenous errors and measurement errors.\nExogenous errors accumulate, which means that an error in the value of a variable xk at given time τ impacts the value of xk at all later times. As a result,\nthere is a dependence between the exogenous errors of xk at different times. In\ncontrast, measurement errors at different times are assumed to be independent.\nExogenous errors will be represented by an additive term, ϵk, , resulting in\nT\nJ\ndxk (τ t ) X X\nγk, ,i,j xi (τ tj ) + ϵk, (τ ).\n=\ndτ\ni=1 j=1", "(2)", "We can approximate the solution to Equation (2) using the one-step backwards Euler method with time step h,\ndxk (τ t )\n1\n (xk (τ t ) xk (τ t h)) ,\ndτ\nh\nso that Equation (2) becomes a system of M = T J linear algebraic equations,\nxk (τ t ) xk (τ t h) + h", "J X\nT\nX\ni=1 j=1", "γk, ,i,j xi (τ tj ) + hϵk, (τ ).", "(3)", "If we fix a value of τ and organize the set of values {xk (τ t )} into a vector\nX of length M ), Equation (3) can be compactly written in matrix form as\nX PX + E,", "(4)", "where the entries of the M M path coefficient matrix P contain both the path\ncoefficients from the DSEM (scaled by h) and the additional nonzero entries due\n6", "the xk (τ t h) terms. In what follows, we will take h = 1, so that the path\ncoefficients in the DSEM appear unchanged as elements of the matrix P.\nTo obtain the path coefficient matrix P from observations of X, we assume\nthe exogenous errors follow a multivariate normal distribution with variance V,\nnamely\nE MVN(0, V),\nwhere E is the length M vector containing errors ϵtj .\nEquation (4) can then be re-arranged to yield a Gaussian Markov random\nfield,\nX MVN(0, Q 1 )\nT", "Q = (id P )V", "1", "(5)\n(id P),", "(6)", "where id is the identity matrix. The path coefficient matrix P can be obtained\nfrom the Cholesky decomposition of Q. The necessary calculations can be efficiently evaluated using sparse libraries, such as Eigen and CHOLMOD [11], and we\nuse Template Model Builder [25] to incorporate automatic differentiation and\nimplement the Laplace approximation [39] to marginalize across random effects.", "Now we address measurement errors. Assume the distribution of measurement errors of the variable xk is given by a distribution fj parameterized by θj\nat time tj . (If one does not wish to model measurement errors explicitly, so that\nmeasurement errors are entirely captured by the exogenous error term, this is\nobtained by choosing fj so that it has probability 1 at xk,j .", ") Let us write yk,j\nfor the observation of the variable xk,j . We therefore can express the mean of\nthe distribution of yk,j through a link function gj , via", "yk,j fj gj 1 ( j + xk,j ), θj ,\nwhere j is the true mean.\nThe clearest way to obtain the required sparsity in solving for P is to assume\nadditionally that the measurement errors for a given variable do not depend on\ntime tj . Let G be the J J matrix that is diagonal, and whose diagonal terms\nare given by the link functions gj . With this in hand, V takes the form\nV = id T T GGT ,", "(7)", "where is the Kronecker product. This implies that V is block diagonal, and\nis thereby efficient to invert.", "2.2", "Ecological background and the DSEM system for the\nBering Sea", "To demonstrate the use of sheaves for dynamical systems, we make a sheaf\nfrom a DSEM for ecological mechanisms linking regional oceanography (winter sea ice extent) to first-winter survival of juvenile Alaska pollock (Gadus\nchalcogrammus) in the eastern and northern Bering Sea [47]. The model starts\n7", "by specifying that abundance of age-0 pollock Rt (termed age-0 recruitment )\ncan be predicted from the biomass of spawning females St in a given year t:\nRt = St eα βSt +ϵt", "(8)", "α", "where e is the maximum expected recruits per spawning biomass, β is the expected density-dependent decrease in recruits per spawning biomass as biomass\nincreases, and ϵt is additional process error representing unmodeled variation\nin recruitment. This Ricker stock-recruit model [33] has been used for over\n70 years to represent density-dependent changes in juvenile survival, and as the\nbasis for defining biological reference points that are used worldwide to identify\nsustainable levels of fishing mortality [42]. The Ricker model is expected to\narise for species where adult abundance directly impacts juvenile survival for\nexample, due to cannibalism or interference competition [15].", "Alaska pollock\nare cannibalistic, so the Ricker model has theoretical justification. Usefully, the\nRicker model can be linearized as:", "Rt\n= α βSt + ϵt\n(9)\nlog\nSt\nand a DSEM can be used to elaborate the mechanisms that contribute to process\nerrors ϵt based on prior ecological hypotheses. The DSEM we translate into a sheaf was previously developed by Thorson\net al. [47].", "It specifies that variable winter sea ice formation (SeaIce) drives\nresidual variation in log-recruits per spawning biomass (Survival ) via two paths,\nmediated by sea-ice impacts on either copepod abundance (Copepod ) or krill\nabundance (Krill ), and resulting consumption by juvenile pollock. See Table\n1 and 2 for more details on the variables and mechanisms in the model. The\nDSEM includes a first-order autoregressive term for each variable, to allow the\nmodel to correct for bias that can arise when correlating variables that follow\nan autoregressive process (summarized in [28]).", "This first-order autoregression\ncan also be interpreted to represent Gompertz density-dependence and therefore\nhas some scientific interest [23], although it is not further discussed here.", "3", "Sheaf encodings of composite systems", "In this section, we explain how to construct a netlist sheaf whose global sections\ncorrespond bijectively to the solutions of a DSEM. This is performed in two\nmain steps: (1) the DSEM is translated into a netlist, and (2) the netlist is\ntranslated into the netlist sheaf. Since the machinery of sheaves is not in wide\nusage, Section 3.2 provides the necessary background.\nWith the machinery and the translation in place, Theorem 6 establishes that\nthe two representations, the DSEM and the netlist sheaf, are equivalent. The\nglobal sections of the netlist sheaf are in bijective correspondence with solutions\nto the DSEM. Moreover, a process called consistency radius minimization in\nthe sheaf finds approximate solutions to the DSEM, and this process is robust\nto perturbations.\n8", "Table 1: Variables that describe Alaska pollock recruitment used in the DSEM\nand sheaf. All except Spawners are transformed by the natural logarithm and\nthen centered (i.e., subtracted by their mean) prior to analysis. Timeseries of\nthe variables are taken from [47].\nName\nSeaIce", "Description\nAverage spatial extent (km2 ) of sea ice in the Bering Sea\nfrom Oct.15 to Dec.15 the preceding year, from the National\nSnow and Ice Center s Sea Ice Index, Version 3 [14]", "ColdPool", "Spatial extent (km2 ) of waters with temperatures 2 C\nnear the seafloor, interpolated from measurements by the\neastern Bering Sea bottom trawl survey and compiled in Rpackage coldpool [37]", "Spawners", "Female spawning biomass (in units of 106 kg) for Alaska pollock in the eastern and northern Bering Sea, estimated by\nthe age-structured stock assessment model used for management [20]", "Survival", "Age-0 recruits per spawning biomass (103 count/kg), calculated as age-1 abundance the following year (109 count)\nestimated by the age-structured stock assessment model [20]\ndivided by Spawners", "Copepods", "Density of 2 mm copepods (count/m3 ) from the Bering\nSea middle shelf [38], averaged across samples obtained during the fall mooring cruise along the 70 isobath from Sept.\nto early Oct. [12] (calculated by Dave Kimmel, pers. comm.)", "Krill", "Index of euphausiid abundance (count/m3 ) [32] obtained\nfrom backscatter measured during a summer acoustic-trawl\nsurvey in the eastern Bering Sea and converted to abundance\nusing a target-strength model [41]", "DietCopepods", "Biomass of copepods divided by total prey biomass in juvenile stomach samples (kg/kg), calculated from a fall surfacetrawl survey in the eastern Bering Sea [30]. For each surface\ntrawl, total catch of juvenile pollock is weighed, individual\npollock are subsampled, and stomach contents for subsampled individuals are identified to species and weighed. The\ndiet index is calculated as the average across subsampled\nstomachs, weighted by the catch of juvenile pollock in the associated surface trawl sample (calculated by Alex Andrews,\npers. comm.).", "DietKrill", "Same as DietCopepods, but for euphausiids (krill)\n9", "Table 2: List of path coefficients connecting variables (defined in Table 1),\nsupporting ecological hypotheses, and hypothesized sign for the path used in\nthe DSEM case study. We also include a first-order autoregressive term for\neach variable (i.e., 8 AR1 coefficients, not shown here) for reasons discussed in\nSection 2.2.\nPath\nSeaIce ColdP ool", "Ecological hypothesis and evidence\nSea ice formation (SeaIce) causes\nvariation in summer cold-pool extent\n(ColdPool )", "Sign\n+", "ColdP ool Copepods", "Warmer\nwater\ntemperatures\n(ColdPool ) result in higher copepod metabolism and therefore earlier\nonset of winter diapause, resulting in\na decrease in fall copepod abundance\n(Copepods) [10]", "+", "ColdP ool Krill", "Water temperatures (ColdPool ) might\naffect krill overwinter survival, affecting summer krill abundance (Krill )", "?", "Copepods DietCopepods", "Increased copepod abundance will result in them being a higher proportion of age-0 fall stomach contents\n(DietCopepods), due to pollock being hypothesized to be a relative nonselective predator", "+", "Krill DietKrill", "Same as Copepods DietCopepods\nbut for krill", "+", "DietCopepods Survival", "Increased fraction of fall diet from\ncopepods (Copepods) will increase energy reserves and subsequent survival of age-0 over their first winter\n(Survival ) [19]", "+", "DietKrill Survival", "Same as DietCopepods Survival,\nbut for krill", "+", "Spawners Survival", "Increased\nspawning\n(Spawners) will cause a\ndependent decrease in\n(Survival ) [15]", "10", "biomass\ndensitysurvival", "SeaIce", "out", "ColdPool", "f\n n", "ColdPool", "in", "Copepods_block\nout", "Krill_block\nout", "Copepods", "Krill", "Krill", "in", "Copepods_block", "Krill_block", "in", "DietCope_block\nDiet_Cop", "Diet_Krill", "Spawners", "out", "Diet_Cop\nSurvival", "in_copepods", "out", "Spawners", "out", "Diet_Cop\nin_copepods", "in_spawners", "Diet_Krill", "Spawners", "in_krill", "Survival_block", "in_spawners", "g2", "g1", "n", "id\n n\nh", "id\n n\nk\n n", "n\npr1", "pr2\nn", "n", "Survival", "m\n n", "(b)", "(c)", "(d)", "out", "n", "n", "Survival", "out", "(a)", "in", "DietKrill_block", "DietCope_block", "out", "Diet_Krill\nin_krill", "Survival_block", "Krill", "in", "in", "DietKrill_block", "n", "out", "Copepods", "id", "id", "in\nin", "out", "Copepods", "n", "ColdPool_block", "out", "ColdPool", "id", "in", "ColdPool_block\nSeaIce", "n", "SeaIce", "in", "n", "n\npr3", "Figure 1: (a) The DSEM model for part of a food web in the Bering Sea [46], (b)\nits wiring hypergraph, (c) its netlist graph, and (d) its sheaf diagram. The arrows in each subfigure have different meanings: in (a) they denote causal, linear\nrelationships (Sec. 2.", "1); in (c), they point from netlist parts to nets (Sec. 3. 1);\nand in (d), they denote restriction functions (Sec.", "3. 2). While the DSEM also\nestimates a first-order autoregressive term for each variable (not shown in (a)\nto simplify presentation), there is no autoregressive structure assumed in the\nsheaf model.", "This remedied in Section 3. 4. Throughout this section, we refer to Figure 1 for intuition.", "Figure 1(a) shows\nthe DSEM for part of the food web in the Bering Sea. The DSEM-to-netlist\ntranslation, described in Section 3. 1, results in Figure 1(b).", "Figure 1(c) shows a\ndifferent representation of the netlist that is more expedient for the construction\nof the netlist sheaf. Proposition 3 establishes that the two representations of\nnetlists (Figures 1(b) (c)) determine each other, so we may use whichever is\nmore convenient. Finally, the netlist-to-sheaf translation, described in Section 3,\nresults in Figure 1(d).", "Section 3. 4 shows how to encode autoregressive timeseries\nmodels as netlist sheaves, which ultimately makes handling missing data both\ntransparent and automatic within the netlist sheaf.", "3.1", "Netlists", "The term netlist appears to have entered the technical lexicon in the early\ndays of computing, when IBM started to automate the wiring of mainframe\nback planes [3]. Since that time, the term netlist has been in wide usage but\noften without a precise definition. In order to formalize the concept, we say\nthat a netlist describes a system of parts interconnected with nets, which carry\ntime-varying signals (briefly, variables).", "Each variable consists of the specification of a set of possible values for a\nnet. In this chapter, the values for a variable in a net are initially assumed to be\ncontinuous timeseries, usually of the form C 1 (R). We will also consider sampled\ntimeseries of the form Rn , where n is the length of the timeseries.", "In Section\n3. 4, we show how to handle missing values in such a timeseries. Each part has a number of ports, to which connections can be made.", "Each\nport is either an output, which means that it determines the value of the variable\n11", "Part 2 (capacitor)\nNet 1", "in", "out", "Net 2\nin", "out", "Part 1\n(Battery)", "Part 3 (resistor)\nin", "out", "Net 3", "Figure 2: A netlist for an electric circuit, described in Example 1. of a net connected to it, or an input, which means that it does not determine\nthe value of the variable of a net connected to it. Each net specifies that a collection of distinct ports on a pair of parts (which\nneed not be distinct) are connected, with the requirement that not more than\none of these ports be an output.", "Finally, each part specifies an input-output\nfunction for each output port. The domain of an input-output function is from\nthe product of the set of its input variables, and its codomain (range) is the set\nof output variables at the output port. This formulation leaves open the possibility of nets that are not attached\nto any output ports, which are called external inputs, and nets which are not\nattached to any input ports, which are called external outputs.", "Clearly each\nexternal output must attach to exactly one port, which must be an output port. Example 1. Figure 2 shows an electrical circuit with three parts: a battery,\na capacitor, and a resistor.", "These parts are connected to each other by three\nnets:\n1. Connecting the positive (output) port of the battery to the input port of\nthe capacitor,\n2. Connecting the output port of the capacitor to the input port of the\nresistor, and\n3.", "Connecting the output port of the resistor to the input port of the battery. The values of the variables on the nets specify electrical currents flowing along\nthem. We note that the labeling ports as input and output in this kind of\ncircuit is arbitrary, since the electrical current can flow in either direction along\na net.", "The input-output functions simply recount classical Ohm s law for each\nof the parts in the circuit. This circuit contains no external inputs nor external\noutputs. A DSEM graph can be translated into a netlist via the following construction.", "Definition 2. Given a DSEM, its corresponding netlist is given by the following\nrecipe:\n each DSEM variable (node) becomes a net,\n12", "each DSEM variable with more than one input becomes a part,\n each net is connected to input ports via its out-neighbors,\n each net is connected to output ports via matching the name of the net\nto the part with the same name (if any exist), and\n the part s input-output function is collected from the matrix block in\nEquation (4) corresponding to the input and output variables. There are two combinatorial structures associated to a netlist, the wiring\nhypergraph and the netlist graph. Definition 3.", "The wiring hypergraph of a netlist is a vertex- and edge-labeled\npartition-directed multi-hypergraph that has a vertex for each part and an hyperedge for each net. The label on each vertex is simply the name of the part corresponding to\nthat vertex. The vertices within a hyperedge correspond to the parts connected to the\ncorresponding net.", "The label on each hyperedge is an ordered triple, consisting\nof the inputs port of the net (if any), the output port of the net (if any), and the\nvariable name of the net. The partition direction of each hyperedge separates\nthe output port from the input ports; either of these may be empty. Because the labeling on the wiring hypergraph is complicated, we represent\nit with a standard visual grammar borrowed from electronics.", "Each part is\nrepresented by a rectangle with its label in the center of the rectangle. Each\nnet is drawn as a path (with right-angle bends as needed) to connect the corresponding parts. If a net has more than two ports, the path is drawn as a tree\nstructure.", "The label of the variable of the net is shown next to the path, but\nthe name of the net s input and output ports are shown inside the connected\nparts rectangles, around the edge of the rectangle. The input-output functions\nare not shown explicitly. Figure 1(b) shows the wiring hypergraph for the netlist constructed using\nDefinition 2 for the Bering Sea DSEM.", "Notice that the net ColdPool corresponds\nto a hyperedge of size 3 in the wiring hypergraph, because it is connected to\none output port and two input ports. Proposition 1. The solutions to a DSEM are in bijective correspondence with\nlabelings of the nets with values of variables that are consistent with the netlist s\ninput-output functions.", "Proof. The solutions to the DSEM are characterized by Equation (4), which is\na matrix block assembly of everything that is needed to construct the netlist. Assume we have a set of variables for all nets that are consistent with the\ninput-output functions.", "As noted above, each variable takes values in a set of\nthe form C 1 (R). On the other hand, each input-output function was constructed\nfrom a matrix block in Equation (4). Because all of the DSEM variables appear\nas nets in the netlist, all such matrix blocks appear as input-output functions\n13", "somewhere in the netlist. This means that Equation (4) is satisfied by construction. Assume that we have a solution to Equation (4).", "Definition 2 constructed\nthe input-output function from the subblock of Equation (4), so there is nothing\nfurther to prove. The wiring hypergraph is closely related to the DSEM, but for constructing\nthe netlist sheaf in Section 3, it is more convenient to use another combinatorial\nrepresentation. Definition 4.", "The netlist graph is a vertex- and edge-labeled directed graph\nthat has a vertex for each part, a vertex for each variable, and two edges for\neach net. The label on a vertex is simply the name of the corresponding part\nor variable. The two edges for each net are defined as follows.", "The first edge is\nlabeled with the input port of the net, and leads from that corresponding part\nto the net. The second edge is labeled with the output port of the net, and\nleads from that corresponding part to the net. Figure 1(c) shows the netlist graph for the Bering Sea example.", "Corollary 2. The netlist graph is a directed acyclic graph, and induces a preorder on the set of parts and variables. In the preorder, each variable is above\nthe parts to which it is connected.", "Proposition 3. The netlist graph is the incidence bipartite graph of the wiring\nhypergraph, whose edges are labeled by projecting out the first and second components of the labels of the hyperedges. Consequently, the netlist graph and the\nwiring hypergraph determine each other fully.", "As we will see, the correspondence between the wiring hypergraph and the\nnetlist graph is convenient. Although Proposition 1 showed that the wiring\nhypergraph is most closely related to the DSEM, we will later show that the\nnetlist graph is most closely related to the netlist sheaf (Theorem 6).", "3.2", "Sheaves and cosheaves", "Sheaves and cosheaves are topological constructions that allow one to study the\nlocal consistency structure of a model. In the case of a DSEM, locality is useful\nbecause variables that are near one another in the graph are likely to be related. This nearness can be most easily formalized by using the netlist graph defined\nin the previous section.", "Since the netlist graph is a directed acyclic graph, it naturally induces a\npre-ordered set on the vertices. That is, if a b in a directed graph, we define\na b. When the graph is directed and acyclic, generalizing to paths within\nthe graph results in a relation that is reflexive and transitive.", "Pre-ordered\nsets have a natural notion of neighborhoods, hence a natural topology. A topological space is a mathematical formalism that captures the notion of\n neighborhoods. 14", "Definition 5. A topology on an arbitrary set X is a collection T of subsets of\nX satisfying the following four axioms:\nEmpty set The empty set is an element of T ,\nWhole set The set X is an element of T ,\nFinite intersection If U and V are elements of T , then U V is an element\nof T , and\nArbitrary union If U T then U is an element of T . The ordered pair (X, T ) is called a topological space.", "Often, rather than specifying T directly, we specify a collection of subsets U\nof X that generate the topology, which is the smallest topology (in the sense of\ninclusion) that contains U. The following are elementary examples of topological spaces,\nDiscrete topology For any set X, let T be the power set of X,\nTrivial topology For any set X, let T = { , X},\nEuclidean topology For X = R, the usual topology T is generated by the set\nof open intervals (a, b) for a < b R. Additionally, there is a powerful combinatorial theory of topological spaces\n(X, T ) in which the topology T is a finite set [7].", "For our purposes, the most\ninteresting of these finite topological spaces are those that arise naturally from\na pre-ordered set, given by the definition below. Definition 6. Suppose that (P, ) is a pre-ordered set, which is to say that\n is a reflexive and transitive relation.", "The Alexandrov topology Alex(P, ) on\n(P, ) is the topology generated by all subsets of P of the form Ux = {x y :\ny P }. The idea of sheaves and cosheaves is that each open set an element of the\na topology is associated with a set of values, called the stalk (for sheaves) or\ncostalk (for cosheaves). Definition 7.", "Suppose (X, T ) is a topological space. A presheaf S of sets on\n(X, T ) consists of the following specification:\n1. For each open set U T , a set S(U ), called the stalk at U ,\n2.", "For each pair of open sets U V , there is a function S(U V ) : S(V ) \nS(U ), called a restriction function (or just a restriction), such that\n3. For each triple U V W of open sets, S(U W ) = S(U V ) S(V \nW ) and\n4. S(U U ) is the identity function.", "15", "Dually, a precosheaf C of sets on (X, T ) consists of the opposite specification:\n1. For each open set U T , a set C(U ), called the costalk at U ,\n2. For each pair of open sets U V , there is a function C(U V ) : C(U ) \nC(V ), called an extension function (or just a extension), such that\n3.", "For each triple U V W of open sets, C(U W ) = C(V W ) C(U \nV ) and\n4. C(U U ) is the identity function. If for every U T there is a pseudometric dU on the (co)stalk at U , and each\nrestriction (or extension) is continuous with respect to the corresponding pseudometrics, we call the entire collection of data a pre(co)sheaf of pseudometric\nspaces.", "As Definition 7 makes clear, pre(co)sheaves on a topological space are only\nsensitive to the poset of open sets, and not to the points in those open sets. In\nour context, the set of values should be interpreted as the set of values that a\ncollection of variables in a DSEM can take. Definition 8.", "Suppose S is a presheaf on a topological space (X,QT ). An assignment a supported on U T is an element of the direct product, U U S(U ). The direct product is in general not the direct sum, since the topology\nmay be infinite!", "For this reason, dually, if C is a precosheaf on (X, T ), then a\ncoassignment supported on U T is an element of\n! G\nC(U ) . U U", "If U = T , we usually say that the (co)assignment is global.\n(Co)assignments may or may not be consistent with their pre(co)sheaf structure. When they are fully consistent, we highlight this fact by calling them\n(co)sections.\nDefinition 9. A global section of a presheaf S on a topological space (X, T ) is a\nglobal assignment s such that for all open V U then S(V U ) (s(U )) = s(V ).\nDually, a global cosection of a precosheaf C on a topological space is a global\ncoassignment c of the disjoint union under an equivalence,", "G\nC(X) = \nC(U ) / ,\nU open", "where is the equivalence relation generated by c1 c2 whenever c1 C(U1 ),\nc2 C(U2 ), with U1 U2 , and (C(U1 U2 )) (c1 ) = c2 .\nLocal (co)sections are defined similarly, but refers to some collection U of\nopen sets.\n16", "Intuitively, a (co)section corresponds to data that is fully consistent with the\nhypothesis posed by a (co)sheaf. The set of global sections of a presheaf on a topological space may be quite\ndifferent from S(X). It is for this reason that when studying presheaves over\ntopological spaces, an additional gluing axiom is included to remove this distinction.", "A similar axiom applies for cosheaves. Definition 10. Let P be a presheaf on the topological space (X, T ).", "We call\nP a sheaf on (X, T ) if for every open set U T and every collection of open\nsets U T with U = U , then P(U ) is isomorphic to the space of sections over\nthe set of elements U. Dually, a precosheaf C is a cosheaf on (X, T ) if for every open set U T\nand every collection of open sets U T with U = U, then C(U ) is isomorphic\nto the space of cosections over the set of elements U . For the time being, we will focus on sheaves.", "Cosheaves will reappear in\nSection 5. Given that most assignments are not sections, it is useful to be able to\nmeasure how far away an assignment is from being a section. When we have\npseuodmetrics on the stalks, one useful estimate of that distance is the consistency radius.", "Definition 11. If S is a presheaf of pseudometric spaces on a topological space\n(X, T ) and a is a global assignment, the p-norm consistency radius of a is the\nquantity\n 1/p", "cS (a) :=", "X", "X", "U T , V T :V U", "p", "(dV (a(V ), S(V U )a(U )))", ",", "(10)", "where p 1.\nIn all of our examples, p = 2 is used. A subtle point is that the relative\nweight of each of the different terms in Equation (10) is implicitly carried by the\npseudometrics dV . For instance, if x, y Rn , a weighted form of the Euclidean\npseudometric could be written\ndV (x, y) = αV", "n\nX\nk=1", "!1/p\np", "|xk yk |", ",", "where αV > 0 is a constant that weighs the importance of the value in the stalk\non V in the overall consistency radius. In some cases, for instance if different\nunits of measure are involved, the correct choice of αV is clear. In others, the\nαV is a nuisance parameter that needs to be explored by the modeler.\nCorollary 4. If s is a global section of a presheaf S of pseudometric spaces,\nthen cS (s) = 0.", "17", "Consistency radius is stable under perturbations, which means that it can\nbe reliably estimated.\nTheorem 5. [35, Thm. 1] Consistency radius is a continuous real-valued function of the assignment.\nWe will often need to consider local assignments as well. A natural definition\nis to define the consistency radius of a local assignment to be the consistency\nradius of the best extension of the local assignment to a global one.\nDefinition 12. [35, Def. 16] If S is a presheaf of pseudometric spaces on a\ntopological space (X, T ) and a is an assignment supported on U T , then its\nconsistency radius is\n(\n)\nY\nS(U ) such that b(U ) = a(U ) if U U .\ncS (a; U) := min cS (b) : b \nU T", "We will use the phrase minimizing the consistency radius of a as a shorthand\nfor finding the global assignment\n(\n)\nY\nb := argmin cS (b) : b \nS(U ) such that b(U ) = a(U ) if U U .\nU T", "As the rest of this chapter shows, minimizing the consistency radius of a\ngiven local assignment is the primary tool for sheaf-based inference.", "3.3", "The netlist sheaf", "The key result of this section is that inference for a DSEM corresponds to\nconsistency radius minimization. In general, it is enabled by Definition 2 that\ntranslates a DSEM into a netlist, and Definition 13 that translates a netlist into\na sheaf, in such a way that solutions correspond to global sections (Theorem 6). In order to motivate the construction, and to explain some of its subtleties,\nwe delay the formal construction (Definition 13) until after we have discussed\ntwo examples.", "The first example represents a classic linear regression problem\nfirst as a SEM (which is not dynamical), then as a netlist, and finally as a sheaf. This progression is summarized in Figure 3. Before delving into the details, let us consider the meaning of the arrows\nshown in Figure 3.", "The arrows in each of the frames of Figure 3 mean different\nthings. In the SEM the arrows have a causal interpretation: the value of x\ndetermines that of y. This interpretation carries over into the netlist, where\nports are either inputs or outputs.", "In the sheaf diagram the arrows are functions between the stalks. Since\nthe stalks represent the set of possible values for each variable, the functions\nrepresented by the arrows will be used to extract data stored on the ports and\nplace them on the nets regardless of whether they are inputs or outputs. There\nis no intuitive issue with the outputs.", "An output variable is determined by the\n18", "Constraints", "x", "x", "m b x", "x", "m b", "pr1", "x", "n", "pr2", "pr3", "y = mx + b", "y = mx + b", "y", "y", "y", "y", "y", "n", "(a)", "(b)", "(c)", "(d)", "n", "f", "Assignment support", "Figure 3: A linear regression problem as (a) a SEM, (b) a netlist with hardcoded\ncoefficients, (c) a netlist with coefficients exposed as inputs, and (d) a sheaf. To\nsolve the linear regression problem, the partial assignment supported on the\ndarkest shaded region is supplied by the observations, and then the assignment\nis extended to the remaining stalks. Finally, the copies of m, b, and x that\nshould be constrained so that they are identical are shown by the three lighter\nshadings.", "data within the part it is attached to. However, for an input, the only thing the\narrow does is extract the corresponding port s value unmodified. This seems\nparadoxical!", "The point is that when two parts are connected to each other on\na net, they both have a claim on what the value of the variable should be. If\nthe values correspond to a global section of the sheaf, this is the assertion that\nboth claims on that variable agree, namely the variable produced by the output\nof one port is the same as the variable that reaches the input port attached to\nthe same net. Beginning the example in earnest, suppose that (x1 , y1 ), .", ". . , (xn , yn ) are n\npoints in the plane R2 .", "As a modeling choice, we suppose that the x values can\nbe used to predict the y values, or alternatively that x is an explantory variable\nand y is a response variable. If we assert that the model should be linear, we\nare assuming\ny b + mx,\nwhere b and m are parameters to be found. To express this modeling assumption\ngraphically, we write an arrow x y, yielding the SEM graph in Figure 3(a).", "The netlist for the problem represents the same information as in the SEM. As shown in Figure 3(b), the netlist consists of two variables (x and y), and one\npart (the linear equation that predicts y from x). The prediction process depends on the two parameters b and m, which can\nalso be considered as inputs.", "This change results in a netlist with four variables\n(x, y, b, and m) and the same part as before, shown in Figure 3(c). The sheaf representation of the same system is shown in Figure 3(d). It is\nconsiderably more explicit about variable type information.", "The stalk over m\nand b is R, since each of these parameters takes a real value. On the other hand,\n19", "the stalk over x and y is Rn , since they are each a sequence of n real values. The\nstalk over the single part is the set of its inputs, namely R R Rn , corresponding\nto m, b, and x, respectively. The restriction maps from the part to the inputs\nare all projection maps, which select the different inputs. Explicitly,\npr1 (m, b, (x1 , . . . , xn )) = m,\npr2 (m, b, (x1 , . . . , xn )) = b,\nand\npr3 (m, b, (x1 , . . . , xn )) = (x1 , . . . , xn ).\nThe remaining restriction map f shown in Figure 3(d) performs the prediction\nprocess, and is given by\n(y1 , . . . , yn ) = f (m, b, (x1 , . . . , xn )) = (mx1 + b, . . . , mxn + b).", "(11)", "The function f applies the common coefficients (b and m) to each of the input\nvalues xk to yield the corresponding output values yk .\nThe space of global assignments for the sheaf shown in Figure 3(d) is given\nby the product of all of the stalks. This means there are two copies of m, b, and\nx in the space of global assignments, one for the value of the variable and one\nas a component of the part. A typical global assignment a is of the form", "a := m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), m,\ne eb, (f\nx1 , . . . , x\nfn ) ,\n(12)\nwhere we have listed the four variables first followed by the part. The consistency radius of this assignment is\nc(a) =", "p", "p", "|m\ne m| + |eb b| +", "n\nX\nk=1", "p", "|f\nxk xk | +", "n\nX\nk=1", "!1/p\np", "|b + mf\nx k yk |", "(13)", "for a given p. In what follows, we will take p = 2, so as to agree with classical\nlinear regression. The problem of classical linear regression seeks real numbers m and b minimizing the last term in Equation (13).", "Therefore, minimizing consistency radius\nsubject to the constraint that each pair of copies of m, b, and x is equal, and\nthat only m and b are allowed to vary will recover linear regression from the\nsheaf. These copies are identified in the lighter shaded regions in Figure 3(d). To follow the paradigm of consistency radius minimization, we specify a local\nassignment to the variables x and y, and then extend the assignment to a global\none.", "The support of the local assignment is expressed by the darkest shaded\nregion in Figure 3(d). Notice that the nets have no higher elements in the partial\norder shown in Figure 3, so the support of this assignment is U = {{x}, {y}}. Explicitly, we start with a non-global assignment supported on U,\n( , , (x1 , .", ". . , xn ), (y1 , .", ". . , yn ), ) ,\n20", "(14)", "where the dashes indicate stalks outside the support of the assignment. If we\nseek a global assignment g such that\ng = argmin {c(b) : g(U ) = a(U ) for U U},\nthis means that we wish to find the entries in the assignment in Equation (12)\nthat are marked with the dashes in Equation (14), namely\nm,\ne eb, m, b, and (f\nx1 , . . . , x\nfn ).\nMinimizing consistency radius is therefore given by the problem\nargmin m,\ne e\nb,m,b,(x1 ,...,xn )", "|m\ne m|2 + |eb b|2 +", "n\nX\nk=1", "|f\nxk xk |2 +", "n\nX\nk=1", "!1/2\n|b + mf\nx k y k |2", "But since both m\ne and m, and eb and b are being minimized, the consistency\nradius reduces to\n!1/2\nn\nn\nX\nX\n2\n2\nargmin m,b,(x1 ,...,xn )\n|f\nxk xk | +\n|b + mf\nx k yk |\n.\nk=1", "k=1", "This permits the values of the variables x and y to differ from their copies,\nsubject to a penalty. Instead of least squares regression, this problem is what\nis usually called total least squares; see Figure 4. After minimization, the differences between each of the copies\n|f\nxk xk |\nexpresses the uncertainty of their values if the model is to be taken as a given.\nTo obtain classical least squares regression, we must constrain x\nfk = xk for\nall k. The global assignment we seek is of the form\ng = (m, b, (x1 , . . . , xn ), (y1 , . . . , yn ), (m, b, (x1 , . . . , xn ))) ,\nso that the consistency radius minimization problem subject to this constraint\nbecomes\n!1/2\nn\nX\n2\nargmin m,b\n|b + mxk yk |\n.\nk=1", "Consistency radius minimization unifies several different inference tasks in\nFigure 3, depending on the support of the initial assignment:\nForward prediction Choose an assignment supported on x, b, and m, of the\nform\n(m, b, (x1 , . . . , xn ), , ) .\nConsistency radius minimization will infer the values for y. Because the\nabove assignment extends to a global section, namely,\n(m, b, (x1 , . . . , xn ), (b + mx1 , . . . , b + mxn ), (m, b, (x1 , . . . , xn ))) ,\nconsistency radius minimization does not require constraints in this case.\n21", ".", "y", "y1", "y = mx + b\nb + mx~1\nunconstrained\nconsistency\nradius", "b + mx1\nconstrained\nconsistency\nradius", "x\nx1", "x~1", "Figure 4: Geometric meanings of the terms contributing to consistency radius\nin Equation 13.\nBackward prediction Choose an assignment supported on y and b, and m,\nof the form\n(m, b, , (y1 , . . . , yn ), ) .\nConsistency radius minimization will infer the values for x. If m = 0, this\nalways results in a global section,", "(m, b, ((y1 b)/m, . . .", ", (yn b)/m, (y1 , . . .", ", yn ), (m, b, ((y1 b)/m, . . .", ", (yn b)/m)) ,\nso consistency radius minimization does not require constraints. If m = 0\nthen the minimizers of consistency radius all have the same consistency\nradius, and are assignments of the form\n(0, b, (x1 , . .", ". , xn , (y1 , . .", ". , yn ), (0, b, (x1 , . .", ". , xn ))) . Noting that the two copies of the x variable are always identical, applying\nconstraints does not change the result.", "Regression (model fitting) (Details above, included for completeness here. )\nChoose an assignment supported on x and y, of the form\n( , , (x1 , . .", ". , xn ), (y1 , . .", ". , yn ), ) . Consistency radius minimization will infer the values for b and m.", "As\nnoted above, without constraints consistency radius minimization solves\ntotal least squares, while constraints are necessary to recover classical\nregression. 22", "Constraints", "pr1", "...", "pr3", "pr2", "prn+2", "f1 f2", "n", "Assignment support", "fn", "...", "Figure 5: Modification to the sheaf in Figure 3(d) to allow for missing data. Hybrid versions of the above problems can also be addressed. Assignments are populated stalk-wise, so the sheaf in Figure 3(d) explicitly\nrequires that we have access to all of the n data points, since the stalks for x\nand y are each Rn .", "If there is missing data, a different sheaf construction is\npossible, in which each separate component of x and y is given its own stalk. Figure 5 shows the resulting construction. The fk restriction maps appearing in Figure 5 are the individual components\nof the f restriction map in Figure 3(d), namely given Equation (11),\nyk = fk (m, b, (x1 , .", ". . , xn )) = mxk + b.", "The set of global assignments for the sheaf in Figure 3(d) is the same as\nthat for the sheaf in Figure 5, but its components are delineated differently. A\ntypical global assignment a for the sheaf in Figure 5 is given by", "a := m, b, x1 , . . .", ", xn , y1 , . . .", ", yn , m,\ne eb, x\nf1 , . . .", ", x\nfn ,\nwhere the main difference between the above and Equation (12) is in the placement of parentheses. The consistency radius for a global assignment in both\nsheaves is given by exactly the same formula. As in the previous sheaf, we can\nexpress the linear regression problem as a consistency radius minimization problem, in which a local assignment supported on the xk and yk variables (shown\nby the darkest shaded regions in Figure 5) is extended to a global assignment,\nsubject to the constraint that each of the copies of the duplicated variables are\nidentical (shown by the three lighter shaded regions in Figure 5).", "But now, if\nthere is a missing xk or yk value, this can simply be excluded from the support\nof the initial assignment, leaving the specification of the task as a consistency\nradius minimization unchanged. Feedback connections are easily represented in all of the frameworks under\nconsideration. Moreover, depending on the set of variables that are permissible,\nthe resulting sheaf will or will not have global sections (Definition 9).", "23", "X", "x", "x\nout", "f", "g", "g", "id", "X", "X", "id", "f", "in", "g", "f", "in", "out", "y", "y", "X", "(a)", "(b)", "(c)", "Figure 6: Feedback connections can be handled: (a) a (D)SEM model with\nfeedback, (b) its netlist, (c) its sheaf representation. Consider the setting shown in Figure 6:\nX = R, f (x) = x, g(x) = x (Linear SEM) global sections occur whenever the\ntwo variables have the same value. X = R, f (x) = x, g(x) = x (Linear SEM) the only global section is for both\nvariables to be 0.", "X = R, f (x) = 1 x, g(x) = x (Affine, nonlinear SEM) The only global section is for both variables to take the value 1/2. X = Z, f (x) = 1 x, g(x) = x (Discrete values) No global sections exist. Feedback will play an important role in defining a sheaf to model autoregressive timeseries in Section 3.", "4. With the preliminary intuition established by the previous two examples, we\nare now in a position to discuss the general translation algorithm. Definition 13.", "If we have a netlist N , we build the netlist sheaf on the Alexandrov topology of the preorder of its netlist graph of N . The stalk on each net\nis the set of variables for that net. The stalk on each part is the product of\nits input ports.", "The restriction from a part to a net along an input port is the\nprojection function for the corresponding variable set. The restriction from a\npart to a net along an output port is the function that computes the output\nvariable from the set of input variables. It is often useful to have individual observations on their own stalks, like we\ndid in Figure 5.", "The following modification to Definition 13 allows for missing\ndata in general. Definition 14. Starting with a netlist sheaf as defined in Definition 13, add\nan additional element to the preorder of the netlist graph for each observation\nof each variable.", "These elements are located above their respective variables in\nthe preorder. The restriction map from each variable to each observation is the\nprojection that selects the corresponding observation from its parent timeseries. 24", "x1, ... xn\nS\nin", "a1, ... ak", "coef", "LCF(k)", "pr2\nk", "S", "out", "yn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "(a)", "pr1", "k", "S\n(b)", "Figure 7: A linear causal filter LCF(k) with a sliding window size k as (a) netlist\nwiring hypergraph and (b) netlist sheaf.\nTheorem 6. Variable values on the netlist correspond bijectively to DSEM\nsolutions and to global sections.\nProof. (see also [34][Prop. 6]) There is a direct correspondence between the\nvalues of variables on the nets and the nodes in the DSEM. If these are values\ncorrespond to a solution, then they directly imply consistency with the restriction maps.\nMoreover, according to [35, Thm. 1] there is stability in consistency radius\nwhen we perturb away from a consistent set of variables. This is classical in the\ncase of the linear regression example, because the linear regression coefficients\nm and b are stable with respect to perturbations in the data variables x and y.", "3.4", "Sheaves modeling autoregressive timeseries", "Autoregressive timeseries are sequences . . .", ", x0 , x1 , . . .", "that obey an equation of\nthe form\nxn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nfor some fixed a1 , . . .", ", ak . We say that such a sequence is AR(k) autoregressive. Autoregressive timeseries can be modeled using the graphical framework being\ndeveloped in this chapter by the use of feedback connections.", "It is easiest to see how the construction of autoregressive timeseries works by\nstarting with a one-step delayed Linear Causal Filter with sliding window size k\n(which we write as LCF(k) for short in diagrams). Like the linear regression\nexample from the previous section, a variable x is considered an explanatory\nvariable that predicts the values of a response variable y. This prediction is\ngiven by\nyn = a1 xn 1 + a2 xn 2 + + ak xn k\nwhere the a1 , .", ". . ak are constants.", "We can realize this equation as a netlist with an input for x, an input for a,\nand an output for y shown in Figure 7(a). Using Definition 13, we obtain the\n25", "... x1, ... xn", "s", "out", "in", "identity", "LCF(k)", "coef", "a1, ... ak", "out", "in", "id", "pr2", "s", "k s", "id\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "s", "(a)", "(b)", "pr1", "k", "Figure 8: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries. netlist sheaf shown in Figure 7(b), where S is the set of infinite sequences of\nreal numbers. To handle autoregressive timeseries, we merely need to consider the pair of\nequations\n(\nyn = a1 xn 1 + a2 xn 2 + + ak xn k ,\nxn = yn .", "This is implemented as a netlist with two parts and a feedback connection,\nas shown in Figure 8(a), where again S is the set of infinite sequences of real\nnumbers. The linear causal filter part is the same as before, but the identity\npart implements the second equation above. Error terms are not explicitly\nmentioned, because they are accounted for in the consistency radius calculation\n(Equation (10)).", "The associated netlist sheaf is shown in Figure 8(b). Again, consistency\nradius measures how well the data x fit the model given with coefficients a. Following a theme already present in the linear regression example, there is\nduplication of data in the sheaf model.", "Indeed, the values of x are effectively\nduplicated in four places: the x and y = x variables, and in the two parts. Once again, if we consider an assignment supported on the two variables (with\nthe same values on each! ), minimizing consistency radius will infer the values\nof the a coefficients.", "Once again, if we run an unconstrained optimization, this\nassumes that some uncertainty is permitted in the values of x. When the timeseries are finite in length, the equation defining an AR(k)\nsequence cannot represent any of the first k time steps. Therefore, instead of\nthe identity part in Figure 8, the sheaf for an AR(k) sequence of length n must\ncrop off the first k components of the vector in the stalk, resulting in a sequence\nof length n k.", "The resulting construction is shown in Figure 9, where we note\nthat a slight abuse of definition occurs in Figure 9(a) because the two outputs\nare connected to each other. While this means that the netlist is not valid as\nsuch, the sheaf constructed in Figure 9(b) correctly represents an autoregressive\nsequence. Global sections of the sheaf in Figure 9(b) are precisely the AR(k)\nsequences of length n.", "26", "x1, ... xn\n n\nin", "in", "crop", "LCF(k)", "a1, ... ak", "coef", "out", "out", "id", "pr2", "n", "k n", "prk..n\nxn = a1 xn-1 + a2 xn-2 + ... ak xn-k", "n-k", "(a)", "(b)", "pr1", "k", "Figure 9: A (a) netlist and (b) a sheaf that encodes an autoregressive timeseries.", "n", "in", "k n", "DietCope_lag", "Copepods\nin", "pr2", "out", "id", "crop", "n\nh", "DietCope_block\nout", "LCF(k)\nprk..n", "n - k", "n", "Diet_Cop\n(a)", "(b)", "Figure 10: Modification to Figure 1(d) to support autoregressive timeseries,\nshown for the Copepods variable: (a) netlist wiring hypergraph, (b) sheaf diagram. This modification is performed for each variable in Figure 1 resulting in\nFigure 13.", "27", "Autoregressive sequences can be modeled in the sheaf shown in Figure 1(d),\nour ecological example. All that is needed is a modification to each variable in\nthe netlist to ensure that each variable is an autoregressive sequence. Specifically, each of the input variables for each of the parts in the netlist shown in\nFigure 1(b) must be duplicated to represent a lagged copy of the variable, and\nthere must be a new part added for each variable to perform the autoregression\nitself.", "As in Figure 9, each original variable gets wired to the input of the corresponding LCF part. The duplicated (lagged) input on each preexisting part\nis cropped to be only the most recent samples (since the timeseries is finite),\nand then that is what is attached to the output port of the LCF part. The\ntransformation that is required for the Copepods variable is shown in Figure 10.", "4", "Sheaf encoding of the Bering Sea", "We now return to the ecological DSEM example introduced in Section 2. 2, and\nrefer the reader to Figure 1. The reader is directed to [36] for the software that\ngenerates the sheaf results presented in this section.", "The DSEM is shown in Figure 1(a), its corresponding netlist wiring hypergraph is shown in Figure 1(b), its netlist graph is shown in Figure 1(c), and its\nnetlist sheaf is shown in Figure 1(d). The netlist sheaf in Figure 1(d) does not express the path coefficients as\nvariables, as they are instead hard coded within each part. Nevertheless,\nif the path coefficients are known (for instance, they can be taken from [46]),\nthen the sheaf model can be used to predict the values of each of the variables,\nstarting from SeaIce and Spawners.", "If we apply the modification to the sheaf\nto require AR(1) timeseries so that missing data values are interpolated, and\nuse the path coefficients stated in [46] (see Table 3), the resulting timeseries are\nshown in Figure 11. The DSEM was constrained to fit the measurements exactly, whereas the\nsheaf had no such constraints applied. Where the sheaf differs from the measurements, the extent of that difference is a measure of the uncertainty in the\nvalue of the variable at the given time.", "This uncertainty is composed of both\nthe measurement and exogenous errors; the sheaf model does not distinguish\nbetween the types of error. Moreover, where there are no measurements available (especially for the earlier measurements), the DSEM reports the expected\nmean. The sheaf predictions are typically close to these mean values.", "Nevertheless, there is close agreement throughout. This is not unexpected, because\nboth the sheaf and the DSEM approach are approximations to the same DSEM\nsolution. There are some differences on the behavior of the earlier inferred data,\nbecause many of the observations are missing there.", "In these regions, the sheaf\ntends to yield somewhat less variable predictions than the DSEM (except in the\ncase of the Krill variable). As noted earlier, we will compute consistency radius using the Euclidean p =\n2 norm. Lacking other information, we chose to weight the terms in Equation\n(10) equally.", "The consistency radius of the assignment after minimization is", "28", "SeaIce", "ColdPool\nln (ColdPool ) [ln(km2 )]", "ln (SeaIce) [ln(km2 )]", "1\n0.5\n0.0\n 0.5\n 1.0", "0\n 1\n 2", "2018 ColdPool", "3", "DietCopepods\nln (DietCopepods) [ ]", "3", "ln (Copepods) [ln(count/m )]", "Copepods\n2.5\n0.0\n 2.5\n 5.0", "1\n0\n 1\n 2", "DietKrill", "3", "ln (Krill ) [ln(count/m )]", "Krill\nln (DietKrill ) [ ]", "0.5\n0.0\n 0.5", "2016 Krill", "1.0", "0.5\n0.0\n 0.5\n 1.0", "Spawners", "Survival\nln (Survival ) [103 count/kg]", "Spawners[106 kg]", "4\n3\n2\n1\n1960", "1970", "1980", "1990", "2000", "2010", "2020", "measurement", "2\n1\n0\n 1\n 2\n1960", "DSEM", "1970", "1980", "1990", "2000", "2010", "2020", "sheaf", "Figure 11: Comparison between the DSEM output and the sheaf with hardcoded path coefficients shown in Figure 1(d) and AR(2) timeseries. The DSEM\nwas constrained to fit the measurements exactly, whereas the sheaf had no such\nconstraints applied.", "29", "Copepods_pc", "Copepods", "n\npr2", "pc", "in", "DietCope_block", "pr1", "out", "Diet_Cop", "n\ng~1\n n", "(a)", "(b)", "Figure 12: Modification to the netlist to include path coefficients and constants\nas an input. 11. 9.", "Since this is not zero, this means that the fit between the data and the\nmodel is not perfect. While the DSEM fits the data for maximum likelihood,\nthe sheaf fits for minimum inconsistency. This difference in optimization task\nresults in the observed differences between the sheaf and the DSEM.", "Taking a cue from Figure 3 in the previous section, we can break out path\ncoefficients as separate variables so that they can be adjusted or estimated. Figure 12 shows how one of the parts in the netlist shown in Figure 1(b) can\nbe modified so that its path coefficients are inputs. To handle missing data, we\napply Definition 14 to the netlist sheaf, which results in Figure 13.", "Using the sheaf shown in Figure 13, we can infer the path coefficients and\nautoregressive coefficients by consistency radius minimization. Specifically, we\nconstruct an assignment supported only on the values of the variables that correspond to observations present in the data. Then, when we minimize consistency\nradius, the values of the path coefficients, autoregressive coefficients, and any\nmissing observations will be inferred.", "The resulting global assignment has a\ncomplete timeseries no missing observations for each variable as well as path\ncoefficients and autoregressive coefficients. Because the approach explained in\nSection 2. 1 uses a different strategy for approximating solutions to the problem\nposed by the DSEM, the inferred path coefficients and missing observations will\nbe somewhat different from those inferred by the sheaf.", "There are some differences between the sheaf and the measurement data. The contributions to consistency radius are not uniformly distributed over the\nsheaf. Some of the inconsistency is due to disagreements between the measurements and the DSEM graph model, and some of the inconsistency is due to\nthe fact that the measurements are not AR(1) timeseries.", "This is visually apparent in Figure 13, where it is shown that the two largest contributors to the\nconsistency radius are\n1. the autoregression cell for Copepods (labeled Copepods lagvar ), and\n2. the year 2018 observations of ColdPool (labeled 2018 ColdPool ).", "The second of these is easier to interpret. We should suspect that the 2018\nobservation of ColdPool is an outlier (in the L2 sense) from what was expected\n30", "SeaIce\nSeaIce_lag\nSeaIce_lagvar", "ColdPool_block", "ColdPool_lagvar", "SeaIce_pc", "ColdPool_lag\nColdPool", "2018_ColdPool", "ColdPool_Copepods_pc", "ColdPool_Krill_pc\nKrill_block", "Copepods_block", "2016_Krill\nCopepods", "Krill\nCopepods_lag", "Copepods_pc\nDietCopepods_block", "Copepods_lagvar", "Krill_lag", "Krill_pc\nDietKrill_block", "Krill_lagvar", "DietCopepods", "Spawners", "DietKrill\nDietCopepods_pc DietKrill_pc", "DietCopepods_lag", "DietKrill_lag\nSpawners_pc", "DietCopepods_lagvar", "Spawners_lag", "DietKrill_lagvar\nSpawners_lagvar", "Survival_block", "Survival", "cells", "restrictions\nprojection map\nother function (see text)", "inferred variable (shown in Fig.11)\nobserved variable highlighted in Fig.11\npseudometric not present\npseudometric present", "0\n2\n4\nconsistency radius contribution", "Figure 13: The full sheaf for the DSEM described in Section 2.2. Its structure\nreflects the hexagonal backbone shown in the diagrams in Fig. 1. The black cells\nrepresent inferred variables, with the variable names shown in italics. Variable\nnames that are also bold correspond to variables plotted in Fig. 11. White cells\nrepresent variables that are observed. All observed variables except for two are\nnot labeled for clarity. The two that are labeled have their names in white italics\nwith black backgrounds. These variables exhibit relatively large contributions\nto the consistency radius and are highlighted in Fig. 11.\n31", "Source", "Target", "SeaIce\nColdPool\nColdPool\nCopepods\nColdPool\nKrill\nCopepods\nDietCopepods\nKrill\nDietKrill\nDietCopepods\nSurvival\nDietKrill\nSurvival\nSpawners\nSurvival\nConsistency radius\nRuntime (s)", "DSEM [46]\nAR(1)\n0.6\n1.79\n0.18\n0.29\n0.06\n0.15\n0.13\n 0.59\n11.9\n2", "none\n1.68\n4.45\n0.44\n0.32\n0.52\n 0.50\n7.56\n 0.82\n6.60\n2848", "Sheaf\nAR(1) AR(2)\n1.81\n1.78\n4.38\n4.47\n0.38\n0.41\n0.35\n0.36\n0.70\n0.65\n 0.12 0.05\n5.29\n7.19\n 0.65 0.55\n9.48\n9.03\n2637\n2679", "AR(10)\n1.74\n4.17\n0.39\n0.34\n0.56\n 0.32\n5.63\n 0.74\n7.93\n2907", "Table 3: Comparison between path coefficients estimated from the DSEM and\nthe sheaf\nfrom the model, and that these differences may have propagated into other parts\nof the model. This probably explains why the 2018 observations of Krill and\nDietKrill are substantially different from the sheaf predictions in Figure 11. We should interpret the largest contributor to consistency radius as suggesting that the Copepods variable is not well represented by an AR(1) timeseries.", "Notice that the Copepods observations contribute equally to consistency radius,\nsince the small white diamonds encircling the Copepods variable are about the\nsame size. This suggests that it is simply that the assumption of Copepods\nbeing represented by an AR(1) timeseries is faulty, rather than any particularly\nbad observation. Table 3 shows the path coefficients inferred by the DSEM (using maximum\nlikelihood as explained in Section 2.", "2) and by the sheaf (using minimum consistency radius). Table 4 shows the autoregressive coefficients estimated by\nthe sheaf for the AR(1) and AR(2) cases. (The AR(10) case is not shown for\nspace considerations.", ") The DSEM-derived path coefficients were obtained using\nthe assumption of AR(1) timeseries. Several different sheaves were constructed\nwith autoregressive sequences of different window sizes. As a consequence of\nthe construction of consistency radius, minimizing consistency radius infers the\nfollowing information: (1) missing observations in any variable, (2) all path\ncoefficients, and (3) autoregressive coefficients for each variable.", "There is broad agreement about the values of the path coefficients between\nthe sheaves with different autoregressive window sizes, and some agreement\nbetween the DSEM and the sheaves. Since the DSEM does not natively imply\na consistency radius, the consistency radius shown for the DSEM is that for\nthe sheaf using AR(1) timeseries and the hard-coded path coefficients as shown. Because the consistency radius minimization process on that sheaf cannot adjust\nthe path coefficients it can only adjust the missing observation values and the\nautoregressive coefficients the consistency radius is notably higher in this case.", "Some caution in comparing consistency radius across the columns of Table", "32", "Variable\nColdPool\nSeaIce\nCopepods\nKrill\nSpawners\nDietCopepods\nDietKrill", "AR(1)\nlag 1\n0.582\n0.361\n0.828\n0.692\n1.01\n0.886\n0.060", "AR(2)\nlag 1\nlag 2\n0.480\n0.202\n0.287\n0.190\n1.16\n-0.442\n0.308\n0.411\n1.78\n-0.768\n1.68\n-0.924\n0.0596 0.0445", "Table 4: Autoregressive cofficients estimated by the sheaf for AR(1) and AR(2)\nmodels. 3 is needed. The number of terms in the consistency radius is the same for\neach of the sheaves in all but the non-autoregressive case (the fourth column\nfrom the left).", "This is because the autoregressive coefficients and timeseries\nare bundled as shown in Figure 9. Naturally enough, the non-autoregressive\nsheaf s consistency radius contains no terms pertaining to the autoregressive\ncoefficients, and so is expected to be smaller than the others. The sheaf column\nlisted as none means that no autoregressive timeseries assumptions were applied.", "Because with no autoregressive assumptions in play, the resulting sheaf\ndiagram is smaller, consequently the consistency radius is smaller. Interestingly,\nthe consistency radius is smallest for the AR(10) case, which suggests that more\nflexibility in the autoregressive coefficients leads to somewhat better prediction\naccuracy in the measurement data. Runtimes shown in Table 3 are representative when run on an Intel Core\nUltra 7 155U at 1.", "4 GHz with 32 GB RAM. The process was not memory limited\nand consumes less than 500 MB RAM. The sheaf runs roughly 1500 times slower\nthan the DSEM.", "This is because the DSEM solves a sparse linear problem, while\nthe sheaf methodology supports fully nonlinear, non-convex problems. The\nsheaf software does not attempt to detect whether the problem is linear, so the\nconsistency radius minimization is always performed as a nonlinear, non-convex\noptimization problem.", "5", "The topology of subsystems", "Classically, dynamical systems have been studied using the structure of invariant\nsets. These are subsets of the space of variable values that are preserved by the\naction of the dynamical system. This section shows that invariant sets are one\nhalf of a duality pair. We can take two different perspectives of a multi-scale\ndynamical system: invariant sets (which lead to cosheaves) versus subsystems\n(which lead to sheaves).\nWe will establish that a dynamical system induces a cosheaf of invariant\nsets. The cosheaf of invariant sets breaks the global state of the system into\ndifferent regimes of behavior, which are parameterized by the open sets of the\n33", "base space topology. Conversely, there is also a sheaf of subsystems that splits\nthe variables into nested collections that each act independently. We will formalize the topology of subsystems as a finite topological space, by\nusing the Alexandrov topology for a specific preorder (Definition 6).", "Each subsystem corresponds to a preorder element, with composite subsystems hooked\ntogether according to the preorder. The preorder relation decomposes composite subsystems into their component pieces. Intuitively, moving up in the\npreorder yields more abstracted high-level systems.", "This is not entirely compatible with all system decompositions in the literature, so caution is advised! (The intuition of the presentation here is compatible with Kearney et al. [22],\nwhere the system is modeled as a graph.", "In Kearney et al. [22], vertices are the\nloci of state variables, and are above edges in the preorder constructed in that\npaper. Our presentation is also compatible with Steward [43], after transitive\nclosure.", ")", "5.1", "Dynamical systems", "Definition 15. A dynamical system is a continuous bijection f : S S. The\nset S in this case is called the set of states of the dynamical system.", "It is a classical fact that for a fixed timestep, the solutions to a smooth first\norder differential equation of the form (1) induce a dynamical system [44]. As\na consequence, the DSEM, netlist, and sheaf models of the previous sections\nrepresent dynamical systems. Definition 16.", "For a dynamical system f : S S, a subset V S is called\nan invariant set if\nf (V ) V. Corollary 7. If V is an invariant set of f : S S, then f restricts to a\nfunction f : V V .", "Definition 17. Suppose that A B. The inclusion is the function i : A B\nis a function such that i(x) = x for every x A.", "Notice that (i|A) i = i. Dually, a projection is a function p : B A such that p p = p and\np|A = id A . Proposition 8.", "Suppose that U and V are two invariant sets for a dynamical\nsystem f : S S and that U V . Then the following diagram\nU", "f", "i", "V", "f", "/U", "/V", "i", "commutes, where i and i are appropriate inclusion maps, which is to say that\nf i = i f.\n34", "Proof. Suppose that x U . Since U is an invariant set, f (U ) U . However,\nsince U V , x V . Therefore, f (x) V because V is also an invariant set.\nDefinition 18. The category Dyn of dynamical systems has as its objects\ndynamical systems. Each morphism of Dyn is a commutative diagram of the\nform\nf1\n/ S1\nS1\ng", "g", "S2", "f2", "/ S2", "Composition of morphisms is given by composing the g functions.\nProposition 9. Isomorphisms in Dyn are conjugacy classes of dynamical systems.", "5.2", "The cosheaf endomorphism of invariant sets", "The state space of a dynamical system can be decomposed as the (non-disjoint)\nunion of all its invariant sets. This collection of invariant sets of a dynamical\nsystem is also partially ordered by subset inclusion, which means that the collection of invariant sets can be given an Alexandrov topology. A cosheaf can be\ndefined to capture the relationship between an invariant set and the invariant\nsets that contain it.", "To this end, the cosheaf identifies duplicate points within\nthese invariant sets with each other. We begin by observing that the invariance of a collection of subsets with\nrespect to a dynamical system is not necessary to define a cosheaf; it can be\nconstructed generally. Lemma 10.", "Suppose that U 2X is an arbitrary collection of subsets of a set\nX. Consider the inclusion partial order on U, given by U V whenever U V . Define the following precosheaf CU on the Alexandrov topology of the inclusion\npartial order (U , ):\n1.", "CU (U ) = U\n2. CU (U V ) = CU (U V ) : U V via the inclusion map. Then CU is a cosheaf of sets on the Alexandrov topology of the inclusion partial\norder (U, ).", "Proof. Suppose that V U, and that V U is a collection of subsets with\nV = V. We need to establish that the space of global cosections on V is\nidentical to CU (V ) = V .", "The space of global cosections on V is\n! ! G\nG\n[\nCU (W ) / =\nW / =\nW = V = V,\nW V", "W V", "W V", "since the equivalence identifies points that agree on overlaps.\n35", "The above cosheaf construction is functorial, which means that it is compatible with transformations of the underlying sets. In order to establish functoriality, we need to formalize these transformations by defining the class of\nmorphisms for sheaves and cosheaves.\nDefinition 19. Suppose that R is a sheaf on (X, TX ), S is a sheaf on (Y, TY ),\nand that f : (X, TX ) (Y, TY ) is a continuous function. A sheaf morphism\nm : R S is a collection of maps mU : R(f 1 (U )) S(U ) for each U TY\nsuch that the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nR(f 1 (U ) f 1 (V ))", "mV", "/ S(V )\nS(U V )", "R(f 1 (U )) mU / S(U )", "Dually, if R is a cosheaf on (X, TX ), and S is a cosheaf on (Y, TY ), a cosheaf\nmorphism m : R S is a collection of maps mU : R(f 1 (U )) S(U ) such\nthat the following diagram commutes whenever U, V TY and U V ,\nR(f 1 (V ))\nO", "mV", "/ S(V )\nO", "R(f 1 (U ) f 1 (V ))", "S(U V )", "R(f 1 (U ))", "mU", "/ S(U )", "With the definition of morphisms in hand, we can now establish that the\ncosheaf construction in Lemma 10 is functorial.\nLemma 11. There is a functor Top CoShv that takes a topological space\n(X, T ) to a cosheaf C(X,T ) of sets on (X, T ) via C(X,T ) (U ) := U and C(X,T ) (U \nV ) is the inclusion U , V .\nProof. First, we observe that Lemma 10 establishes that C(X,T ) is a well-defined\ncosheaf on (X, T ).\nSuppose that f : (X, TX ) (Y, TY ) is a continuous map. This lifts to\na cosheaf morphism F : C(X,TX ) C(Y,TY ) . Suppose that U V are two\nopen sets in Y . Then we have that f 1 (U ) f 1 (V ) are two open sets in X.\nTherefore, the following diagram commutes\nC(X,TX ) (f 1 (U )) = f 1 (U )", "FU :=f |U", "C(X,TX ) (f 1 (U ) f 1 (V ))", "C(X,TX ) (f 1 (V )) = f 1 (V )", "/ C(Y,T ) (U ) = U\nC(Y,TY ) (U V )", "FV :=f |V", "/ C(Y,T ) (V ) = V\nY", "which establishes definitions for the component maps of F , and therefore that\nF is a cosheaf morphism.\n36", "Now suppose that we have two continuous maps f : (X, TX ) (Y, TY ) and\ng : (Y, TY ) (Z, TZ ). We must show that the corresponding composition of\ncosheaf morphisms G F is the equal to the one induced by (g f ). This follows\nimmediately because the components maps of the cosheaf morphism G F are\nsimply restrictions of the composition (g f ).", "Suppose that f : S S is a dynamical system. The invariant sets of f are\nindeed a collection of subsets, which are partially ordered by inclusion. Therefore, Lemma 10 establishes that there is a well-defined cosheaf S of invariant\nsets of f .", "Proposition 12. A dynamical system f : S S induces an morphism m :\nS S on the cosheaf of invariant sets, and for which the induced map on\nglobal cosections is mS = f . Proof.", "Suppose that U is an invariant set of f . Let mU : U U be the\nrestriction of f to U . If U V are two invariant sets, then Proposition 8\nimplies that\nU", "mU =f", "/U", "i", "V", "mV =f", "/V", "i", "commutes, where i is the inclusion map. It is immediate that this is exactly\nthe condition that the m maps are the components of a cosheaf morphism.\nMoreover, since S is itself an invariant set, the proof is complete.", "5.3", "Subsystem decomposition sheaf", "Rather than carving up the state space into different regimes of behavior, we\ncan instead carve it into non-interacting collections of variables. In this way, we\narrive at the subsystem sheaf instead of the invariant set cosheaf. The global\nsections combine variables together into vectors, whereas global cosections paste\nsubsets of values together.\nDualizing the condition for an invariant set yields the condition for a subsystem. Suppose that f : S S is a bijection and that U S is an invariant\nset for f . If i : U S is the inclusion map, then the diagram at left below\ncommutes:\nf\nf\n/S\n/S\nSO\nS\nO\ni", "U", "p", "i", "f |U", "B", "/U", "p", "g", "/B", "Dually, the diagram at right above captures the situation where B is a subsystem\nof f .", "37", "Definition 20. If f : S S is a dynamical system, a subsystem is a pair (g, p)\nconsisting of a dynamical system g : B B and a surjection p : S B such\nthat p f = g p. We will call p the subsystem projection.", "When p is clear from\ncontext, we will often say g is a subsystem of f . We can think of the function g as a dynamical system in its own right. The idea of a subsystem is neatly compatible with the DSEM construction.", "As will be shown later in Corollary 21, when the DSEM graph is acyclic, the\nsubsystems can be read off directly. For the moment, a few examples will\nbuild the necessary intuition. Example 2.", "Consider the DSEM with two variables A and B, given by the\ngraph with one edge A B. The variable A is a subsystem on its own, whereas\nB cannot be a subsystem on its own because its value cannot be predicted from\nB alone. As a result, there are two nested subsystems: {A} and {A B}.", "To see this explicitly, suppose that the values of A are given by the timeseries\n{an } and the values of B are given by the timeseries {bn }, with the prediction\nof B from A given by the formula\nbn+1 = β(an , an 1 , . . .", "). The dynamical system implied by this DSEM is represented by shifting the\ntimeseries by one timestep. Specifically, the dynamical system is given by the\nfunction f : A B A B given by\nf (.", ". . ,an , an 1 , .", ". . , .", ". . , bn , bn 1 , .", ". . )\n= (.", ". . , an+1 , an , .", ". . , .", ". . , β(an , an 1 , .", ". . ), β(an 1 , an 2 , .", ". . ), .", ". . ).", "Because of this formula, it should be clear that {B} cannot be a subsystem\nbecause the values of the {bn } timeseries depend on the values of {an }. Under\na projection that removes the {an } from the domain, the values of {bn } cannot\nbe determined. The subsystem {A} arises using the subsystem projection p : A B A,\nnamely\np(.", ". . , an , an 1 , .", ". . , .", ". . , bn , bn 1 , .", ". . ) = (.", ". . , an+1 , an , .", ". . ).", "The subsystem dynamical map g : A A is simply\ng(. . .", ", an , an 1 , . . .", ") = (. . .", ", an+1 , an , . . .", "). Verification that (g, p) is a subsystem is then simply a calculation,\n(p f )(. .", ". , an , an 1 , . .", ". , . .", ". , bn , bn 1 , . .", ". )", "= p(. . . , an+1 , an , . . . , . . . , β(an , an 1 , . . . ), β(an 1 , an 2 , . . . ), . . . )\n= (. . . , an+1 , an , . . . )", "= g(. . . , an , an 1 , . . . )\n= (g p)(. . . , an , an 1 , . . . , . . . , bn , bn 1 , . . . ).\n38", "Example 3.\n?B\nA", "C", "Following the logic of Example 2, the subsystems are {A}, {A B}, {A C},\nand the original system.\nExample 4. Consider the DSEM with three variables A, B, and C given by\nthe graph\nA", "?C", "B\nFollowing the logic of Example 2, the subsystems are {A}, {B}, and the original\nsystem. Notice that {C} cannot be a subsystem on its own because its values\nare determined by both A and B.\nWhen a dynamical system is described by a DSEM with feedback, there are\noften fewer subsystems because the values of the variables cannot be determined\nin isolation.\nExample 5. Consider the DSEM on variables A and B given by the graph\n)", "Ah", "B", "(See also Figure 6 for the sheaf model.) In this case, the only subsystem is the\nentire system, because the values of A cannot be determined without knowing\nB, and conversely the values of B cannot be determined without knowing A.\nLinear systems are special because invariant sets and subsystems reduce to\nthe same thing, as the next example shows.\nExample 6. Let V be a finite dimensional vector space and f : V V be a\nlinear isomorphism. If we use the usual Euclidean norm on V , f is continuous,\nso it is also a dynamical system. Subsystems and invariant subspaces of f are\nin bijective correspondence.\nTo see this, suppose that v V is an eigenvector for f , namely\nf (v) = λv", "39", "for some λ. Then the subspace spanned by v is an invariant set. Conversely,\nevery invariant set of f is a linear subspace, spanned by a set of eigenvectors\n(possibly with complex eigenvalues).\nSince V was assumed to be finite dimensional, every subspace W V also\nhas an associated orthogonal projection prW : V W . If W is an invariant set\nfor f , then (f |W, prW ) is a subsystem. To see this, suppose that v V , which\ncan be written as the decomposition u + w, where w W and prW (u) = 0.\nBecause f is a linear isomorphism, the assumption on u means that prW (f (u)) =\n0. All that remains is to verify that the definition of subsystem holds,\n(prW f )(v) = prW (f (u + w))", "= prW (f (u) + f (w))", "= prW (f (u)) + f (w)\n= f (w)\n= (f |W ) (w)", "= (f |W ) (prW (u + w))\n= (f |W prW )(v).", "Lemma 13. The relation is a subsystem of is a preorder, or in other words\na reflexive, transitive relation.\nProof. Suppose that f : S S is a dynamical system. Reflexivity follows\nimmediately by taking (f, id S ) as a subsystem. For transitivity, suppose that\n(g2 , p2 ) is a subsystem of f , and that (g1 , p1 ) is a subsystem of g2 . That is, we\nhave the commutative diagram\nf", "S\np2", "p1 p2", "B2", "p2\ng2", "p1", "B1", "/S", "/ B2", "p1 p2", "p1", "g1", "/ B1", "so that (g1 , (p1 p2 )) is a subsystem of f .\nIntuitively, the preorder specifies how data can flow from one subsystem to\nthe next. If (g1 , p1 ) is a subsystem of (g2 , p2 ), then each variable in (g2 , p2 ) is\nalso a variable of (g1 , p1 ). As a result, the state of g1 can influence the state of\ng2 .\nExample 7. Consider the dynamical system f : Z3 Z3 given by\nf (x, y, z) := ((1 x), y(1 x) + zx, z(1 x) + yx).", "40", "This has a nontrivial subsystem pr1 : Z3 Z, since the map\ng(x) := 1 x\nmakes the following diagram commute\nZ3\npr1", "Z", "f", "/ Z3\npr1", "g", "/Z", "In this case, the x variable in the subsystem acts as an input to the overall\nsystem, even though its behavior is isolated from the rest of the system.\nIt is not necessarily the case that subsystems are invariant sets.\nExample 8. Consider the dynamical system f : R2 R2 , given by f (x, y) :=\n(x, y+1). Consider the subset B = {(x, 0) : x R}. This set yields a subsystem,\nsince the following diagram commutes\nR2", "f", "p", "B", "/ R2\np", "id", "/B", "where p(x, y) = (x, 0), even though the set B is not an invariant set.\nHowever, conversely, invariant sets of subsystems do determine invariant sets\nof their parent system.\nLemma 14. Suppose that f : S S is a dynamical system with g : B B is\na subsystem with subsystem projection p : S B. If V B is an invariant set\nof g, then p 1 (V ) is an invariant set of f .\nProof. The hypotheses posit a commutative diagram of the form\nS", "f", "p", "B", "/S\np", "g", "/B", "Suppose that x p 1 (V ) S. We have that p(f (x)) = g(p(x)) via the\ncommutative diagram above. Noting that p(x) V by construction, and that\nV is an invariant set of g, this means that g(p(x)) V . Thus, p(f (x)) V , so\nf (x) p 1 (V ), which establishes that p 1 (V ) is an invariant set of f .", "41", "Lemma 15. Suppose that f : S S is a dynamical system and that Y S\nis an invariant set for f . If g : B B is a subsystem of f with subsystem\nprojection p, then g is also a subsystem of f |Y .\nProof. Suppose that i : Y S is the inclusion map. The hypotheses state that\nthe diagram of solid arrows below commutes:\n(f |Y )", "Y", "/Y", "i", "i", "/S", "f", "S\np", "p", "B", "/B", "g", "The conclusion follows by completing the diagram s dashed arrows with the\ncomposition p i as the subsystem projection for g as a subsystem of f |Y .\nA related statement to Lemma 15 could consider the conditions under which\na subsystem of an invariant set lifts to a subsystem of the entire system. Diagrammatically, this consists of a situation where the subsystem projections\ndefined by the dashed arrows in the diagram below could be constructed:\n(f |Y )", "Y", "/Y", "i", "i\nf", "S", "B", "g", "/S", "/B", "Therefore, when studying a dynamical system, one will often encounter problems of the following form. Question 1. When do lifts to the dashed arrows in the diagram above exist?", "Answers to this question relate closely to the expected behavior of systems\nwhen they are rewritten with new variables. This routinely happens with compiled software, as the next example shows. Example 9.", "Suppose that X represents the state space of a computer, perhaps a Turing machine. The design of the computer and physical laws yield a\ndynamical system f : X X. For this example, f is not bijective.", "The way that the computer is used is that the user loads an executable\nand then runs it. The initial state of the executable is a point within a subset\nU X. The user does not have control over the entire state of the machine,\n42", "but rather can constrain it to a smaller portion of the state space. It makes\nsense to require that U is an invariant set, which means that not only the initial\nstate is included, but all possible future states as well. Therefore, the execution\nof the executable is completely determined by the commutative diagram\nU", "f |U", "X", "/U", "f", "/X", "As an example in PDP-11 assembly, we could have\nU = {PC {0, 1}, memory = {0 : ADD R1,R2, 1 : HALT}},\nwhere all values of the unspecified parts of the machine state (other registers,\nthe rest the memory) are included in U . If the program counter PC is initialized\nto 0, the program will execute the instructions at 0 and 1, and then will halt. Evidently, if PC = 1, then the program halts immediately.", "No modifications\nto memory can occur given an initialization with U , and PC cannot be moved\noutside of those two instructions. This ensures that f (U ) U is indeed an\ninvariant set. We might instead imagine that the executable specified by U was the result\nof a compiled, high-level program.", "Such a program would necessarily be of the\nform g : Y Y , where Y holds the values of the two registers R1 and R2. For\na PDP-11, this means Y = ({0, 1}16 )2 , and\ng(x, y) := (x, x + y),\nwhich is to say that R1 is unchanged by the program, and R2 takes the sum of\nR1 and R2. The compilation process essentially ensures that we have the following commutative diagram\nU", "f |U", "q", "Y", "g", "/U", "/Y", "q", "where the q maps select the two registers R1 and R2 from the entirety of the\nmachine state.\nNotice that we may write q = p , where is the inclusion of U , X, and\np still selects the two registers R1 and R2 from the entirety of the machine state.\nSince the machine state is very large in comparison to U , the following diagram\ndoes not commute:\nf\n/U\nX\np", "Y", "g", "43", "/Y", "p", "Values of X for which the commutativity fails egregiously are instances of weird\nmachine states [13].\nHowever, when the operating system loads an executable, there are conventions about initialization. This helps to avoid weird machine states. We can\nformalize this idea by way of an initialization function i : Y U that is a right\ninverse to q, namely q i = (p ) i = id Y . This means that we have the\nfollowing commutative diagrams\nUO", "f |U", "i", "Y", "g", "/U", "/Y", "f", "XO\nq", "i", "Y", "g", "/X", "/Y", "p", "For instance, in the example PDP-11 program, we could use\ni(x, y) := {PC = 0,", "R1 = x,\nR2 = y,", "R[3-6] = 0,\nmemory = {0 : ADD R1,R2, 1 : HALT, [2-] : 0}},\nNotice that since i does not have the ability to change the program counter PC,\nthe following diagram does not commute\nUO", "f |U", "/U\nO", "i", "Y", "i\ng", "/Y", "Inspired by Example 9, suppose that we have a commutative diagram\nXO", "f", "i", "Y", "g", "/X", "/Y", "p", "where i is injective, p is surjective, and f , g are bijective.\nThis leads to another question that is often of interest when studying system\nbehaviors.\nQuestion 2. Under what conditions does\nX", "f", "p", "Y", "g", "44", "/X", "/Y", "p", "commute? Clearly if g is bijective, then a sufficient condition is that p = g 1 \np f . It is probably the case that p i = id Y in most applications, but it is\nunlikely to be the case that i p = id X .\nLemma 16. The subsystem preorder is a meet-semilattice. That is, if we have\ntwo subsystems fi : Si Si for i = 1, 2 of a dynamical system f : S S,\nthere is a common subsystem f3 : S3 S3 of both of them (which might be\ntrivial) that satisfies the following universal property. If f4 : S4 S4 is another\ncommon subsystem of f1 and f2 , then f4 is a subsystem of f3 .\nProof. We start with two subsystems of a common dynamical system f : S S,\nso that we have a commutative diagram\nSO 1", "f1", "/ S1\nO", "p1", "p1", "S", "/S", "f", "p2", "S2", "p2", "f2", "/ S2", "We want to construct a subsystem of all three of these f3 : S3 S3 , that is as\nlarge as possible. Realize that what is needed to satisfy the universal property\nis a definition for the dashed arrows in\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "/ S3", "such that this diagram is a colimit.\nSince each of the Si are sets, there is a standard colimit construction, namely\nS3 = (S1 S2 )/ where x y if x S1 , y S2 such that there is a z S with\np1 (z) = x and p2 (z) = y. The colimit condition implies that when we apply\nthis construction twice, there is a unique f3 completing the diagram below\nS", "p1", "p 3", "p2", "S2", "/ S1", "p \n3", "f1", "/ S3", "S1\nf3", "f2", "S2", "45", "p \n3", "p 3", "/ S3", "Proposition 17. Restrict attention to f : S S being a (not necessarily\nlinear) bijection on a vector space S, and require that the subsystem projection\np : S B for each subsystem (g, p) of f is a linear surjection. In this case,\nthe relation is a subsystem of is also antisymmetric up to conjugacy by linear\nisomorphisms.\nAs a result, data feedback loops are confined to happen within a given subsystem.\nProof. Suppose that (g2 , p2 ) is a subsystem of g1 : B1 B1 , and that (g1 , p1 )\nis a subsystem of g2 : B2 B2 , so that we have the commutative diagram\nB1", "g1", "p2", "B2", "p2", "g2", "/ B2", "g1", "/ B1", "p1", "B1", "/ B1", "p1", "Since p1 and p2 are surjective linear maps, this means that (p1 p2 ) : B1 B1\nis a linear surjection. Since it also evidently preserves dimension, it must be a\nlinear isomorphism. Because both p1 and p2 are surjective, this implies that both\nmust also be injective. Hence both p1 and p2 must also be linear isomorphisms,\n 1\nwhich establishes that g2 = p2 g1 p 1\n2 and g1 = p1 g2 p1 as claimed.\nExample 10. There is no function h that will make the diagram below commute\nZ2\nid", "ZO 2", "f", "/ Z2\nid", "h", "/ Z2\nO", "id", "id", "Z2", "g", "/ Z2", "where\nf (x, y) = (x, 1 x),\nand\ng(x, y) = (y, y).", "46", "There is also no function h that will make the diagram below commute\nZ2\npr1", "ZO", "f", "/Z\nid", "h", "/Z\nO", "pr2", "Z2", "id\ng", "/Z", "where\nf (x, y) = 1 x,", "and", "g(x, y) = y. Suppose that f : S S is a dynamical system in which S is a vector\nspace and the subsystem projections are all linear surjections, as required by\nProposition 17. Let (B, ) be the collection of all subsystems of f , with the\npartial order established by Lemma 13 and Proposition 17.", "Each element of B\nis a pair (gB , pB ) where gB : B B is a bijection and pB : S B. For brevity,\nif g1 is a subsystem of g2 , which is to say that there is a p1,2 : B2 B1 such\nthat p1 = p1,2 p2 , we write (g1 , p1 ) (g2 , p2 ). Definition 21.", "Define the sheaf Ff of subsystems of f according to the following recipe:\nStalks Ff ((gB , pB )) := B, and\nRestrictions Ff ((g1 , p1 ) (g2 , p2 )) := p1,2 . Even if the subsystem projections are not linear surjections, the Alexandrov\ntopology on the subsystem preorder bundles together all collections of subsystems that participate in cycles. Without the conclusion of Proposition 17, the\nstalks of Ff are not necessarily well defined, since there is no guarantee that\nthe subsystems of a given cycle have the same state spaces.", "Lemma 18. For a dynamical system f : S S, the space of global sections of\nFf is precisely S. Proof.", "First of all, notice that id S : S S meets the criteria for a subsystem. We merely need to verify that the definition of global sections for Ff doesn t\nconflict with this. The space of assignments for Ff is\nM\nM\nFf (p) =\nB.", "p:S B subsystem", "p:S B subsystem", "Suppose that we have a global section s. On the other hand, if (gB , pB ) \n(f, id S ), then\n(Ff ((gB , pB ) (f, id S ))) (s(S)) = pB (s(S)) = s(B).\n47", "Therefore, the value of s on the subsystem id S : S S determines the values\nof s on every other subsystem. Proposition 19. A dynamical system f : S S induces an endomorphism on\nthe sheaf of all subsystems, and for which the induced map on global sections is\nf.", "Proof. This follows immediately from the definition, as soon as we notice that\nfor a subsystem p : S B, the g map guaranteed by the definition is the\ncorresponding component map for the sheaf morphism. In short, a multi-scale discrete dynamical system can be encoded as component dynamical systems on some (or all) of the stalks of a sheaf S via self maps\nfx : S(x) S(x).", "One may also consider the action of different semigroups on\nstalks to model continuous dynamical systems. We are now ready to establish the main result of this section, which relates\nthe sheaf of subsystems of a DSEM to its graph representation. As we have seen\nin Example 5, feedback loops in the DSEM graph must be confined to being\nentirely within a subsystem.", "Because we can collapse all feedback loops in an\narbitrary directed graph to obtain an acyclic graph, we will assume that the\nDSEM graph is acyclic without loss of generality. The key insight is that if we select a given variable in the DSEM, any subsystem containing that variable must also contain every variable that can impact\nits value. Any variable with a directed path leading to our variable of interest\nwill therefore need to be included in the subsystem.", "Definition 22. In a directed graph G = (V, E) an in-closed subset I V is a\nset of vertices such that if v I, then if e = (w, v) E, then w I.", "Lemma 20. If a dynamical system is defined by a DSEM, every in-closed subset\nof variables is a subsystem. Proof.", "Suppose that I is a in-closed subset of variables in a DSEM on a directed\ngraph G. If v I then all of the dependencies of v are also in I, so the next\ntimestep of v can be predicted from the variables in I. Therefore, projecting out\njust the variables in I from the set of all variables will result in a new dynamical\nupdate map when restricted to I.", "As a consequence of Lemma 20, we have the following result that explains\nwhy modeling with DSEM is a good idea. Corollary 21. If a dynamical system is defined by a DSEM on a partially\nordered set, then the Alexandrov topology of the dual order is a subspace of the\nbase space topology of its subsystem sheaf.", "Corollary 21 does not establish that the Alexandrov topology of the dual\norder of the DSEM is the subsystem sheaf. This is because if the original\nvariables in the DSEM are chosen coarsely, there may be additional subsystems\nthat are hidden within them. These hidden subsystems will be present in\nthe subsystem sheaf, but will not correspond to distinct in-closed subsets of the\nDSEM graph.", "48", "f", "k\npr1", "k\npr1", "k", "k", "pr1", "pr1", "( k )", "( k ) \npr1", "pr1", "g", "( k ) \n( k ) \npr1,2,5,6\npr1,2,3,4", "pr1\n( k ) \npr1", "pr1\n( k ) \npr1", "( k ) \n( k ) \npr1,2,5,6", "k", "pr1,2,3,4", "pr7", "k k", "k", "pr7", "k k", "Figure 14: Sheaf of subsystems for the Bering Sea example. Solid arrows are\nthe subsystem projection maps; dashed arrows are the dynamical system state\nupdate maps. Maps f and g are explained in the text.", "6", "Subsystems of the Bering Sea system", "Figure 14 shows the sheaf of subsystems for the Bering Sea example, with the\nstalks organized in the same way as shown in Figure 13.\nThe function f performs an AR (k) update:\n!\nk 1\nX\nai xk i ,\nf (x1 , . . . , xk ) = x2 , . . . , xk ,\ni=0", "while the function g performs the dynamical update for the subsystem containing the Krill variables:\n!\nk 1\nX\ng(x1 , . . . , xk , y, z) = x2 , . . . , xk ,\nai xk i , y + cxk , z + dy .\ni=0", "Notice how f is obtained from g by projecting out the first k components, in\naccordance with the commutativity of Figure 14.\nAlthough Figures 1(d) (with modifications to support autoregressive timeseries), 13, and 14 represent different sheaves, they all represent the same dynamical system. Consequently, the global sections of these three sheaves are\ndifferent but are in a natural bijective correspondence. The three sheaves offer\nthree distinct perspectives, with increasing granularity,\nDefinition 21: Figure 14 Stalks are nested collections of dynamically related\nvariables, each represented by sliding windows of timeseries,\n49", "Definition 13: Figure 1(d) Each variable is an entire timeseries and appears\nalone in at least one stalk, and\nDefinition 14: Figure 13 Each observation (a timestep for a single variable)\nappears alone in at least one stalk.\nWith this perspective, the boundaries between subsystems are easily seen in\nFigure 13: those restriction maps that are identity maps from parts to nets are\nthose that cross subsystem boundaries. The variables at the heads of any identity maps in Figure 13 are those that are removed by the subsystem projections\ninvolved. Moreover, the state spaces arise as one time step of the space of local\nsections over each subsystem, once cut.", "7", "Conclusion", "In this chapter, we have demonstrated how the general framework of sheaf modeling applies to several composite dynamical systems, including an ecological\nmodel of the Bering Sea and a dynamical model of low-level computer software. Sheaf modeling provides a coherent mathematical framework for studying the\ncomplicated interaction of various dynamical subsystems that together determine a larger system. The guiding principles of sheaf modeling are that\n a sheaf represents a hypothesis about how variables will interact,\n a non-global assignment represents the observations collected on the variables in its support,\n minimizing consistency radius predicts values of the variables that were\nnot observed, and\n the minimal consistency radius is a measure of the consistency between\nthe observations and the hypothesis.", "This chapter shows that when a dynamical system is described by a DSEM, there\nare three sheaves that provide increasingly granular data about the interactions\nbetween variables:\n1. the sheaf of subsystems (Definition 21),\n2. the netlist sheaf with timeseries as stalks (Definition 13), and\n3.", "the netlist sheaf with additional stalks for individual observations (Definition 14). With these three sheaves in hand, a system modeler can apply the guiding principles above to measure how well their model fits observational data. The sheaf\nencodings allow the modeler to perform a variety of standard inferences (e.", "g. forward prediction, backward prediction, regression, and missing-data imputation) using a unified framework. The sheaf modeling framework easily supports\n50", "hybrid versions, for instance performing simultaneous forward and backward\npredictions, or simultaneously performing regression and prediction. Since the\nsheaf framework measures the fit between observations and the model, the modeler can assess their confidence in these inference tasks. It remains future work to compare estimates of uncertainty computed by\nthe DSEM (appearing in the V and E matrices) to the consistency radius of\nthe corresponding sheaf.", "In particular, it seems possible to view consistency\nradius as a test statistic for the distributional model posited by the DSEM. Indeed, Equation (10) is strikingly close to the log likelihood if the distributions\nof measurement errors are assumed to follow an exponential model. If this is\ntrue, then it should be possible to lift the sheaf modeling discipline described\nhere into a standard statistical hypothesis testing framework.", "Acknowledgments\nThe linear regression example in Section 3.3 is due to Donna Dietz.\nThis article is based upon work supported by the Office of Naval Research\n(ONR) under Contract Nos. N00014-15-1-2090 and N00014-18-1-2541, the Defense Advanced Research Projects Agency (DARPA) SafeDocs program under\ncontract HR001119C0072, and the MITRE Corporation s Independent Research\nand Development (IR&D) Program. Any opinions, findings and conclusions or\nrecommendations expressed in this article are those of the authors and do not\nnecessarily reflect the views of ONR, DARPA, or MITRE."]}
