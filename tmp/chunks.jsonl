{"method": "ahgc", "num_chunks": 6, "avg_chunk_len": 483.0, "std_chunk_len": 400.87487657206293, "max_chunk_len": 1360, "min_chunk_len": 186, "total_chars": 2898, "compression_ratio": 1.0510697032436163, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking\n\nThis sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.", "## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.\n\n- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.", "## Quick Start 1. Collect source documents. 2. Parse structure (titles, headings, paragraphs). 3. Create chunks per level with adaptive thresholds. 4. Build inter-chunk edges. 5. Export to your vector store or graph DB. ```python # minimal, self-contained demo (replace with your pipeline) from collections import defaultdict def adaptive_chunks(text, min_len=200, max_len=800): buf, chunks = [], [] for para in text.split(\"\\n\\n\"): buf.append(para.strip()) size = sum(len(p) for p in buf) if size >= min_len and (size >= max_len or para.endswith(\".\")): chunks.append(\"\\n\\n\".join(buf)); buf = [] if buf: chunks.append(\"\\n\\n\".join(buf)) return chunks def build_graph(chunks): edges = defaultdict(list) for i, c in enumerate(chunks): if i > 0: edges[i].append(i-1) # prev if i < len(chunks)-1: edges[i].append(i+1) # next # naive topical link by shared keywords for j in range(i+1, len(chunks)): if len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10: edges[i].append(j); edges[j].append(i) return edges sample = \"\"\"Title: Demo Intro paragraph about adaptive chunking. Section A talks about graphs and edges. Section B references Section A with similar terms like graph, node, and edge.\"\"\" chunks = adaptive_chunks(sample) graph = build_graph(chunks) print(f\"{len(chunks)} chunks\") for i, nbrs in graph.items(): print(i, \"->\", sorted(set(nbrs))) ```", "## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```", "## Evaluation\n- Coverage: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.\n\n## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"], "num_sections": 6, "num_graph_nodes": 13, "num_graph_edges": 12}
{"method": "fixed", "num_chunks": 4, "avg_chunk_len": 761.25, "std_chunk_len": 66.54087089901965, "max_chunk_len": 800, "min_chunk_len": 646, "total_chars": 3045, "compression_ratio": 1.000328407224959, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking\n\nThis sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.\n\n## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.\n\n- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.\n\n## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector sto", "re or graph DB.\n\n```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict\n\ndef adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks\n\ndef build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if", "len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges\n\nsample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.\n\nSection A talks about graphs and edges.\n\nSection B references Section A with similar terms like graph, node, and edge.\"\"\"\n\nchunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)\n\nprint(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```\n\n## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```\n\n## Evaluation\n- Coverag", "e: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.\n\n## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.\n\n## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
{"method": "sliding", "num_chunks": 7, "avg_chunk_len": 777.8571428571429, "std_chunk_len": 53.831558910121466, "max_chunk_len": 800, "min_chunk_len": 646, "total_chars": 5445, "compression_ratio": 0.5594123048668503, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking\n\nThis sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.\n\n## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.\n\n- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.\n\n## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector sto", "aptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.\n\n## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector store or graph DB.\n\n```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict\n\ndef adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunk", "re or graph DB.\n\n```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict\n\ndef adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks\n\ndef build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if", "s.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks\n\ndef build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges\n\nsample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.\n\nSection A talks about graphs and edges.\n\nSection B references Section A with similar terms like graph, node, and edge.\"\"\"\n\nchunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)\n\nprint(f\"", "len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges\n\nsample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.\n\nSection A talks about graphs and edges.\n\nSection B references Section A with similar terms like graph, node, and edge.\"\"\"\n\nchunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)\n\nprint(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```\n\n## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```\n\n## Evaluation\n- Coverag", "{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```\n\n## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```\n\n## Evaluation\n- Coverage: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.\n\n## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.\n\n##", "e: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.\n\n## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.\n\n## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
{"method": "sentence", "num_chunks": 17, "avg_chunk_len": 179.11764705882354, "std_chunk_len": 128.0445153994322, "max_chunk_len": 527, "min_chunk_len": 17, "total_chars": 3045, "compression_ratio": 1.000328407224959, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking\n\nThis sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details. ## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.", "- Adaptive: chunk sizes vary based on content structure. - Hierarchical: chunks form levels (sections → paragraphs → sentences). - Graph: edges capture references, topical proximity, and chronology.", "## Quick Start\n1. Collect source documents. 2.", "Parse structure (titles, headings, paragraphs). 3. Create chunks per level with adaptive thresholds.", "4. Build inter-chunk edges. 5.", "Export to your vector store or graph DB. ```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict\n\ndef adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text. split(\"\\n\\n\"):\n        buf.", "append(para. strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para. endswith(\".", "\")):\n            chunks. append(\"\\n\\n\". join(buf)); buf = []\n    if buf: chunks.", "append(\"\\n\\n\". join(buf))\n    return chunks\n\ndef build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i]. append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].", "append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c. lower(). split()) & set(chunks[j].", "lower(). split())) > 10:\n                edges[i]. append(j); edges[j].", "append(i)\n    return edges\n\nsample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking. Section A talks about graphs and edges. Section B references Section A with similar terms like graph, node, and edge.", "\"\"\"\n\nchunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)\n\nprint(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph. items():\n    print(i, \"->\", sorted(set(nbrs)))\n```\n\n## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0. 12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```\n\n## Evaluation\n- Coverage: percent of source tokens represented in chunks.", "- Redundancy: average overlap between neighboring chunks. - Connectivity: average node degree in the graph. - Retrieval@k: relevant chunk found within top-k results.", "## Tips\n- Start with conservative min_len and increase if recall is low. - Add headings as anchors; preserve section boundaries. - Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example. com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example. com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.", "com/eval-recipes)"]}
{"method": "paragraph", "num_chunks": 17, "avg_chunk_len": 177.2941176470588, "std_chunk_len": 112.24697805751825, "max_chunk_len": 439, "min_chunk_len": 39, "total_chars": 3014, "compression_ratio": 1.010617120106171, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking", "This sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.", "## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.", "- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.", "## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector store or graph DB.", "```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict", "def adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks", "def build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges", "sample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.", "Section A talks about graphs and edges.", "Section B references Section A with similar terms like graph, node, and edge.\"\"\"", "chunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)", "print(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```", "## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```", "## Evaluation\n- Coverage: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.", "## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
{"method": "recursive", "num_chunks": 17, "avg_chunk_len": 177.2941176470588, "std_chunk_len": 112.24697805751825, "max_chunk_len": 439, "min_chunk_len": 39, "total_chars": 3014, "compression_ratio": 1.010617120106171, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking", "This sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.", "## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.", "- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.", "## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector store or graph DB.", "```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict", "def adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks", "def build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges", "sample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.", "Section A talks about graphs and edges.", "Section B references Section A with similar terms like graph, node, and edge.\"\"\"", "chunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)", "print(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```", "## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```", "## Evaluation\n- Coverage: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.", "## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
{"method": "semantic", "num_chunks": 17, "avg_chunk_len": 179.11764705882354, "std_chunk_len": 128.0445153994322, "max_chunk_len": 527, "min_chunk_len": 17, "total_chars": 3045, "compression_ratio": 1.000328407224959, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking\n\nThis sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details. ## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.", "- Adaptive: chunk sizes vary based on content structure. - Hierarchical: chunks form levels (sections → paragraphs → sentences). - Graph: edges capture references, topical proximity, and chronology.", "## Quick Start\n1. Collect source documents. 2.", "Parse structure (titles, headings, paragraphs). 3. Create chunks per level with adaptive thresholds.", "4. Build inter-chunk edges. 5.", "Export to your vector store or graph DB. ```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict\n\ndef adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text. split(\"\\n\\n\"):\n        buf.", "append(para. strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para. endswith(\".", "\")):\n            chunks. append(\"\\n\\n\". join(buf)); buf = []\n    if buf: chunks.", "append(\"\\n\\n\". join(buf))\n    return chunks\n\ndef build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i]. append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].", "append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c. lower(). split()) & set(chunks[j].", "lower(). split())) > 10:\n                edges[i]. append(j); edges[j].", "append(i)\n    return edges\n\nsample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking. Section A talks about graphs and edges. Section B references Section A with similar terms like graph, node, and edge.", "\"\"\"\n\nchunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)\n\nprint(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph. items():\n    print(i, \"->\", sorted(set(nbrs)))\n```\n\n## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0. 12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```\n\n## Evaluation\n- Coverage: percent of source tokens represented in chunks.", "- Redundancy: average overlap between neighboring chunks. - Connectivity: average node degree in the graph. - Retrieval@k: relevant chunk found within top-k results.", "## Tips\n- Start with conservative min_len and increase if recall is low. - Add headings as anchors; preserve section boundaries. - Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example. com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example. com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.", "com/eval-recipes)"]}
{"method": "delimiter", "num_chunks": 17, "avg_chunk_len": 177.2941176470588, "std_chunk_len": 112.24697805751825, "max_chunk_len": 439, "min_chunk_len": 39, "total_chars": 3014, "compression_ratio": 1.010617120106171, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking", "This sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.", "## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.", "- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.", "## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector store or graph DB.", "```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict", "def adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks", "def build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges", "sample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.", "Section A talks about graphs and edges.", "Section B references Section A with similar terms like graph, node, and edge.\"\"\"", "chunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)", "print(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```", "## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```", "## Evaluation\n- Coverage: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.", "## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
{"method": "token_limit", "num_chunks": 4, "avg_chunk_len": 761.25, "std_chunk_len": 66.54087089901965, "max_chunk_len": 800, "min_chunk_len": 646, "total_chars": 3045, "compression_ratio": 1.000328407224959, "avg_chunk_tokens": 190.0, "max_chunk_tokens": 200, "min_chunk_tokens": 161, "tokenizer": "", "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking\n\nThis sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.\n\n## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.\n\n- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.\n\n## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector sto", "re or graph DB.\n\n```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict\n\ndef adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks\n\ndef build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if", "len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges\n\nsample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.\n\nSection A talks about graphs and edges.\n\nSection B references Section A with similar terms like graph, node, and edge.\"\"\"\n\nchunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)\n\nprint(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```\n\n## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```\n\n## Evaluation\n- Coverag", "e: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.\n\n## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.\n\n## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
{"method": "format_aware", "num_chunks": 7, "avg_chunk_len": 433.42857142857144, "std_chunk_len": 440.2213265239777, "max_chunk_len": 1498, "min_chunk_len": 178, "total_chars": 3034, "compression_ratio": 1.003955174686882, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking\n\nThis sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.", "## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.\n\n- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.", "## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector store or graph DB.\n\n```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict\n\ndef adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks\n\ndef build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges\n\nsample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.\n\nSection A talks about graphs and edges.\n\nSection B references Section A with similar terms like graph, node, and edge.\"\"\"\n\nchunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)\n\nprint(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```", "## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```", "## Evaluation\n- Coverage: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.", "## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
{"method": "hybrid", "num_chunks": 17, "avg_chunk_len": 177.2941176470588, "std_chunk_len": 112.24697805751825, "max_chunk_len": 439, "min_chunk_len": 39, "total_chars": 3014, "compression_ratio": 1.010617120106171, "chunks": ["# Sample: Adaptive Hierarchical Graph Chunking", "This sample document demonstrates headings, lists, code blocks, and links in Markdown. Replace placeholder text with your project details.", "## Overview\nAdaptive hierarchical graph chunking splits long documents into semantically coherent chunks, builds a graph over them (nodes = chunks, edges = relationships), and enables multi-scale retrieval.", "- Adaptive: chunk sizes vary based on content structure.\n- Hierarchical: chunks form levels (sections → paragraphs → sentences).\n- Graph: edges capture references, topical proximity, and chronology.", "## Quick Start\n1. Collect source documents.\n2. Parse structure (titles, headings, paragraphs).\n3. Create chunks per level with adaptive thresholds.\n4. Build inter-chunk edges.\n5. Export to your vector store or graph DB.", "```python\n# minimal, self-contained demo (replace with your pipeline)\nfrom collections import defaultdict", "def adaptive_chunks(text, min_len=200, max_len=800):\n    buf, chunks = [], []\n    for para in text.split(\"\\n\\n\"):\n        buf.append(para.strip())\n        size = sum(len(p) for p in buf)\n        if size >= min_len and (size >= max_len or para.endswith(\".\")):\n            chunks.append(\"\\n\\n\".join(buf)); buf = []\n    if buf: chunks.append(\"\\n\\n\".join(buf))\n    return chunks", "def build_graph(chunks):\n    edges = defaultdict(list)\n    for i, c in enumerate(chunks):\n        if i > 0: edges[i].append(i-1)  # prev\n        if i < len(chunks)-1: edges[i].append(i+1)  # next\n        # naive topical link by shared keywords\n        for j in range(i+1, len(chunks)):\n            if len(set(c.lower().split()) & set(chunks[j].lower().split())) > 10:\n                edges[i].append(j); edges[j].append(i)\n    return edges", "sample = \"\"\"Title: Demo\nIntro paragraph about adaptive chunking.", "Section A talks about graphs and edges.", "Section B references Section A with similar terms like graph, node, and edge.\"\"\"", "chunks = adaptive_chunks(sample)\ngraph = build_graph(chunks)", "print(f\"{len(chunks)} chunks\")\nfor i, nbrs in graph.items():\n    print(i, \"->\", sorted(set(nbrs)))\n```", "## Configuration (example)\n```yaml\nlevels:\n  - name: section\n    min_len: 600\n    max_len: 1500\n  - name: paragraph\n    min_len: 200\n    max_len: 800\nlinkers:\n  - type: adjacency\n  - type: keyword_overlap\n    threshold: 0.12\nexports:\n  - type: jsonl\n  - type: networkx_pickle\n```", "## Evaluation\n- Coverage: percent of source tokens represented in chunks.\n- Redundancy: average overlap between neighboring chunks.\n- Connectivity: average node degree in the graph.\n- Retrieval@k: relevant chunk found within top-k results.", "## Tips\n- Start with conservative min_len and increase if recall is low.\n- Add headings as anchors; preserve section boundaries.\n- Limit cross-links to avoid dense, noisy graphs.", "## References\n- Concept note: [Graph-based chunking primer](https://example.com/graph-chunking-primer)\n- Retrieval best practices: [RAG guide](https://example.com/rag-guide)\n- Metrics cookbook: [Evaluation recipes](https://example.com/eval-recipes)"]}
