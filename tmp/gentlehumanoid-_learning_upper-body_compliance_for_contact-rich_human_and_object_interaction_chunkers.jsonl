{"method": "ahgc", "num_chunks": 47, "avg_chunk_len": 1241.127659574468, "std_chunk_len": 237.29769587796358, "max_chunk_len": 1504, "min_chunk_len": 420, "total_chars": 58333, "compression_ratio": 0.8682392470814119, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction Qingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu Stanford University Project Page: gentle-humanoid.axell.top arXiv:2511.04679v1 [cs.RO] 6 Nov 2025 (a) Sit-to-stand Support (b) Handshaking (d) Balloon Handling GentleHumanoid (c) Shape-aware Hugging Vanilla Tracking RL Tracking RL w/ Large Perturbation Fig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits. It enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder); (b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous shape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a comfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail. Abstract Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a", "tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified springbased formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the Equal contribution. This work was done during Qingzhou Lu s intern- ship at Stanford University. Qingzhou is now with Tsinghua University. shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments. I. INTRODUCTION Safe and", "and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments. I. INTRODUCTION Safe and compliant physical interaction is essential for deploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive whole-body locomotion and manipulation [1] [8]. However, most policies emphasize rigid position or velocity tracking and treat external forces as disturbances to suppress, which limits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent works have integrated impedance or admittance control into RL [9] [11] or attempted to learn forceful loco-manipulation implicitly [12]. However, these approaches are restricted to base or end-effector control and typically emphasize resisting extreme forces rather than supporting compliant interaction. In contrast, interactions such as giving a comforting hug or assisting with sit-to-stand support require compliance across the entire upper-body kinematic chain, where multiple links including shoulders, elbows, and hands may be in contact simultaneously. Depending on the scenario, compliance must range from gentle yielding (e.g., hugging people or handling fragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds. This raises two main challenges: (1) coordinating", "from gentle yielding (e.g., hugging people or handling fragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds. This raises two main challenges: (1) coordinating force responses across multiple links of the kinematic chain, and (2) adapting to diverse contact scenarios, from gentle touch to strong supportive forces. We address these challenges with GentleHumanoid, a framework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with upper-body compliance. The humanoid s action is influenced by two forces: a driving force for motion tracking, modeled as a virtual spring damper system that pulls link positions toward target motions, and an interaction force that represents physical contact with humans or objects. Since collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines such as MuJoCo and IsaacGym can generate contact forces at colliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed in human human interactions. They also only occur when collisions arise during rollout, limiting coverage of diverse interaction scenarios. To address this, we introduce a unified spring-based formulation with two cases: (i) resistive contact, when the humanoid presses against a", "arise during rollout, limiting coverage of diverse interaction scenarios. To address this, we introduce a unified spring-based formulation with two cases: (i) resistive contact, when the humanoid presses against a surface, modeled by fixing the spring anchor at the initial contact point to generate restoring forces; and (ii) guiding contact, when the humanoid is pushed or pulled by external agents, modeled by sampling spring anchors from upper-body postures in human motion datasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain (e.g., shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically consistent and diverse interaction forces, enabling the policy to learn robust compliance. To further ensure safety, we apply force-thresholding during training, with adjustable limits at deployment based on task requirements. We evaluate GentleHumanoid against baselines, including a vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the Unitree G1 humanoid. Quantitative tests use commercial force gauges and conformable, customized waist-mounted pressure sensing pads with 40 calibrated capacitive taxels to measure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and softobject manipulation. We also", "with 40 calibrated capacitive taxels to measure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging pipeline that integrates our policy with vision-based human shape estimation for personalized hugs. In summary, the main contributions of this work are: We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to achieve whole-body humanoid control with upper-body compliance. Central to the framework is a unified formulation of interaction force modeling that covers both resistive and guiding contacts, sampling from human motion datasets to ensure kinematic consistency and capture diverse interaction scenarios. We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling comfortable and safer physical human robot interaction. We design a hugging evaluation setup with a custom pressure-sensing pad tailored for hugging, providing reliable measurement of distributed contact forces. We validate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more adaptable performance than baselines across hugging, sit-to-stand assistance, and object manipulation. II. R ELATED W ORK A. Humanoid Whole Body Control Whole-body control for humanoid robots is a longstanding challenge in robotics.", "than baselines across hugging, sit-to-stand assistance, and object manipulation. II. R ELATED W ORK A. Humanoid Whole Body Control Whole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated by high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC), can produce stable behaviors but demand extensive expert design and meticulous tuning to balance feasibility and computational cost [13] [15]. More recently, learning-based methods have alleviated many of the challenges of tedious design in model-based methods. In particular, learning from human motion data has been successful for producing highly dynamic motions with single-skill policies [5] and generalist policies [3], [4], [6]. Similar frameworks have also been used for whole-body tele-operation [2], [7], [8]. However, these approaches often neglect scenarios involving complex contact dynamics, which reduces their robustness to external disturbances and raises safety concerns in close physical interaction with humans. B. Force-adaptive Control To address the aforementioned issue of robust and safe contact, classical force-adaptive methods such as impedance and admittance control regulate interaction forces and have been extended to whole-body frameworks [15] [17]. More recently, RL-based approaches have incorporated impedance or admittance control for adaptive contact", "as impedance and admittance control regulate interaction forces and have been extended to whole-body frameworks [15] [17]. More recently, RL-based approaches have incorporated impedance or admittance control for adaptive contact behaviors [9] [11], while others aim to implicitly learn robustness to external disturbances and extreme forces [12], [18]. However, these methods typically focus on end-effector interactions rather than interactions that involve other body parts. In tasks such as carrying large objects or interacting with a human, contact is not restricted to the wrists/hands but may involve coordinated force distribution across multiple links, including elbows, and shoulders. Our work addresses this gap by introducing a framework that models compliance across the whole upper body kinematic chain. C. Human-humanoid Interaction As humanoid robots move closer to deployment in humancentered environments, their ability to interact physically with people becomes increasingly important. Towards this goal, early works have explored using human-in-the-loop strategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied traditional control methods to assist humans in specific tasks such as sit-to-stand transitions [21], [22]. However, these approaches are typically tailored to a single scenario, and the resulting policies do not generalize across different interaction contexts", "in specific tasks such as sit-to-stand transitions [21], [22]. However, these approaches are typically tailored to a single scenario, and the resulting policies do not generalize across different interaction contexts such as both hugging and sit-to-stand assistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable humanoids to consistently avoid human collisions [23]. In contrast, our approach proposes a general motion-tracking policy capable of handling multiple interaction scenarios. In particular, for hugging tasks, we combine the policy with visual perception to customize hugging positions for people of different body shapes. III. M ETHOD A. Problem Formulation Our goal is to achieve whole-body humanoid control that is both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and deformable objects. We frame this as learning a compliant motion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to interaction forces. Unlike rigid trajectory tracking, humans naturally adjust their actions based on contact feedback, which motivates our use of impedance-based control. Since most physical interactions occur in the upper body, we focus on modeling it as a multi-link impedance system with keypoints at the shoulders, elbows, and", "use of impedance-based control. Since most physical interactions occur in the upper body, we focus on modeling it as a multi-link impedance system with keypoints at the shoulders, elbows, and hands. As illustrated in Fig. 2, the motion of each link position is influenced by the combination of driving forces from target motions and interaction forces from humans or objects: M x i = fdrive,i + finteract,i , (1) where xi is the position of link i, x i is acceleration, and M is a scalar virtual mass (kg) per link. We set M as 0.1 kg in our reference dynamics model. The driving force fdrive,i is a virtual spring damper term from classical impedance control, pulling the link position toward its target motion, and finteract,i captures forces arising from interactions with the environment, including humans and objects. In the following sections, we detail the formulation of each force component. For clarity, we introduce the index i once and omit it henceforth. All link positions x and velocities x are 3D Cartesian quantities expressed in the robot s root frame. B. Impedance-Based Driving Force from Target Motion Following prior work [10], [15], we generate driving forces from the target motion", "3D Cartesian quantities expressed in the robot s root frame. B. Impedance-Based Driving Force from Target Motion Following prior work [10], [15], we generate driving forces from the target motion to pull each link position toward its target trajectory. The force is modeled as a virtual spring damper system: fdrive = Kp (xtar xcur ) + Kd (vtar vcur ) , (2) where xcur , vcur are the current link position and velocity, and xtar , vtar are the corresponding target link position and velocity from the target motion. The gains Kp and Kd denote the impedance stiffness and damping, respectively, controlling how strongly the link position tracks its target. To ensure stable and smooth p behavior, we set the damping to the critical value, Kd = 2 M Kp . All x and v terms above denote 3D Cartesian link states (in the root frame), while the policy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate these compliant forces across multiple joints, mapping them into joint-level actions that balance stability and adaptability in whole-body control. C. Interaction Force Modeling When no interaction occurs, the driving force alone enables", "forces across multiple joints, mapping them into joint-level actions that balance stability and adaptability in whole-body control. C. Interaction Force Modeling When no interaction occurs, the driving force alone enables the humanoid to follow target motions. In real scenarios, however, physical contact introduces additional interaction forces across multiple links, often correlated in direction and magnitude. To capture these effects, we design a unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases: Resistive contact: Forces generated when the humanoid itself presses against a human or object. Guiding contact: Forces applied by an external agent, such as a human pushing or pulling the humanoid s arm. Both cases are modeled using the same spring formulation with a consistent anchor terminology: finteract = Kspring xanchor xcur , (3) where Kspring is the stiffness, xcur is the current link position, and the spring anchor xanchor is defined as xcur (t0 ), resistive contact, xanchor = (4) x guiding contact. sample , Here, xcur (t0 ) is the link position at the moment of initial contact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external agent steering the", "the link position at the moment of initial contact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external agent steering the humanoid toward a new configuration. This formulation provides a unified framework: Resistive contact yields restoring forces that resist deviations from the contact point, while Guiding contact yields guiding forces that pull the humanoid toward externally defined postures. Posture samples are drawn from real human motion data, ensuring that the guiding forces are kinematically valid and (a) Reference Dynamics (c) Deployment HRI Motion Contact Hugging Planner timestep Vision Driving Force Interaction Force Hugging Motion Motion Target Current Pos Anchor Pos GentleHumanoid Policy Proprioception Privileged Obs (b) Training Reward Safe & Compliant Interactions Policy Target Motion Action Simulator Fig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion tracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands) positions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and is optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment: the trained GentleHumanoid policy is applied", "and is optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment: the trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other human robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and handling large deformable objects. D. Safety-Aware Force Thresholding In Equation 2, the driving force grows proportionally with tracking error. Without limitation, large deviations from the target motion can result in unbounded forces, potentially exceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the Right Shoulder Link Right Elbow Link 0.200 Right Hand Link 0.16 0.7 0.175 0.6 0.14 0.150 0.5 Density correspond to plausible upper-body movements. Specifically, we precompute posture distributions from motion dataset, during training, select postures close to the current multi-link positions. From these, a target position is randomly sampled and used as the spring anchor to generate guiding forces. To further increase interaction diversity, we randomize both stiffness and the active links. The stiffness is sampled as Kspring U(5, 250). Active-contact sets are chosen with the following probabilities: 40% no external force; 15% both arms (all", "both stiffness and the active links. The stiffness is sampled as Kspring U(5, 250). Active-contact sets are chosen with the following probabilities: 40% no external force; 15% both arms (all 6 links) under force; 30% a single arm (left or right; its 3 links) under force (15% each arm); and 15% only a single link under force. Anchors and selections are resampled every 5 seconds with a short transition window to ensure continuity. This exposes the policy to a broad range of interaction dynamics, enabling it to learn robust compliance while preserving consistency along the kinematic chain. As a result, the model can simulate diverse external force directions and magnitudes; Figure 3 visualizes the resulting distribution, showing that forces span a wide range of directions on the sphere with magnitudes from 0 to 25 N. 0.4 0.12 0.125 0.10 0.100 0.08 0.3 0.075 0.06 0.2 0.050 0.04 0.1 0.025 0.0 0 5 10 15 20 25 Force magnitude & direction 0.000 0.02 0 5 10 15 20 25 Force magnitude & direction 0.00 0 5 10 15 20 25 Force magnitude & direction Fig. 3: Interaction force distributions across upper-body links. Probability densities of force magnitudes are shown for the", "& direction 0.00 0 5 10 15 20 25 Force magnitude & direction Fig. 3: Interaction force distributions across upper-body links. Probability densities of force magnitudes are shown for the right shoulder (left), right elbow (middle), and right hand (right). Insets (top right) illustrate the corresponding force directions on a sphere. maximum allowable force applied by the robot. We define a range of force thresholds and sample a piecewise-constant value τsafe during training: F1 τsafe F2 . The threshold is resampled every 5 seconds, encouraging the policy to remain robust across a range of safety limits. The current threshold is also provided to the policy as part of the observation. Here, F1 and F2 define the range for the maximal allowable force the robot should apply in various tasks. When the driving force exceeds the threshold, we apply a scaling mechanism: τsafe fdrive limited = min 1.0, fdrive , (5) fdrive compliance. The threshold directly tunes compliance: lower values yield softer, safer behavior for gentle interactions like hugging, while higher values allow firmer support for tasks such as sit-to-stand assistance, all while maintaining safety bounds. The choice of exact threshold depends on the application. Since our focus is humanoid interaction", "values allow firmer support for tasks such as sit-to-stand assistance, all while maintaining safety bounds. The choice of exact threshold depends on the application. Since our focus is humanoid interaction with humans and fragile objects (e.g., balloons), we set F1 = 5 N and F2 = 15 N. These values are benchmarked against both ISO/TS 15066 [24] safety ceilings and comfort studies. In the extreme case of a minimal 0.5 0.5 cm contact area (0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below ISO/TS 15066 pain-onset limits for torso and arms (e.g., back/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds to 3 9 kPa, consistent with measurements of children s hugs (soft hugs < 7 kPa, strong hugs 18 kPa) [25] and rehabilitation studies recommending pressures 13 kPa for comfort [26]. Thus, our thresholds remain well below ISO ceilings while lying in a comfort-oriented band. E. RL-based Control Policy Formally, we consider a humanoid robot at time t with observation ot containing its proprioception and a target motion sequence mtar . The policy π(at | ot ) outputs joint position targets at at 50 Hz", "at time t with observation ot containing its proprioception and a target motion sequence mtar . The policy π(at | ot ) outputs joint position targets at at 50 Hz for low-level PD tracking, enabling the humanoid to follow the target motion while exhibiting compliant responses to interaction forces finteract . To incorporate the impedance-based reference dynamics, we simulate the model using semi-implicit Euler integration, with a fixed time step of 0.005 s: fdrive +finteract ref , x ref t+1 = x t + t M (6) ref ref xref t+1 = xt + t x t+1 . (7) Where t is the integration step size, and xref t denotes the link position in the reference dynamics model, which we distinguish from the actual robot link position xsim in the simulator. The objective is to guide the robot to follow the impedance rules encoded in the reference dynamics. At each timestep, velocities and positions are updated according to the net driving and interaction forces, with semi-implicit Euler ensuring numerical stability. This impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce. We compute xref via the above integration and use it in the link-position tracking rewards", "This impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce. We compute xref via the above integration and use it in the link-position tracking rewards (details in Reward Design). During training, the RL agent observes ot and outputs at such that the resulting behavior aligns with this dynamics model. In effect, the policy learns to track target motions while adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios. 1) Teacher-Student Architecture: We employ a two-stage teacher student training framework for sim-to-real transfer. We adopt the same teacher-student architecture and training procedure from prior work [10], and train both policies with PPO [27]. The student policy observes only information available during real-world deployment: ot = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) , where τsafe represents the current force-safety limit, that can be changed by use during deployment; mtar contains target motion information including future root poses and target joint position; ω is the root angular velocity; and g is gravity expressed in the robot s root frame (projected gravity). qthist provide joint-position history, and at 3:t 1 contains the recent action history. The teacher policy additionally", "and g is gravity expressed in the robot s root frame (projected gravity). qthist provide joint-position history, and at 3:t 1 contains the recent action history. The teacher policy additionally receives comprehensive privileged information: ref sim opriv = (xref t t , x t , finteract , finteract , ht , τt 1 , ecum ) , ref where xref t and x t are the integrated link positions and velocities from the impedance-based reference dynamics (Eq. 7); finteract denotes the interaction force predicted by sim the reference dynamics, while finteract is the actual interaction force measured in simulation. Ideally, finteract should closely sim . ht represents link heights relative to the match finteract ground; τt 1 are the previous joint torques; and ecum denotes the cumulative tracking error. Both policies output joint position targets at R29 which are tracked by low-level PD controllers. 2) Motion Datasets: We use diverse human motion to train our policy, covering data for both human-human and human-object interactions datasets. Specifically, we use GMR [28] to retarget the AMASS [29], InterX [30], and LAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately obtaining approximately 25 hours of", "to retarget the AMASS [29], InterX [30], and LAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately obtaining approximately 25 hours of dataset with a sampling frequency of 50Hz. 3) Reward Design: Following prior work on whole-body humanoid control [2], [8], we adapt rewards for motion tracking and locomotion stability, as summarized in Table I, to encourage accurate motion tracking and stable balance. In GentleHumanoid, we additionally design a compliance reward composed of three terms: Reference Dynamics Tracking. We encourage the robot to follow the compliant reference dynamics by minimizing the discrepancy between the actual link state in simulation sim ref ref (xsim t , x t ) and the reference state (xt , x t ) from Eq. 7: x sim x ref xsim xref t 2 t 2 + exp t . rdyn = exp t σx σv Exponential kernels provide smooth gradients, with σx and σv controlling sensitivity. Reference Force Tracking. To align predicted interaction forces with actual forces measured in simulation, we penalize the discrepancy between finteract from the reference dynamics sim and finteract from the environment: sim finteract finteract 2 rforce = exp . σf This term", "measured in simulation, we penalize the discrepancy between finteract from the reference dynamics sim and finteract from the environment: sim finteract finteract 2 rforce = exp . σf This term complements position tracking by explicitly regulating force magnitudes, which is crucial for enforcing safe maximum force thresholds. Unsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety margin τsafe , in addition to the driving force thresholding in Eq. 5: rpen = I( finteract > τsafe + δtol ) . rcompliance = wdyn rdyn + wforce rforce + wpen rpen . The weights for each term along with those for motion tracking and locomotion stability are provided in Table I. TABLE I: Reward Terms and Weights. Reward Vanilla-RL Extreme-RL Right Elbow Link 20 Right Shoulder Link 15 15 10 5 0 Force (N) 20 Force (N) Force (N) Here, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This prevents the policy from becoming overly conservative while still discouraging forces that are clearly unsafe. In practice, we set δtol as 10 N based on empirical observations. The overall compliance reward is a weighted sum of these terms: GentleHumanoid", "discouraging forces that are clearly unsafe. In practice, we set δtol as 10 N based on empirical observations. The overall compliance reward is a weighted sum of these terms: GentleHumanoid Right Hand Link 15 10 5 0 0 2 4 6 8 10 10 5 0 0 2 Time (s) 4 6 8 10 0 2 Time (s) 4 6 8 10 Time (s) Fig. 4: Forces applied by different upper-body links under external interaction. Force profiles over time are shown for the right hand (left), right elbow (middle), and right shoulder (right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force levels across all links, showing safer and more compliant responses during contact. !\"#$ =10N !\"#$ =15N Weight !\"#$ =5N Compliance Reference Dynamics Tracking Reference Force Tracking Unsafe Force Penalty 2.0 2.0 6.0 GentleHumanoid with different force limits Motion Tracking Root Tracking Joint Tracking 0.5 1.0 Vanilla-RL Locomotion Stability Survival Feet Air Time Impact Force Slip Penalty Action Rate Joint Velocity Joint Limit 5.0 10.0 4.0 2.0 0.1 5.0e-4 1.0 Extreme-RL Fig. 5: Comparison of interaction forces across policies. Top: GentleHumanoid with tunable force limits, which maintains safe interaction by keeping contact forces within specified", "4.0 2.0 0.1 5.0e-4 1.0 Extreme-RL Fig. 5: Comparison of interaction forces across policies. Top: GentleHumanoid with tunable force limits, which maintains safe interaction by keeping contact forces within specified thresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent compliance, with higher peak forces or oscillatory responses. Force gauge readings (N) are highlighted for clarity. IV. E XPERIMENTS We conduct both simulation and real-world experiments to evaluate the effectiveness of GentleHumanoid. We compare against two baselines that adopt different training strategies: Vanilla-RL: an RL-based motion tracking policy trained without force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector force perturbations, representative of prior force-adaptive methods. A. Simulation Results We first benchmark against baselines in simulation using a hugging motion. To evaluate compliance, we simulate an external pulling force that attempts to move the robot away from its hugging posture, mimicking a human trying to break free from an embrace. As shown in Figure 4, our method consistently maintains lower and more stable interaction forces across the hand, elbow, and shoulder links. At the hand, GentleHumanoid stabilizes around 10 N, whereas Vanilla-RL settles above 20 N", "method consistently maintains lower and more stable interaction forces across the hand, elbow, and shoulder links. At the hand, GentleHumanoid stabilizes around 10 N, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow and shoulder: while baselines quickly saturate at 15 20 N with rigid responses, GentleHumanoid remains bounded near 7 10 N. These results show that our method adapts smoothly to external interaction, yielding compliant motions, while baselines remain overly stiff and exert higher peak forces. B. Real-World Experiments We deploy our whole-body control policy on the Unitree G1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered: 1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s base remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces are applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5, both baselines resist stiffly: rather than letting the arm move, the torso shifts, often leading to imbalance. Extreme-RL is particularly rigid, requiring a peak force of 51.14 N, while Vanilla-RL", "both baselines resist stiffly: rather than letting the arm move, the torso shifts, often leading to imbalance. Extreme-RL is particularly rigid, requiring a peak force of 51.14 N, while Vanilla-RL requires 24.59 N. In contrast, GentleHumanoid responds smoothly and consistently, requiring much lower forces to reposition the arm while maintaining balance. A C. More Applications GentleHumanoid enables applications where compliance is critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to control walking and trigger pre-defined reference motions such as hugging, sit-to-stand assistance, and object handling. Demonstrations of joystick-based control are provided in the supplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body teleoperation such as TWIST [8] is an important direction for future work. The inherent compliance of our method ensures safe interactions even during teleoperation under direct physical contact, making it particularly promising Hugging in right position Hugging with misalignment Sensor Pad Real-time Pressure Visualization Extreme-RL Hugging with misalignment 0 0 0 0 0 0 0 1 0 0 8 0 0 1 0 0 0 0 1 3 4 2 0 0 0 0 6 16 8 0 0 0 0 0 1 0 0 1", "0 0 8 0 0 1 0 0 0 0 1 3 4 2 0 0 0 0 6 16 8 0 0 0 0 0 1 0 0 1 2 0 17 9 0 0 0 0 50 6 6 0 0 0 1 0 0 0 28 5 5 0 30 7 1 1 1 3 45 9 8 1 0 0 0 0 0 1 111 5 3 0 11 1 1 1 0 8 34 41 4 0 0 0 0 0 0 19 26 37 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 15 10 5 0 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 7 4 0 0 0 0 0 0 0 0 2 1 1 0 0 0 0 0 15 20 0 0 0 0 0 1 0 0 0 0 0 4 4 0 0 0 0 38 13 17", "2 1 1 0 0 0 0 0 15 20 0 0 0 0 0 1 0 0 0 0 0 4 4 0 0 0 0 38 13 17 1 0 0 0 0 0 0 0 0 0 0 11 0 16 1 0 0 0 1 3 11 0 0 0 0 0 1 7 12 2 4 1 2 5 1 0 0 0 1 8 1 0 0 0 0 0 0 9 8 16 2 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 208 61 8 3 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 10 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 13 0 1 4 7 23 1 0 1 4 11 12 0 4 2 2 0 0 0 2 27 6 0 0 0 19 11 0 0 62 0 2 11 25 4 1 0 0 4 2 52 7 0 0 0 0 0 0 5 5", "27 6 0 0 0 19 11 0 0 62 0 2 11 25 4 1 0 0 4 2 52 7 0 0 0 0 0 0 5 5 0 2 3 64 4 0 0 0 22 6 83 0 0 0 0 0 0 0 0 1 0 2 2 6 15 0 0 0 2 17 8 0 0 0 0 0 0 0 0 2 0 0 16 417 10 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 19 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4 0 0 0 0 0 0 0 0 0 4 551 1 0 0 0 0 0 14 16 GentleHumanoid Vanilla-RL Hugging in right position 350 300 250 200 150 100 50 0 Extreme-RL Hugging with misalignment 30 25 25 Peak Force (N) 30 20 15 10 5 0 400 Pressure (kPa) Vanilla-RL GentleHumanoid Hugging in right position Force (N) key observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to modulate arm position across different configurations. Moreover, compliance level matches the", "in right position Force (N) key observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to modulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit. For example, when set to 10 N, the robot maintains balance around that threshold across postures, with effective ranges between 5 15 N. This uniform, predictable response arises from our formulation, which regulates compliance through virtual spring damper dynamics and safety thresholds rather than raw joint mechanics. As a result, human interaction feels safer and more consistent than with baselines. 2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin is properly aligned with the robot, and the G1 executes a hugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact. Pressure-sensing pads attached to the mannequin measure contact forces. We set τsafe as 10 N in GentleHumanoid to compare with baselines. For sensor calibration, a motorized stage with a PDMS applicator was used to map normalized sensor values to ground-truth pressures measured by a force gauge. Under localized contact, we approximate the effective contact area of each texel as 6 mm 6 mm", "to map normalized sensor values to ground-truth pressures measured by a force gauge. Under localized contact, we approximate the effective contact area of each texel as 6 mm 6 mm and compute forces from the corresponding pressure values recorded in the pad. The evaluation setups and results are shown in Figure 6, GentleHumanoid maintains bounded and stable forces even under misalignment, whereas the baselines Vanilla-RL and Extreme-RL generate higher, less predictable forces or fail to sustain the motion. 3) Handling deformable objects. Finally, we test the ability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range: insufficient force fails to stabilize the object, while excessive force causes deformation or collapse. For this experiment, the force threshold in GentleHumanoid is set to 5 N. As shown in Figure 1(d), GentleHumanoid successfully holds the balloon without damage, whereas both baselines apply excessive pressure, eventually squeezing the balloon until the G1 loses balance and drops it. Across all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact. 20 15 10 5 0 2 4 6 8 Time (s) 10 12 14 16 0 0 2 4 6", "forces compared to baselines, resulting in safer and smoother contact. 20 15 10 5 0 2 4 6 8 Time (s) 10 12 14 16 0 0 2 4 6 8 Time (s) 10 12 Fig. 6: Evaluation of hugging interactions with and without misalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle: pressure maps of peak force frames for different controllers under correct hugging alignment (left) and misalignment (right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks, especially under Vanilla-RL. Bottom: Force profiles over time, where GentleHumanoid maintains bounded and stable forces, while baselines exhibit increasing or unstable peaks. for healthcare and assistive scenarios where caregivers or operators remotely guide humanoid motions. We also develop an autonomous, shape-aware pipeline for personalized hugging. The human s location and height are obtained using a motion-capture system with markers placed on a hat, while an additional RGB camera mounted on the G1 s head provides input for single-image human shape estimation, as shown in Figure 1(c). From this image, we reconstruct a personalized body mesh using an existing human mesh estimation method [32] and scale it to the subject s true height. Waist points are", "1(c). From this image, we reconstruct a personalized body mesh using an existing human mesh estimation method [32] and scale it to the subject s true height. Waist points are then extracted from the mesh to optimize the humanoid s hugging motion by aligning its hands with these target locations. This allows the G1 to adapt its hugging posture to individuals of different body shapes in a fully autonomous manner. Experiments with participants of varying heights and builds show that the pipeline generates stable and comfortable hugging motions. V. D ISCUSSION AND L IMITATIONS Our study shows that GentleHumanoid enables upper-body compliance in humanoid robots. By integrating impedance control into whole-body motion tracking and training with a unified spring-based formulation, the policy generates coordinated responses across multiple links and reduces peak contact forces compared to baselines. Demonstrations in hugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios, underscoring its potential for human-centered interaction. Several limitations remain. First, we use human motion data to maintain kinematic consistency across links, but the dataset itself constrains the force distribution. For instance, forces applied to the shoulder are relatively small due to limited variation in the recorded motions.", "kinematic consistency across links, but the dataset itself constrains the force distribution. For instance, forces applied to the shoulder are relatively small due to limited variation in the recorded motions. Incorporating more diverse motion datasets, such as dancing, could further improve coverage. Second, our interaction modeling relies on simulated spring forces, which provide structured coverage and kinematic consistency but do not fully capture the complexity of real human contact, such as frictional effects or the viscoelastic properties of human tissue. Third, although the safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N due to sim-to-real discrepancies. Additional tactile sensing may be necessary for more precise force regulation. Finally, human localization and height are currently obtained from a motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality, particularly in long-horizon tasks. Future work will focus on integrating richer sensing, combining general perception and reasoning systems such as vision language models, and extending evaluations to long-horizon interactions where the humanoid must adapt its motion dynamically to human partners behaviors. VI. ACKNOWLEDGMENT We would like to thank Haoyang Weng, Botian Xu, Haochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei", "its motion dynamically to human partners behaviors. VI. ACKNOWLEDGMENT We would like to thank Haoyang Weng, Botian Xu, Haochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their valuable discussions. We are also grateful to Yu Sun for assistance with motion capture from video and to Jiaxin Lu for support with the motion dataset. We further thank the Unitree team for their timely and reliable hardware support. R EFERENCES [1] D. J. Agravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and A. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2 [2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus: Humanoid shadowing and imitation from humans, in Conference on Robot Learning (CoRL), 2024. 2, 5 [3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang, Exbody2: Advanced expressive humanoid whole-body control, arXiv preprint arXiv:2412.13196, 2024. 2 [4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang, Gmt: General motion tracking for humanoid whole-body control, arXiv:2506.14770, 2025. 2 [5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J.", "Peng, X. B. Peng, and X. Wang, Gmt: General motion tracking for humanoid whole-body control, arXiv:2506.14770, 2025. 2 [5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He, N. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills, arXiv preprint arXiv:2502.01143, 2025. 2 [6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu, Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion, 2025. 2 [7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani, C. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint arXiv:2406.08858, 2024. 2 [8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and C. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv preprint arXiv:2505.02833, 2025. 2, 5, 7 [9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372. 2 [10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H.", "manipulation, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372. 2 [10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive control via impedance reference tracking for legged robots, arXiv preprint arXiv:2505.06883, 2025. 2, 3, 5 [11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force and position control for legged loco-manipulation, arXiv preprint arXiv:2505.20829, 2025. 2 [12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon: Learning force-adaptive humanoid loco-manipulation, arXiv preprint arXiv:2505.06776, 2025. 2 [13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021. 2 [14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse, P. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole body model predictive control with a memory of motion: Experiments on a torque-controlled talos, in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208. 2 [15] M. Sombolestan", "control with a memory of motion: Experiments on a torque-controlled talos, in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208. 2 [15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of dynamic legged locomotion over uneven terrain, IEEE Transactions on Robotics, 2024. 2, 3 [16] , Hierarchical adaptive loco-manipulation control for quadruped robots, arXiv preprint arXiv:2209.13145, 2022. 2 [17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical optimization-based control for whole-body loco-manipulation of heavy objects, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2 [18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging the sim-to-real gap for athletic loco-manipulation, arXiv preprint arXiv:2502.10894, 2025. 2 [19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and S. Hosoe, Development of a nursing-care assistant robot riba that can lift a human in its arms, in 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2010, pp. 5996 6001. 3 [20] A. E. Block, Huggiebot: An interactive hugging robot with visual and haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3 [21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive", "3 [20] A. E. Block, Huggiebot: An interactive hugging robot with visual and haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3 [21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space force control for humanoid-to-human assistance, IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3 [22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and A. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE Robotics and Automation Letters, 2024. 3 [23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao, and C. Liu, Spark: A modular benchmark for humanoid robot safety, arXiv preprint arXiv:2502.03132, 2025. 3 [24] International Organization for Standardization, Robots and robotic devices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5 [25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s hugging for interactive robot design, in 2016 25th IEEE International Symposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5 [26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing rehabilitation robot, Sensors, vol. 23,", "Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing rehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5 [27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, CoRR, vol. abs/1707.06347, 2017. 5 [28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion retargeting, 2025, gitHub repository. 5 [29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black, AMASS: Archive of motion capture as surface shapes, in International Conference on Computer Vision, Oct. 2019, pp. 5442 5451. 5 [30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Qiao, X. Zhu, J. Liu, R. Zhang et al., Inter-x: Towards versatile human-human interaction analysis, arXiv preprint arXiv:2312.16051, 2023. 5 [31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust motion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5 [32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic dataset of bodies exhibiting detailed lifelike animated motion, in Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun.", "P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic dataset of bodies exhibiting detailed lifelike animated motion, in Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10 [33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas, Prompthmr: Promptable human mesh recovery, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 1148 1159. 11 A PPENDIX B. Reference Dynamics Integration A. External Force Application Logic We apply interaction forces at a subset of upper-body links (shoulders, wrists, hands). The procedure runs every simulation step and consists of: (i) selecting which links are currently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction forces in the robot root frame and integrating the compliant reference, and (iv) applying forces/torques in the simulator. 1) Activation and Gain Scheduling: An active link is a force-application point that is enabled in the current interval; we denote the active set by a binary mask m {0, 1}M over the M candidate links. At the beginning of an interval we sample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset)", "mask m {0, 1}M over the M candidate links. At the beginning of an interval we sample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every active link we assign an interaction spring gain Kspring (t) that varies smoothly over time (piecewise-linear in discrete steps). Gains may gently increase, hold, and then decrease back to zero at the end of the interval. In parallel, a force safety threshold τsafe (t) is adjusted smoothly within a bounded range and later used for clamping and reward shaping. 2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame. We use two behaviors consistent with the two interaction types introduced: (1) Resistive contact: the anchor remains at its previously established location (relative to the root), modeling a resisting load at the current contact site; (2) Guiding contact: the anchor is smoothly moved toward a newly sampled surface point. In both cases the updates are smooth, avoiding discontinuities when the active set or targets change. 3) One-Sided Projection: We model contact as one-sided: interaction forces only act when the link compresses toward the anchor along the intended direction", "when the active set or targets change. 3) One-Sided Projection: We model contact as one-sided: interaction forces only act when the link compresses toward the anchor along the intended direction of interaction; when the link moves away (i.e., leaves the contact side), the interaction force drops to zero. Practically, we compute the displacement from the link to the anchor, take only its component along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral contacts. 4) Application in the Simulator: Forces are applied in world coordinates at the active links. To prevent excessive overall disturbance, we bound the net wrench about the torso: we sum all per-link forces/torques, and if the totals exceed preset limits, we inject an opposite residual on the torso. TABLE II: External Force Application Parameters. Parameter Symbol Typical value / range Max per-link force cap Safety threshold (per link) Net force limit (about torso) Net torque limit (about torso) Interaction spring gain Fmax τsafe (t) τF τM Kspring (t) 30 N 5 15 N (default 10 N) 30 N 20 N m 5 250 All reference quantities are expressed in the robot root tar frame. Let xt , x t be the", "15 N (default 10 N) 30 N 20 N m 5 250 All reference quantities are expressed in the robot root tar frame. Let xt , x t be the current link state and xtar t , x t the target state. The reference dynamics used in this work are tar M x t = fdrive (xtar t , x t , xt , x t ) + finteract ( ) D x t . (8) The driving and interaction forces follow the definitions in the method, and D x t is an additional damping term for stability. We integrate this system with explicit Euler using a small fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity at each step. TABLE III: Reference Dynamics and Integration Parameters. Parameter Symbol Value Virtual mass Integration damping Tracking stiffness Tracking damping Time step Substeps per simulator step Velocity clip Acceleration clip M D Kp Kd t Nsub x max x max 0.1 kg 2.0 Derived from Kp = τsafe /0.05 p 2 M Kp Same as simulation dt = 0.02s 4 4 m/s 1000 m/s2 C. Autonomous Hugging Pipeline For a comfortable hugging experience, ensuring both safety", "= τsafe /0.05 p 2 M Kp Same as simulation dt = 0.02s 4 4 m/s 1000 m/s2 C. Autonomous Hugging Pipeline For a comfortable hugging experience, ensuring both safety and an appropriate hugging position is essential. While our compliant RL policy enforces force limits for safe contact, achieving comfort requires adapting the hugging posture to the person s body shape. To accomplish this, we first estimate the human body shape using BEDLAM [32], and rescale it according to the subject s absolute height obtained from motion capture. We then extract the waist position, denoted as x , as the target contact point. Next, we optimize the default upper-body motion of G1 so that selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal plane. We optimize upper-body joint angles q and a planar floating base r = (x, y, ψ) with fixed height z = z0 . Let p (q, r) be the forward-kinematics position of link , {bk } the target points on the waist, and Πxy the xy-projection. The objective is X 2 min w k p (q, r) bk q, r ( ,k) S 2 + wt Πxy ptorso", "points on the waist, and Πxy the xy-projection. The objective is X 2 min w k p (q, r) bk q, r ( ,k) S 2 + wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront ) + λreg q q0 2 . Here S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight their relative importance, δ 5 cm is a small forward offset for the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the heading. The regularizer q q0 2 keeps the solution close to a neutral upper-body pose. The optimized motion sequence is then updated as a personalized reference motion for the specific individual. After obtaining the target posture and contact locations, the robot must first stand in the proper place. We train a locomotion policy that get the robot human relative pose from motion-capture markers and directly commands joint targets to walk to a stance directly in front of the person, with a 10 cm standoff and frontal alignment. Once this condition is met, control switches to the GentleHumanoid policy to execute the hug. D. Video to Humanoid We use a phone", "a 10 cm standoff and frontal alignment. Once this condition is met, control switches to the GentleHumanoid policy to execute the hug. D. Video to Humanoid We use a phone to record monocular RGB videos, and apply PromptHMR [33] to estimate the corresponding human motion as an SMPL-X motion sequence. The estimated motion is then retargeted to the G1 humanoid using GMR. Finally, we execute the retargeted motion using our trained policy. As shown in the supplementary video, our method remains robust and compliant even when the estimated reference motions are noisy (e.g., with foot skating). It successfully handles interactions with various objects such as pillows, balloons, and baskets of different sizes and deformabilities."], "num_sections": 1, "num_graph_nodes": 49, "num_graph_edges": 94}
{"method": "fixed", "num_chunks": 64, "avg_chunk_len": 790.984375, "std_chunk_len": 68.66460427949305, "max_chunk_len": 800, "min_chunk_len": 246, "total_chars": 50623, "compression_ratio": 1.0004740928036664, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top\n\narXiv:2511.04679v1 [cs.RO] 6 Nov 2025\n\n(a) Sit-to-stand Support\n\n(b) Handshaking\n\n(d) Balloon Handling\n\nGentleHumanoid\n(c) Shape-aware Hugging\n\nVanilla Tracking RL\n\nTracking RL w/ Large Perturbation\n\nFig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits.\nIt enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-", "aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.\nAbstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.\nWe introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is", "a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-\n\nship at Stanford University. Qingzhou is now with Tsinghua University.\n\nshoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our poli", "cy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.\n\nI. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive\n\nwhole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance", "or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.\nIn contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.g., hugging people or handling\nfragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds.\nThis raises two main challenges: (1) coordina", "ting force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces.\nWe address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.\nSince collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncollidin", "g surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.g., shoulder, elbow, w", "rist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.\nWe evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted\n\npressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincludi", "ng gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.\nIn summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.\n We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical", "human robot interaction.\n We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.\nII. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous", "tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.\nB. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand ad", "mittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these\n\nmethods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.", "C. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to", "consistently avoid human collisions [23]. In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes.\nIII. M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces. Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact", "feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig. 2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,\n\n(1)\n\nwhere xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactio", "ns with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.\n\nFor clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,\n\n(2)\n\nwhere xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedanc", "e stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target.\nTo ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp . All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control.\nC. Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often", "correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object.\nGuiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm.\nBoth cases are modeled using the same spring formulation\nwith a consistent anchor terminology:\n\nfinteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as\n\n xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (", "fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and\n\n(a) Reference Dynamics\n\n(c) Deployment\nHRI Motion\n\nContact\n\nHugging\nPlanner\n\ntimestep\n\nVision\n\nDriving\nForce\n\nInteraction\nForce\n\nHugging Motion\n\nMotion Target\nCurrent Pos\nAnchor Pos\n\nGentleHumanoid Policy\nProprioception\n\nPrivileged Obs\n\n(b) Training\nReward\n\nSafe & Compli", "ant Interactions\n\nPolicy\n\nTarget Motion\nAction\n\nSimulator\n\nFig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large de", "formable objects.\n\nD. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the\n\nRight Shoulder Link\n\nRight Elbow Link\n\n0.200\n\nRight Hand Link\n0.16\n\n0.7\n0.175\n0.6\n\n0.14\n\n0.150\n\n0.5\n\nDensity\n\ncorrespond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.\nTo further increase interaction div", "ersity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force. Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions", "on the sphere with magnitudes from 0 to 25 N.\n\n0.4\n\n0.12\n\n0.125\n\n0.10\n\n0.100\n\n0.08\n\n0.3\n\n0.075\n\n0.06\n\n0.2\n\n0.050\n\n0.04\n\n0.1\n\n0.025\n\n0.0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.000\n\n0.02\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.00\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\nFig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.\n\nmaximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nth", "e policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:\n\nτsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive \n\ncompliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e.g., balloons), we set F1 = 5 N\nand", "F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.5 0.5 cm contact area\n(0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.g.,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.\nE. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a", "target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .\nTo incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0.005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM\n\n(6)\n\nref\nref\nxref\nt+1 = xt + t x t+1 .\n\n(7)\n\nWhere t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velo", "cities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.\nThis impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce.\nWe compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design).\nDuring training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel. In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios.\n1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer.\nWe adopt th", "e same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27]. The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,\n\nwhere τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history.\nThe teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum )", ",\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq. 7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.\nBoth policies output joint position targets at R29 which\nare tracked by low-level PD controllers.\n2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [3", "1] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.\n3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance.\nIn GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq. 7:\n\n x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = e", "xp t\nσx\nσv\n\nExponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:\n\nsim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety\n\nmargin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .\n\nrcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe we", "ights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward\n\nVanilla-RL\n\nExtreme-RL\n\nRight Elbow Link\n\n20\n\nRight Shoulder Link\n15\n\n15\n10\n5\n0\n\nForce (N)\n\n20\n\nForce (N)\n\nForce (N)\n\nHere, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:\n\nGentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n10\n\n5\n\n0\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\nTime (s)\n\nFig. 4: Forces applied by different u", "pper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N\n\n !\"#$ =15N\n\nWeight\n !\"#$ =5N\n\nCompliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty\n\n2.0\n2.0\n6.0\n\nGentleHumanoid with different force limits\n\nMotion Tracking\nRoot Tracking\nJoint Tracking\n\n0.5\n1.0\nVanilla-RL\n\nLocomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit\n\n5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0\n\nExtreme-RL\n\nFig. 5: Comparison of interaction forces across pol", "icies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.\n\nIV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbation", "s, representative of prior force-adaptive\nmethods.\nA. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N\n\nwith rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoo", "thly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.\nB. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\npartic", "ularly rigid, requiring a peak force of 51.14 N, while\nVanilla-RL requires 24.59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A\n\nC. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent", "compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising\n\nHugging\nin right position\n\nHugging with\nmisalignment\n\nSensor Pad\nReal-time Pressure\nVisualization\n\nExtreme-RL\n\nHugging with misalignment\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n8\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n3\n\n4\n\n2\n\n0\n\n0\n\n0\n\n0\n\n6\n\n16\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n2\n\n0\n\n17\n\n9\n\n0\n\n0\n\n0\n\n0\n\n50\n\n6\n\n6\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n28\n\n5\n\n5\n\n0\n\n30\n\n7\n\n1\n\n1\n\n1\n\n3\n\n45\n\n9\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n111\n\n5\n\n3\n\n0\n\n11\n\n1\n\n1\n\n1\n\n0\n\n8\n\n34\n\n41\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n19\n\n26\n\n37\n\n2\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n10\n\n5\n\n0\n\n0\n\n6\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1", "7\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n20\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n4\n\n0\n\n0\n\n0\n\n0\n\n38\n\n13\n\n17\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n11\n\n0\n\n16\n\n1\n\n0\n\n0\n\n0\n\n1\n\n3\n\n11\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n12\n\n2\n\n4\n\n1\n\n2\n\n5\n\n1\n\n0\n\n0\n\n0\n\n1\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n9\n\n8\n\n16\n\n2\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n208\n\n61\n\n8\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n2\n\n0\n\n10\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n7\n\n13\n\n0\n\n1\n\n4\n\n7\n\n23\n\n1\n\n0\n\n1\n\n4\n\n11\n\n12\n\n0\n\n4\n\n2\n\n2\n\n0\n\n0\n\n0\n\n2\n\n27\n\n6\n\n0\n\n0\n\n0\n\n19\n\n11\n\n0\n\n0\n\n62\n\n0\n\n2\n\n11\n\n25\n\n4\n\n1\n\n0\n\n0\n\n4\n\n2\n\n52\n\n7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n5\n\n5\n\n0\n\n2\n\n3\n\n64\n\n4\n\n0\n\n0\n\n0\n\n22\n\n6\n\n83\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n2\n\n6\n\n15\n\n0\n\n0\n\n0\n\n2\n\n17\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n0\n\n16\n\n417\n\n10", "1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n2\n\n19\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n551\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n14\n\n16\n\nGentleHumanoid\n\nVanilla-RL\n\nHugging in right position\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\nExtreme-RL\n\nHugging with misalignment\n30\n\n25\n\n25\n\nPeak Force (N)\n\n30\n\n20\n15\n10\n5\n0\n\n400\n\nPressure (kPa)\n\nVanilla-RL\n\nGentleHumanoid\n\nHugging in right position\n\nForce (N)\n\nkey observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.\nFor example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges", "between 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.\n2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.\nPressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nse", "nsor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.\n3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force thre", "shold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it.\nAcross all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.\n\n20\n15\n10\n5\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\n14\n\n16\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\nFig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumano", "id maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.\n\nfor healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions.\nWe also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c). From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [3", "2] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner. Experiments\n\nwith participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions.\nV. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots. By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demons", "trations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction.\nSeveral limitations remain. First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage. Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoel", "astic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation. Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.\nVI. ACKNOWLEDGMENT\nWe would like to", "thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset. We further thank the\nUnitree team for their timely and reliable hardware support.\nR EFERENCES\n[1] D. J. Agravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2\n[2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024. 2, 5\n[3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang", ",\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412.13196, 2024. 2\n[4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506.14770, 2025. 2\n\n[5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He,\nN. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.01143, 2025. 2\n[6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\nC. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid w", "hole-body teleoperation and learning, arXiv preprint\narXiv:2406.08858, 2024. 2\n[8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and\nC. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505.02833, 2025. 2, 5, 7\n[9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372.\n2\n[10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505.06883, 2025. 2, 3, 5\n[11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.20829, 20", "25. 2\n[12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.06776, 2025. 2\n[13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021.\n2\n[14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse,\nP. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (IC", "RA). IEEE, 2021, pp. 8202 8208. 2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024. 2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209.13145, 2022. 2\n[17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2\n[18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502.10894, 2025. 2\n[19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and\nS. Hosoe, Deve", "lopment of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A. E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3\n[22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024. 3\n[23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao,\nand C. Liu, Spark: A modular benchmark for", "humanoid robot safety, \narXiv preprint arXiv:2502.03132, 2025. 3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5\n[25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5\n[26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5\n\n[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms,", "CoRR,\nvol. abs/1707.06347, 2017. 5\n[28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.\nBlack, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451. 5\n[30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Qiao, X. Zhu, J. Liu,\nR. Zhang et al., Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.16051, 2023. 5\n[31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5\n[32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motio", "n, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10\n[33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.\n1148 1159. 11\n\nA PPENDIX\n\nB. Reference Dynamics Integration\n\nA. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) appl", "ying forces/torques in the simulator.\n1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).\nGains may gently increase, hold, and then decrease back to\nzero at the end of the interval.\nIn parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping.\n2) Anchor (Interaction Spring Origin) Upda", "te: Each active link maintains an anchor o(t) in the robot root frame.\nWe use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change.\n3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.e., leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacem", "ent from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.\n4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.\nTABLE II: External Force Application Parameters.\nParameter\n\nSymbol\n\nTypical value / range\n\nMax per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain\n\nFmax\nτsafe (t)\nτF\nτM\nKspring (t)\n\n30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250\n\nAll reference quan", "tities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)\n\nThe driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter\n\nSymbol\n\nValue\n\nVirtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip\n\nM\nD\nKp\nKd\n t\nNsub", "x max\n x max\n\n0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2\n\nC. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.\nWhile our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.\nNext, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays pro", "perly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.\nThe objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r\n\n( ,k) S\n\n2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nsp", "ecific individual.\n\nAfter obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.\nD. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.\nFinally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our metho", "d\nremains robust and compliant even when the estimated\nreference motions are noisy (e.g., with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "sliding", "num_chunks": 126, "avg_chunk_len": 798.3888888888889, "std_chunk_len": 13.734426807035415, "max_chunk_len": 800, "min_chunk_len": 645, "total_chars": 100597, "compression_ratio": 0.5034643180214121, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top\n\narXiv:2511.04679v1 [cs.RO] 6 Nov 2025\n\n(a) Sit-to-stand Support\n\n(b) Handshaking\n\n(d) Balloon Handling\n\nGentleHumanoid\n(c) Shape-aware Hugging\n\nVanilla Tracking RL\n\nTracking RL w/ Large Perturbation\n\nFig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits.\nIt enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-", "king RL w/ Large Perturbation\n\nFig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits.\nIt enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.\nAbstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (R", "aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.\nAbstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.\nWe introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is", "L) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.\nWe introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-\n\nship at Stanford University. Qingzhou is now with Tsinghua University.", "a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-\n\nship at Stanford University. Qingzhou is now with Tsinghua University.\n\nshoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our poli", "shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.\n\nI. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered en", "cy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.\n\nI. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive\n\nwhole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance", "vironments. Reinforcement learning (RL) has recently enabled impressive\n\nwhole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.\nIn contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe", "or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.\nIn contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.g., hugging people or handling\nfragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds.\nThis raises two main challenges: (1) coordina", "entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.g., hugging people or handling\nfragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds.\nThis raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces.\nWe address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two force", "ting force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces.\nWe address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.\nSince collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncollidin", "s: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.\nSince collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surfa", "g surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.g., shoulder, elbow, w", "ce, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.g., shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.\nWe evaluate GentleHumanoid against baselines, including\na vanilla whole-body", "rist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.\nWe evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted\n\npressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincludi", "RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted\n\npressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.\nIn summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with", "ng gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.\nIn summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.\n We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical", "upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.\n We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction.\n We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.\nII. R", "human robot interaction.\n We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.\nII. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous", "ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for", "tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.\nB. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand ad", "whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.\nB. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these\n\nmethods typically focus on end-effector interactio", "mittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these\n\nmethods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.", "ns rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.\nC. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied\ntraditional control methods", "C. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to", "to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23]. In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes.\nIII. M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control tha", "consistently avoid human collisions [23]. In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes.\nIII. M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces. Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact", "t\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces. Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig. 2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or obj", "feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig. 2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,\n\n(1)\n\nwhere xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactio", "ects:\nM x i = fdrive,i + finteract,i ,\n\n(1)\n\nwhere xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.\n\nFor clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate", "ns with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.\n\nFor clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,\n\n(2)\n\nwhere xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedanc", "driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,\n\n(2)\n\nwhere xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target.\nTo ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp . All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to co", "e stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target.\nTo ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp . All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control.\nC. Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often", "ordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control.\nC. Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object.\nGuiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s", "correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object.\nGuiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm.\nBoth cases are modeled using the same spring formulation\nwith a consistent anchor terminology:\n\nfinteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as\n\n xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (", "arm.\nBoth cases are modeled using the same spring formulation\nwith a consistent anchor terminology:\n\nfinteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as\n\n xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally define", "fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and\n\n(a) Reference Dynamics\n\n(c) Deployment\nHRI Motion\n\nContact\n\nHugging\nPlanner\n\ntimestep\n\nVision\n\nDriving\nForce\n\nInteraction\nForce\n\nHugging Motion\n\nMotion Target\nCurrent Pos\nAnchor Pos\n\nGentleHumanoid Policy\nProprioception\n\nPrivileged Obs\n\n(b) Training\nReward\n\nSafe & Compli", "d postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and\n\n(a) Reference Dynamics\n\n(c) Deployment\nHRI Motion\n\nContact\n\nHugging\nPlanner\n\ntimestep\n\nVision\n\nDriving\nForce\n\nInteraction\nForce\n\nHugging Motion\n\nMotion Target\nCurrent Pos\nAnchor Pos\n\nGentleHumanoid Policy\nProprioception\n\nPrivileged Obs\n\n(b) Training\nReward\n\nSafe & Compliant Interactions\n\nPolicy\n\nTarget Motion\nAction\n\nSimulator\n\nFig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and ta", "ant Interactions\n\nPolicy\n\nTarget Motion\nAction\n\nSimulator\n\nFig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large de", "rget motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects.\n\nD. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the\n\nRight Shoulder Link\n\nRight Elbow Link\n\n0.200", "formable objects.\n\nD. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the\n\nRight Shoulder Link\n\nRight Elbow Link\n\n0.200\n\nRight Hand Link\n0.16\n\n0.7\n0.175\n0.6\n\n0.14\n\n0.150\n\n0.5\n\nDensity\n\ncorrespond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.\nTo further increase interaction div", "Right Hand Link\n0.16\n\n0.7\n0.175\n0.6\n\n0.14\n\n0.150\n\n0.5\n\nDensity\n\ncorrespond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.\nTo further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force. Anchors and selections are\nresampled every 5 seconds", "ersity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force. Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions", "with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.\n\n0.4\n\n0.12\n\n0.125\n\n0.10\n\n0.100\n\n0.08\n\n0.3\n\n0.075\n\n0.06\n\n0.2\n\n0.050\n\n0.04\n\n0.1\n\n0.025\n\n0.0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.000\n\n0.02\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.00\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\nFig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of f", "on the sphere with magnitudes from 0 to 25 N.\n\n0.4\n\n0.12\n\n0.125\n\n0.10\n\n0.100\n\n0.08\n\n0.3\n\n0.075\n\n0.06\n\n0.2\n\n0.050\n\n0.04\n\n0.1\n\n0.025\n\n0.0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.000\n\n0.02\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.00\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\nFig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.\n\nmaximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nth", "orce magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.\n\nmaximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:\n\nτsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive \n\ncompliance. The thres", "e policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:\n\nτsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive \n\ncompliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e.g., balloons), we set F1 = 5 N\nand", "hold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e.g., balloons), we set F1 = 5 N\nand F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.5 0.5 cm contact area\n(0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.g.,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto", "F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.5 0.5 cm contact area\n(0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.g.,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.\nE. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a", "3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.\nE. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .\nTo incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0.005 s:\nfdri", "target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .\nTo incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0.005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM\n\n(6)\n\nref\nref\nxref\nt+1 = xt + t x t+1 .\n\n(7)\n\nWhere t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velo", "ve +finteract\nref\n,\nx ref\nt+1 = x t + t \nM\n\n(6)\n\nref\nref\nxref\nt+1 = xt + t x t+1 .\n\n(7)\n\nWhere t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.\nThis impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce.\nWe compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design).\nDuring training, the RL", "cities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.\nThis impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce.\nWe compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design).\nDuring training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel. In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios.\n1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer.\nWe adopt th", "agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel. In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios.\n1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer.\nWe adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27]. The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,\n\nwhere τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information incl", "e same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27]. The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,\n\nwhere τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history.\nThe teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum )", "uding future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history.\nThe teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq. 7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are t", ",\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq. 7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.\nBoth policies output joint position targets at R29 which\nare tracked by low-level PD controllers.\n2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [3", "he previous joint torques; and ecum denotes\nthe cumulative tracking error.\nBoth policies output joint position targets at R29 which\nare tracked by low-level PD controllers.\n2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.\n3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and", "1] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.\n3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance.\nIn GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq. 7:\n\n x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = e", "stable balance.\nIn GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq. 7:\n\n x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = exp t\nσx\nσv\n\nExponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:\n\nsim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tr", "xp t\nσx\nσv\n\nExponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:\n\nsim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety\n\nmargin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .\n\nrcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe we", "acking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety\n\nmargin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .\n\nrcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward\n\nVanilla-RL\n\nExtreme-RL\n\nRight Elbow Link\n\n20\n\nRight Shoulder Link\n15\n\n15\n10\n5\n0\n\nForce (N)\n\n20\n\nForce (N)\n\nForce (N)\n\nHere, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the pol", "ights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward\n\nVanilla-RL\n\nExtreme-RL\n\nRight Elbow Link\n\n20\n\nRight Shoulder Link\n15\n\n15\n10\n5\n0\n\nForce (N)\n\n20\n\nForce (N)\n\nForce (N)\n\nHere, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:\n\nGentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n10\n\n5\n\n0\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\nTime (s)\n\nFig. 4: Forces applied by different u", "icy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:\n\nGentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n10\n\n5\n\n0\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\nTime (s)\n\nFig. 4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N\n\n !\"#$ =15N\n\nWeight\n !\"#$ =5N\n\nCompliance\nRefe", "pper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N\n\n !\"#$ =15N\n\nWeight\n !\"#$ =5N\n\nCompliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty\n\n2.0\n2.0\n6.0\n\nGentleHumanoid with different force limits\n\nMotion Tracking\nRoot Tracking\nJoint Tracking\n\n0.5\n1.0\nVanilla-RL\n\nLocomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit\n\n5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0\n\nExtreme-RL\n\nFig. 5: Comparison of interaction forces across pol", "rence Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty\n\n2.0\n2.0\n6.0\n\nGentleHumanoid with different force limits\n\nMotion Tracking\nRoot Tracking\nJoint Tracking\n\n0.5\n1.0\nVanilla-RL\n\nLocomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit\n\n5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0\n\nExtreme-RL\n\nFig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.\n\nIV. E XPERIMENTS\nWe conduct both simul", "icies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.\n\nIV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbation", "ation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods.\nA. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more", "s, representative of prior force-adaptive\nmethods.\nA. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N\n\nwith rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoo", "stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N\n\nwith rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.\nB. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces a", "thly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.\nB. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\npartic", "t the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\nparticularly rigid, requiring a peak force of 51.14 N, while\nVanilla-RL requires 24.59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A\n\nC. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1,", "ularly rigid, requiring a peak force of 51.14 N, while\nVanilla-RL requires 24.59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A\n\nC. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent", "allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising\n\nHugging\nin right position\n\nHugging with\nmisalignment\n\nSensor Pad\nReal-time Pressure\nVisualization\n\nExtreme-RL\n\nHugging with misalignment\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n8\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n3\n\n4\n\n2\n\n0\n\n0\n\n0\n\n0\n\n6\n\n16\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n2\n\n0", "compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising\n\nHugging\nin right position\n\nHugging with\nmisalignment\n\nSensor Pad\nReal-time Pressure\nVisualization\n\nExtreme-RL\n\nHugging with misalignment\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n8\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n3\n\n4\n\n2\n\n0\n\n0\n\n0\n\n0\n\n6\n\n16\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n2\n\n0\n\n17\n\n9\n\n0\n\n0\n\n0\n\n0\n\n50\n\n6\n\n6\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n28\n\n5\n\n5\n\n0\n\n30\n\n7\n\n1\n\n1\n\n1\n\n3\n\n45\n\n9\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n111\n\n5\n\n3\n\n0\n\n11\n\n1\n\n1\n\n1\n\n0\n\n8\n\n34\n\n41\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n19\n\n26\n\n37\n\n2\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n10\n\n5\n\n0\n\n0\n\n6\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1", "17\n\n9\n\n0\n\n0\n\n0\n\n0\n\n50\n\n6\n\n6\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n28\n\n5\n\n5\n\n0\n\n30\n\n7\n\n1\n\n1\n\n1\n\n3\n\n45\n\n9\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n111\n\n5\n\n3\n\n0\n\n11\n\n1\n\n1\n\n1\n\n0\n\n8\n\n34\n\n41\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n19\n\n26\n\n37\n\n2\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n10\n\n5\n\n0\n\n0\n\n6\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n20\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n4\n\n0\n\n0\n\n0\n\n0\n\n38\n\n13\n\n17\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n11\n\n0\n\n16\n\n1\n\n0\n\n0\n\n0\n\n1\n\n3\n\n11\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n12\n\n2\n\n4\n\n1\n\n2\n\n5\n\n1\n\n0\n\n0\n\n0\n\n1\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n9\n\n8\n\n16\n\n2\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n208\n\n61\n\n8\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0", "7\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n20\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n4\n\n0\n\n0\n\n0\n\n0\n\n38\n\n13\n\n17\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n11\n\n0\n\n16\n\n1\n\n0\n\n0\n\n0\n\n1\n\n3\n\n11\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n12\n\n2\n\n4\n\n1\n\n2\n\n5\n\n1\n\n0\n\n0\n\n0\n\n1\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n9\n\n8\n\n16\n\n2\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n208\n\n61\n\n8\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n2\n\n0\n\n10\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n7\n\n13\n\n0\n\n1\n\n4\n\n7\n\n23\n\n1\n\n0\n\n1\n\n4\n\n11\n\n12\n\n0\n\n4\n\n2\n\n2\n\n0\n\n0\n\n0\n\n2\n\n27\n\n6\n\n0\n\n0\n\n0\n\n19\n\n11\n\n0\n\n0\n\n62\n\n0\n\n2\n\n11\n\n25\n\n4\n\n1\n\n0\n\n0\n\n4\n\n2\n\n52\n\n7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n5\n\n5\n\n0\n\n2\n\n3\n\n64\n\n4\n\n0\n\n0\n\n0\n\n22\n\n6\n\n83\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n2\n\n6\n\n15\n\n0\n\n0\n\n0\n\n2\n\n17\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n0\n\n16\n\n417\n\n10", "0\n\n0\n\n0\n\n0\n\n1\n\n2\n\n0\n\n10\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n7\n\n13\n\n0\n\n1\n\n4\n\n7\n\n23\n\n1\n\n0\n\n1\n\n4\n\n11\n\n12\n\n0\n\n4\n\n2\n\n2\n\n0\n\n0\n\n0\n\n2\n\n27\n\n6\n\n0\n\n0\n\n0\n\n19\n\n11\n\n0\n\n0\n\n62\n\n0\n\n2\n\n11\n\n25\n\n4\n\n1\n\n0\n\n0\n\n4\n\n2\n\n52\n\n7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n5\n\n5\n\n0\n\n2\n\n3\n\n64\n\n4\n\n0\n\n0\n\n0\n\n22\n\n6\n\n83\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n2\n\n6\n\n15\n\n0\n\n0\n\n0\n\n2\n\n17\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n0\n\n16\n\n417\n\n10\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n2\n\n19\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n551\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n14\n\n16\n\nGentleHumanoid\n\nVanilla-RL\n\nHugging in right position\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\nExtreme-RL\n\nHugging with misalignment\n30\n\n25\n\n25\n\nPeak Force (N)\n\n30\n\n20\n15\n10\n5\n0\n\n400\n\nPressure (kPa)\n\nVanilla-RL\n\nGentleHum", "1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n2\n\n19\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n551\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n14\n\n16\n\nGentleHumanoid\n\nVanilla-RL\n\nHugging in right position\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\nExtreme-RL\n\nHugging with misalignment\n30\n\n25\n\n25\n\nPeak Force (N)\n\n30\n\n20\n15\n10\n5\n0\n\n400\n\nPressure (kPa)\n\nVanilla-RL\n\nGentleHumanoid\n\nHugging in right position\n\nForce (N)\n\nkey observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.\nFor example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges", "anoid\n\nHugging in right position\n\nForce (N)\n\nkey observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.\nFor example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.\n2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis p", "between 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.\n2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.\nPressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nse", "roperly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.\nPressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla", "nsor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.\n3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force thre", "-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.\n3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it.\nAcross all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.\n\n20\n1", "shold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it.\nAcross all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.\n\n20\n15\n10\n5\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\n14\n\n16\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\nFig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumano", "5\n10\n5\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\n14\n\n16\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\nFig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.\n\nfor healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions.\nWe also develop an autonom", "id maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.\n\nfor healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions.\nWe also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c). From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [3", "ous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c). From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner. Experiments\n\nwith participants of varying heights and builds show that the\npipeline generates sta", "2] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner. Experiments\n\nwith participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions.\nV. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots. By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demons", "ble and comfortable hugging motions.\nV. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots. By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction.\nSeveral limitations remain. First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoul", "trations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction.\nSeveral limitations remain. First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage. Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoel", "der are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage. Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation. Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionba", "astic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation. Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.\nVI. ACKNOWLEDGMENT\nWe would like to", "sed pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.\nVI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset. We further thank the\nUnitree team for their timely and reliable hardware support.\nR EFERENCES\n[1] D. J. Ag", "thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset. We further thank the\nUnitree team for their timely and reliable hardware support.\nR EFERENCES\n[1] D. J. Agravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2\n[2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024. 2, 5\n[3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang", "ravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2\n[2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024. 2, 5\n[3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412.13196, 2024. 2\n[4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506.14770, 2025. 2\n\n[5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He,\nN. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world\nphy", ",\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412.13196, 2024. 2\n[4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506.14770, 2025. 2\n\n[5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He,\nN. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.01143, 2025. 2\n[6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\nC. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid w", "sics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.01143, 2025. 2\n[6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\nC. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406.08858, 2024. 2\n[8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and\nC. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505.02833, 2025. 2, 5, 7\n[9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non", "hole-body teleoperation and learning, arXiv preprint\narXiv:2406.08858, 2024. 2\n[8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and\nC. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505.02833, 2025. 2, 5, 7\n[9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372.\n2\n[10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505.06883, 2025. 2, 3, 5\n[11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.20829, 20", "Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372.\n2\n[10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505.06883, 2025. 2, 3, 5\n[11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.20829, 2025. 2\n[12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.06776, 2025. 2\n[13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and sta", "25. 2\n[12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.06776, 2025. 2\n[13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021.\n2\n[14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse,\nP. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (IC", "bilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021.\n2\n[14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse,\nP. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208. 2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024. 2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209.13145, 2022. 2\n[17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for", "RA). IEEE, 2021, pp. 8202 8208. 2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024. 2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209.13145, 2022. 2\n[17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2\n[18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502.10894, 2025. 2\n[19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and\nS. Hosoe, Deve", "whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2\n[18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502.10894, 2025. 2\n[19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and\nS. Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A. E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space\nforce control f", "lopment of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A. E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3\n[22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024. 3\n[23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao,\nand C. Liu, Spark: A modular benchmark for", "or humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3\n[22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024. 3\n[23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502.03132, 2025. 3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5\n[25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSy", "humanoid robot safety, \narXiv preprint arXiv:2502.03132, 2025. 3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5\n[25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5\n[26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5\n\n[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms,", "mposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5\n[26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5\n\n[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol. abs/1707.06347, 2017. 5\n[28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.\nBlack, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451. 5\n[30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Q", "CoRR,\nvol. abs/1707.06347, 2017. 5\n[28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.\nBlack, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451. 5\n[30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Qiao, X. Zhu, J. Liu,\nR. Zhang et al., Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.16051, 2023. 5\n[31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5\n[32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motio", "iao, X. Zhu, J. Liu,\nR. Zhang et al., Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.16051, 2023. 5\n[31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5\n[32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10\n[33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.\n1148 1159. 11\n\nA PPENDIX\n\nB. Reference Dynamics Integration\n\nA. External Force A", "n, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10\n[33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.\n1148 1159. 11\n\nA PPENDIX\n\nB. Reference Dynamics Integration\n\nA. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) appl", "pplication Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator.\n1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nacti", "ying forces/torques in the simulator.\n1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).\nGains may gently increase, hold, and then decrease back to\nzero at the end of the interval.\nIn parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping.\n2) Anchor (Interaction Spring Origin) Upda", "ve link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).\nGains may gently increase, hold, and then decrease back to\nzero at the end of the interval.\nIn parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping.\n2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame.\nWe use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In bot", "te: Each active link maintains an anchor o(t) in the robot root frame.\nWe use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change.\n3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.e., leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacem", "h cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change.\n3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.e., leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.\n4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the", "ent from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.\n4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.\nTABLE II: External Force Application Parameters.\nParameter\n\nSymbol\n\nTypical value / range\n\nMax per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain\n\nFmax\nτsafe (t)\nτF\nτM\nKspring (t)\n\n30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250\n\nAll reference quan", "totals exceed\npreset limits, we inject an opposite residual on the torso.\nTABLE II: External Force Application Parameters.\nParameter\n\nSymbol\n\nTypical value / range\n\nMax per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain\n\nFmax\nτsafe (t)\nτF\nτM\nKspring (t)\n\n30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250\n\nAll reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)\n\nThe driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system wit", "tities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)\n\nThe driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter\n\nSymbol\n\nValue\n\nVirtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip\n\nM\nD\nKp\nKd\n t\nNsub", "h explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter\n\nSymbol\n\nValue\n\nVirtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip\n\nM\nD\nKp\nKd\n t\nNsub\n x max\n x max\n\n0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2\n\nC. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.\nWhile our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s bod", "x max\n x max\n\n0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2\n\nC. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.\nWhile our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.\nNext, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays pro", "y shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.\nNext, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.\nThe objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r\n\n( ,k) S\n\n2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S col", "perly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.\nThe objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r\n\n( ,k) S\n\n2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nsp", "lects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.\n\nAfter obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches", "ecific individual.\n\nAfter obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.\nD. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.\nFinally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our metho", "to the GentleHumanoid policy to\nexecute the hug.\nD. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.\nFinally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e.g., with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "sentence", "num_chunks": 219, "avg_chunk_len": 230.62100456621005, "std_chunk_len": 242.62336732391128, "max_chunk_len": 2457, "min_chunk_len": 15, "total_chars": 50506, "compression_ratio": 1.0027917475151467, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid. axell.", "top\n\narXiv:2511. 04679v1 [cs. RO] 6 Nov 2025\n\n(a) Sit-to-stand Support\n\n(b) Handshaking\n\n(d) Balloon Handling\n\nGentleHumanoid\n(c) Shape-aware Hugging\n\nVanilla Tracking RL\n\nTracking RL w/ Large Perturbation\n\nFig.", "1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits. It enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail. Abstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential.", "However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance.", "At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-\n\nship at Stanford University.", "Qingzhou is now with Tsinghua University. shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds.", "We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.", "I. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive\n\nwhole-body locomotion and manipulation [1] [8].", "However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.", "In contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e. g.", ", hugging people or handling\nfragile objects) to firm, supportive assistance (e. g. , sit-tostand), while always remaining within safe force thresholds.", "This raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces. We address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.", "Since collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios.", "To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e. g.", ", shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.", "We evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted\n\npressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation.", "We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs. In summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.", "We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction. We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.", "II. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics.", "The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods.", "In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.", "B. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18].", "However, these\n\nmethods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.", "C. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20].", "More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23].", "In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes. III.", "M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces.", "Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig.", "2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,\n\n(1)\n\nwhere xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0. 1 kg\nin our reference dynamics model.", "The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component. For clarity, we introduce the index i once and omit it\nhenceforth.", "All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame. B. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory.", "The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,\n\n(2)\n\nwhere xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target. To ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp .", "All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control. C.", "Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities.", "We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object. Guiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm. Both cases are modeled using the same spring formulation\nwith a consistent anchor terminology:\n\nfinteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as\n\n xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.", "sample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration. This formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures. Posture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and\n\n(a) Reference Dynamics\n\n(c) Deployment\nHRI Motion\n\nContact\n\nHugging\nPlanner\n\ntimestep\n\nVision\n\nDriving\nForce\n\nInteraction\nForce\n\nHugging Motion\n\nMotion Target\nCurrent Pos\nAnchor Pos\n\nGentleHumanoid Policy\nProprioception\n\nPrivileged Obs\n\n(b) Training\nReward\n\nSafe & Compliant Interactions\n\nPolicy\n\nTarget Motion\nAction\n\nSimulator\n\nFig.", "2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ).", "(c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects. D. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error.", "Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the\n\nRight Shoulder Link\n\nRight Elbow Link\n\n0. 200\n\nRight Hand Link\n0.", "16\n\n0. 7\n0. 175\n0.", "6\n\n0. 14\n\n0. 150\n\n0.", "5\n\nDensity\n\ncorrespond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.", "To further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force.", "Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.", "0. 4\n\n0. 12\n\n0.", "125\n\n0. 10\n\n0. 100\n\n0.", "08\n\n0. 3\n\n0. 075\n\n0.", "06\n\n0. 2\n\n0. 050\n\n0.", "04\n\n0. 1\n\n0. 025\n\n0.", "0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0. 000\n\n0. 02\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.", "00\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\nFig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right).", "Insets (top right) illustrate the corresponding\nforce directions on a sphere. maximum allowable force applied by the robot. We define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 .", "The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits. The current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks.", "When the driving force exceeds the threshold, we\napply a scaling mechanism:\n\nτsafe\nfdrive limited = min 1. 0,\n fdrive ,\n(5)\n fdrive \n\ncompliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds.", "The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e. g.", ", balloons), we set F1 = 5 N\nand F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.", "5 0. 5 cm contact area\n(0. 25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.", "g. ,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26].", "Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band. E. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar .", "The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract . To incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0. 005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM\n\n(6)\n\nref\nref\nxref\nt+1 = xt + t x t+1 .", "(7)\n\nWhere t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.", "This impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce. We compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design). During training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel.", "In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios. 1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer. We adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27].", "The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,\n\nwhere τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history. The teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq.", "7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.", "Both policies output joint position targets at R29 which\nare tracked by low-level PD controllers. 2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.", "3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance. In GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq.", "7:\n\n x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n. rdyn = exp t\nσx\nσv\n\nExponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity. Reference Force Tracking.", "To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:\n\nsim\n finteract finteract\n 2\nrforce = exp \n. σf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds. Unsafe Force Penalty.", "To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety\n\nmargin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) . rcompliance = wdyn rdyn + wforce rforce + wpen rpen .", "The weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I. TABLE I: Reward Terms and Weights. Reward\n\nVanilla-RL\n\nExtreme-RL\n\nRight Elbow Link\n\n20\n\nRight Shoulder Link\n15\n\n15\n10\n5\n0\n\nForce (N)\n\n20\n\nForce (N)\n\nForce (N)\n\nHere, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties.", "This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations. The overall compliance reward is a weighted sum of these\nterms:\n\nGentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n10\n\n5\n\n0\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\nTime (s)\n\nFig.", "4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.", "! \"#$ =10N\n\n ! \"#$ =15N\n\nWeight\n !", "\"#$ =5N\n\nCompliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty\n\n2. 0\n2. 0\n6.", "0\n\nGentleHumanoid with different force limits\n\nMotion Tracking\nRoot Tracking\nJoint Tracking\n\n0. 5\n1. 0\nVanilla-RL\n\nLocomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit\n\n5.", "0\n10. 0\n4. 0\n2.", "0\n0. 1\n5. 0e-4\n1.", "0\n\nExtreme-RL\n\nFig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures.", "Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses. Force gauge readings (N) are highlighted for clarity. IV.", "E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods. A.", "Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks.", "At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N\n\nwith rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.", "B. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force.", "We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values.", "As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\nparticularly rigid, requiring a peak force of 51. 14 N, while\nVanilla-RL requires 24.", "59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A\n\nC.", "More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling. Demonstrations of joystick-based control are provided in the\nsupplementary video.", "While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising\n\nHugging\nin right position\n\nHugging with\nmisalignment\n\nSensor Pad\nReal-time Pressure\nVisualization\n\nExtreme-RL\n\nHugging with misalignment\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n8\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n3\n\n4\n\n2\n\n0\n\n0\n\n0\n\n0\n\n6\n\n16\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n2\n\n0\n\n17\n\n9\n\n0\n\n0\n\n0\n\n0\n\n50\n\n6\n\n6\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n28\n\n5\n\n5\n\n0\n\n30\n\n7\n\n1\n\n1\n\n1\n\n3\n\n45\n\n9\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n111\n\n5\n\n3\n\n0\n\n11\n\n1\n\n1\n\n1\n\n0\n\n8\n\n34\n\n41\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n19\n\n26\n\n37\n\n2\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n10\n\n5\n\n0\n\n0\n\n6\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n20\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n4\n\n0\n\n0\n\n0\n\n0\n\n38\n\n13\n\n17\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n11\n\n0\n\n16\n\n1\n\n0\n\n0\n\n0\n\n1\n\n3\n\n11\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n12\n\n2\n\n4\n\n1\n\n2\n\n5\n\n1\n\n0\n\n0\n\n0\n\n1\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n9\n\n8\n\n16\n\n2\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n208\n\n61\n\n8\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n2\n\n0\n\n10\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n7\n\n13\n\n0\n\n1\n\n4\n\n7\n\n23\n\n1\n\n0\n\n1\n\n4\n\n11\n\n12\n\n0\n\n4\n\n2\n\n2\n\n0\n\n0\n\n0\n\n2\n\n27\n\n6\n\n0\n\n0\n\n0\n\n19\n\n11\n\n0\n\n0\n\n62\n\n0\n\n2\n\n11\n\n25\n\n4\n\n1\n\n0\n\n0\n\n4\n\n2\n\n52\n\n7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n5\n\n5\n\n0\n\n2\n\n3\n\n64\n\n4\n\n0\n\n0\n\n0\n\n22\n\n6\n\n83\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n2\n\n6\n\n15\n\n0\n\n0\n\n0\n\n2\n\n17\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n0\n\n16\n\n417\n\n10\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n2\n\n19\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n551\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n14\n\n16\n\nGentleHumanoid\n\nVanilla-RL\n\nHugging in right position\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\nExtreme-RL\n\nHugging with misalignment\n30\n\n25\n\n25\n\nPeak Force (N)\n\n30\n\n20\n15\n10\n5\n0\n\n400\n\nPressure (kPa)\n\nVanilla-RL\n\nGentleHumanoid\n\nHugging in right position\n\nForce (N)\n\nkey observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.", "For example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.", "2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion.", "In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact. Pressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines.", "For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.", "3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse.", "For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it. Across all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.", "20\n15\n10\n5\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\n14\n\n16\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\nFig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization.", "Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.", "for healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions. We also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c).", "From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner.", "Experiments\n\nwith participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions. V. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots.", "By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction. Several limitations remain.", "First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage.", "Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation.", "Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.", "VI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset.", "We further thank the\nUnitree team for their timely and reliable hardware support. R EFERENCES\n[1] D. J.", "Agravante, A. Cherubini, A. Sherikov, P.", "-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol.", "35, no. 4, pp. 833 846, 2019.", "2\n[2] Z. Fu, Q. Zhao, Q.", "Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024.", "2, 5\n[3] M. Ji, X. Peng, F.", "Liu, J. Li, G. Yang, X.", "Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412. 13196, 2024.", "2\n[4] Z. Chen, M. Ji, X.", "Cheng, X. Peng, X. B.", "Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506. 14770, 2025.", "2\n\n[5] T. He, J. Gao, W.", "Xiao, Y. Zhang, Z. Wang, J.", "Wang, Z. Luo, G. He,\nN.", "Sobanbab, C. Pan et al. , Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.", "01143, 2025. 2\n[6] Q. Liao, T.", "E. Truong, X. Huang, G.", "Tevet, K. Sreenath, and C. K.", "Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z.", "Luo, X. He, W. Xiao, C.", "Zhang, W. Zhang, K. Kitani,\nC.", "Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406. 08858, 2024.", "2\n[8] Y. Ze, Z. Chen, J.", "P. Arau jo, Z. ang Cao, X.", "B. Peng, J. Wu, and\nC.", "K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505. 02833, 2025.", "2, 5, 7\n[9] T. Portela, G. B.", "Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA).", "IEEE, 2024, pp. 15 366 15 372. 2\n[10] B.", "Xu, H. Weng, Q. Lu, Y.", "Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505. 06883, 2025.", "2, 3, 5\n[11] P. Zhi, P. Li, J.", "Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.", "20829, 2025. 2\n[12] Y. Zhang, Y.", "Yuan, P. Gurunath, T. He, S.", "Omidshafiei, A. -a. Aghamohammadi, M.", "Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.", "06776, 2025. 2\n[13] M. Murooka, K.", "Chappellet, A. Tanguy, M. Benallegue, I.", "Kumagai, M. Morisawa, F. Kanehiro, and A.", "Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp.", "5597 5604, 2021. 2\n[14] E. Dantec, R.", "Budhiraja, A. Roig, T. Lembono, G.", "Saurel, O. Stasse,\nP. Fernbach, S.", "Tonneau, S. Vijayakumar, S. Calinon et al.", ", Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208.", "2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024.", "2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209. 13145, 2022. 2\n[17] A.", "Rigo, M. Hu, S. K.", "Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp.", "15 322 15 328. 2\n[18] N. Fey, G.", "B. Margolis, M. Peticco, and P.", "Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502. 10894, 2025. 2\n[19] T.", "Mukai, S. Hirano, H. Nakashima, Y.", "Kato, Y. Sakaida, S. Guo, and\nS.", "Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A.", "E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph. D.", "dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S.", "Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no.", "3, pp. 5705 5712, 2021. 3\n[22] H.", "Lefe vre, T. Chaki, T. Kawakami, A.", "Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024.", "3\n[23] Y. Sun, R. Chen, K.", "S. Yun, Y. Fang, S.", "Jung, F. Li, B. Li, W.", "Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502. 03132, 2025.", "3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016.", "5\n[25] J. Kim, A. Alspach, I.", "Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp.", "557 561. 5\n[26] Y. Nam, S.", "Yang, J. Kim, B. Koo, S.", "Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no.", "4, p. 2206, 2023. 5\n\n[27] J.", "Schulman, F. Wolski, P. Dhariwal, A.", "Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol. abs/1707.", "06347, 2017. 5\n[28] Y. Ze, J.", "P. Arau jo, J. Wu, and C.", "K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N.", "Mahmood, N. Ghorbani, N. F.", "Troje, G. Pons-Moll, and M. J.", "Black, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451.", "5\n[30] L. Xu, X. Lv, Y.", "Yan, Y. Jin, G. Wu, Y.", "Xu, L. Qiao, X. Zhu, J.", "Liu,\nR. Zhang et al. , Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.", "16051, 2023. 5\n[31] F. G.", "Harvey, M. Yurick, D. Nowrouzezahrai, and C.", "J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.", "04942, 2021. 5\n[32] M. J.", "Black, P. Patel, J. Tesch, and J.", "Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp.", "8726 8737. 7, 10\n[33] Y. Wang, Y.", "Sun, P. Patel, K. Daniilidis, M.", "J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.", "1148 1159. 11\n\nA PPENDIX\n\nB. Reference Dynamics Integration\n\nA.", "External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator. 1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links.", "At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps). Gains may gently increase, hold, and then decrease back to\nzero at the end of the interval.", "In parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping. 2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame. We use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point.", "In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change. 3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i. e.", ", leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.", "4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso. TABLE II: External Force Application Parameters.", "Parameter\n\nSymbol\n\nTypical value / range\n\nMax per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain\n\nFmax\nτsafe (t)\nτF\nτM\nKspring (t)\n\n30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250\n\nAll reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t .", "(8)\n\nThe driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step. TABLE III: Reference Dynamics and Integration Parameters.", "Parameter\n\nSymbol\n\nValue\n\nVirtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip\n\nM\nD\nKp\nKd\n t\nNsub\n x max\n x max\n\n0. 1 kg\n2. 0\nDerived\nfrom Kp = τsafe /0.", "05\np\n2 M Kp\nSame as simulation dt = 0. 02s\n4\n4 m/s\n1000 m/s2\n\nC. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.", "While our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.", "Next, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.", "The objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r\n\n( ,k) S\n\n2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 . Here S collects the link target pairs (e. g.", ", hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.", "After obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.", "D. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.", "Finally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e. g.", ", with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "paragraph", "num_chunks": 708, "avg_chunk_len": 69.53107344632768, "std_chunk_len": 357.37158476774937, "max_chunk_len": 4431, "min_chunk_len": 1, "total_chars": 49228, "compression_ratio": 1.0288250589095638, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top", "arXiv:2511.04679v1 [cs.RO] 6 Nov 2025", "(a) Sit-to-stand Support", "(b) Handshaking", "(d) Balloon Handling", "GentleHumanoid\n(c) Shape-aware Hugging", "Vanilla Tracking RL", "Tracking RL w/ Large Perturbation", "Fig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits.\nIt enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.\nAbstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.\nWe introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-", "ship at Stanford University. Qingzhou is now with Tsinghua University.", "shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.", "I. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive", "whole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.\nIn contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.g., hugging people or handling\nfragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds.\nThis raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces.\nWe address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.\nSince collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.g., shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.\nWe evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted", "pressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.\nIn summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.\n We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction.\n We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.\nII. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.\nB. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these", "methods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.\nC. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23]. In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes.\nIII. M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces. Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig. 2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,", "(1)", "where xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.", "For clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,", "(2)", "where xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target.\nTo ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp . All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control.\nC. Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object.\nGuiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm.\nBoth cases are modeled using the same spring formulation\nwith a consistent anchor terminology:", "finteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as", "xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and", "(a) Reference Dynamics", "(c) Deployment\nHRI Motion", "Contact", "Hugging\nPlanner", "timestep", "Vision", "Driving\nForce", "Interaction\nForce", "Hugging Motion", "Motion Target\nCurrent Pos\nAnchor Pos", "GentleHumanoid Policy\nProprioception", "Privileged Obs", "(b) Training\nReward", "Safe & Compliant Interactions", "Policy", "Target Motion\nAction", "Simulator", "Fig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects.", "D. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the", "Right Shoulder Link", "Right Elbow Link", "0.200", "Right Hand Link\n0.16", "0.7\n0.175\n0.6", "0.14", "0.150", "0.5", "Density", "correspond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.\nTo further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force. Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.", "0.4", "0.12", "0.125", "0.10", "0.100", "0.08", "0.3", "0.075", "0.06", "0.2", "0.050", "0.04", "0.1", "0.025", "0.0", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.000", "0.02\n0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.00", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "Fig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.", "maximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:", "τsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive", "compliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e.g., balloons), we set F1 = 5 N\nand F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.5 0.5 cm contact area\n(0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.g.,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.\nE. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .\nTo incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0.005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM", "(6)", "ref\nref\nxref\nt+1 = xt + t x t+1 .", "(7)", "Where t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.\nThis impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce.\nWe compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design).\nDuring training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel. In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios.\n1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer.\nWe adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27]. The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,", "where τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history.\nThe teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq. 7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.\nBoth policies output joint position targets at R29 which\nare tracked by low-level PD controllers.\n2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.\n3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance.\nIn GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq. 7:", "x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = exp t\nσx\nσv", "Exponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:", "sim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety", "margin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .", "rcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward", "Vanilla-RL", "Extreme-RL", "Right Elbow Link", "20", "Right Shoulder Link\n15", "15\n10\n5\n0", "Force (N)", "20", "Force (N)", "Force (N)", "Here, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:", "GentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0", "2", "4", "6", "8", "10", "10", "5", "0\n0", "2", "Time (s)", "4", "6", "8", "10", "0", "2", "Time (s)", "4", "6", "8", "10", "Time (s)", "Fig. 4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N", "!\"#$ =15N", "Weight\n !\"#$ =5N", "Compliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty", "2.0\n2.0\n6.0", "GentleHumanoid with different force limits", "Motion Tracking\nRoot Tracking\nJoint Tracking", "0.5\n1.0\nVanilla-RL", "Locomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit", "5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0", "Extreme-RL", "Fig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.", "IV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods.\nA. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N", "with rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.\nB. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\nparticularly rigid, requiring a peak force of 51.14 N, while\nVanilla-RL requires 24.59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A", "C. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising", "Hugging\nin right position", "Hugging with\nmisalignment", "Sensor Pad\nReal-time Pressure\nVisualization", "Extreme-RL", "Hugging with misalignment", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "8", "0", "0", "1", "0", "0", "0", "0", "1", "3", "4", "2", "0", "0", "0", "0", "6", "16", "8", "0", "0", "0", "0", "0", "1", "0", "0", "1", "2", "0", "17", "9", "0", "0", "0", "0", "50", "6", "6", "0", "0", "0", "1", "0", "0", "0", "28", "5", "5", "0", "30", "7", "1", "1", "1", "3", "45", "9", "8", "1", "0", "0", "0", "0", "0", "1", "111", "5", "3", "0", "11", "1", "1", "1", "0", "8", "34", "41", "4", "0", "0", "0", "0", "0", "0", "19", "26", "37", "2", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "15", "10", "5", "0", "0", "6", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "7", "4", "0", "0", "0", "0", "0", "0", "0", "0", "2", "1", "1", "0", "0", "0", "0", "0", "15", "20", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "4", "4", "0", "0", "0", "0", "38", "13", "17", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "11", "0", "16", "1", "0", "0", "0", "1", "3", "11", "0", "0", "0", "0", "0", "1", "7", "12", "2", "4", "1", "2", "5", "1", "0", "0", "0", "1", "8", "1", "0", "0", "0", "0", "0", "0", "9", "8", "16", "2", "0", "0", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "208", "61", "8", "3", "1", "0", "1", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "2", "0", "10", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "7", "13", "0", "1", "4", "7", "23", "1", "0", "1", "4", "11", "12", "0", "4", "2", "2", "0", "0", "0", "2", "27", "6", "0", "0", "0", "19", "11", "0", "0", "62", "0", "2", "11", "25", "4", "1", "0", "0", "4", "2", "52", "7", "0", "0", "0", "0", "0", "0", "5", "5", "0", "2", "3", "64", "4", "0", "0", "0", "22", "6", "83", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "2", "2", "6", "15", "0", "0", "0", "2", "17", "8", "0", "0", "0", "0", "0", "0", "0", "0", "2", "0", "0", "16", "417", "10", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "2", "19", "5", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "551", "1", "0", "0", "0", "0", "0", "14", "16", "GentleHumanoid", "Vanilla-RL", "Hugging in right position", "350", "300", "250", "200", "150", "100", "50", "0", "Extreme-RL", "Hugging with misalignment\n30", "25", "25", "Peak Force (N)", "30", "20\n15\n10\n5\n0", "400", "Pressure (kPa)", "Vanilla-RL", "GentleHumanoid", "Hugging in right position", "Force (N)", "key observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.\nFor example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.\n2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.\nPressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.\n3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it.\nAcross all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.", "20\n15\n10\n5", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "14", "16", "0", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "Fig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.", "for healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions.\nWe also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c). From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner. Experiments", "with participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions.\nV. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots. By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction.\nSeveral limitations remain. First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage. Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation. Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.\nVI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset. We further thank the\nUnitree team for their timely and reliable hardware support.\nR EFERENCES\n[1] D. J. Agravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2\n[2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024. 2, 5\n[3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412.13196, 2024. 2\n[4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506.14770, 2025. 2", "[5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He,\nN. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.01143, 2025. 2\n[6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\nC. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406.08858, 2024. 2\n[8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and\nC. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505.02833, 2025. 2, 5, 7\n[9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372.\n2\n[10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505.06883, 2025. 2, 3, 5\n[11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.20829, 2025. 2\n[12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.06776, 2025. 2\n[13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021.\n2\n[14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse,\nP. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208. 2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024. 2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209.13145, 2022. 2\n[17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2\n[18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502.10894, 2025. 2\n[19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and\nS. Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A. E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3\n[22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024. 3\n[23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502.03132, 2025. 3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5\n[25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5\n[26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5", "[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol. abs/1707.06347, 2017. 5\n[28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.\nBlack, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451. 5\n[30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Qiao, X. Zhu, J. Liu,\nR. Zhang et al., Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.16051, 2023. 5\n[31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5\n[32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10\n[33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.\n1148 1159. 11", "A PPENDIX", "B. Reference Dynamics Integration", "A. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator.\n1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).\nGains may gently increase, hold, and then decrease back to\nzero at the end of the interval.\nIn parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping.\n2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame.\nWe use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change.\n3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.e., leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.\n4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.\nTABLE II: External Force Application Parameters.\nParameter", "Symbol", "Typical value / range", "Max per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain", "Fmax\nτsafe (t)\nτF\nτM\nKspring (t)", "30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250", "All reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)", "The driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter", "Symbol", "Value", "Virtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip", "M\nD\nKp\nKd\n t\nNsub\n x max\n x max", "0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2", "C. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.\nWhile our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.\nNext, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.\nThe objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r", "( ,k) S", "2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.", "After obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.\nD. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.\nFinally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e.g., with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "recursive", "num_chunks": 873, "avg_chunk_len": 56.243986254295535, "std_chunk_len": 131.46013655829853, "max_chunk_len": 758, "min_chunk_len": 1, "total_chars": 49101, "compression_ratio": 1.031486120445612, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top", "arXiv:2511.04679v1 [cs.RO] 6 Nov 2025", "(a) Sit-to-stand Support", "(b) Handshaking", "(d) Balloon Handling", "GentleHumanoid\n(c) Shape-aware Hugging", "Vanilla Tracking RL", "Tracking RL w/ Large Perturbation", "Fig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits. It enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.", "Abstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.", "We introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution.", "This work was done during Qingzhou Lu s intern-", "ship at Stanford University. Qingzhou is now with Tsinghua University.", "shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.", "I. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive", "whole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12].", "However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction. In contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.", "g. , hugging people or handling\nfragile objects) to firm, supportive assistance (e. g.", ", sit-tostand), while always remaining within safe force thresholds. This raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces. We address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance.", "The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects. Since collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions.", "They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.", "g. , shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance.", "To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements. We evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted", "pressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.", "In summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios. We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction.", "We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation. II.", "R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability.", "Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6].", "Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans. B.", "Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these", "methods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.", "C. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20].", "More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23].", "In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes. III.", "M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces.", "Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig.", "2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,", "(1)", "where xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.", "For clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,", "(2)", "where xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target. To ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp .", "All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control. C.", "Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities.", "We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object. Guiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm. Both cases are modeled using the same spring formulation\nwith a consistent anchor terminology:", "finteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as", "xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and", "(a) Reference Dynamics", "(c) Deployment\nHRI Motion", "Contact", "Hugging\nPlanner", "timestep", "Vision", "Driving\nForce", "Interaction\nForce", "Hugging Motion", "Motion Target\nCurrent Pos\nAnchor Pos", "GentleHumanoid Policy\nProprioception", "Privileged Obs", "(b) Training\nReward", "Safe & Compliant Interactions", "Policy", "Target Motion\nAction", "Simulator", "Fig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects.", "D. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the", "Right Shoulder Link", "Right Elbow Link", "0.200", "Right Hand Link\n0.16", "0.7\n0.175\n0.6", "0.14", "0.150", "0.5", "Density", "correspond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.", "To further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force.", "Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.", "0.4", "0.12", "0.125", "0.10", "0.100", "0.08", "0.3", "0.075", "0.06", "0.2", "0.050", "0.04", "0.1", "0.025", "0.0", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.000", "0.02\n0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.00", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "Fig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.", "maximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:", "τsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive", "compliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication.", "Since our focus is humanoid interaction with\nhumans and fragile objects (e. g. , balloons), we set F1 = 5 N\nand F2 = 15 N.", "These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0. 5 0.", "5 cm contact area\n(0. 25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e. g.", ",\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.", "E. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .", "To incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0. 005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM", "(6)", "ref\nref\nxref\nt+1 = xt + t x t+1 .", "(7)", "Where t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.", "This impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce. We compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design). During training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel.", "In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios. 1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer. We adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27].", "The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,", "where τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history. The teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq.", "7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.", "Both policies output joint position targets at R29 which\nare tracked by low-level PD controllers. 2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.", "3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance. In GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq.", "7:", "x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = exp t\nσx\nσv", "Exponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:", "sim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety", "margin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .", "rcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward", "Vanilla-RL", "Extreme-RL", "Right Elbow Link", "20", "Right Shoulder Link\n15", "15\n10\n5\n0", "Force (N)", "20", "Force (N)", "Force (N)", "Here, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:", "GentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0", "2", "4", "6", "8", "10", "10", "5", "0\n0", "2", "Time (s)", "4", "6", "8", "10", "0", "2", "Time (s)", "4", "6", "8", "10", "Time (s)", "Fig. 4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N", "!\"#$ =15N", "Weight\n !\"#$ =5N", "Compliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty", "2.0\n2.0\n6.0", "GentleHumanoid with different force limits", "Motion Tracking\nRoot Tracking\nJoint Tracking", "0.5\n1.0\nVanilla-RL", "Locomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit", "5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0", "Extreme-RL", "Fig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.", "IV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods.", "A. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace.", "As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N", "with rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces. B.", "Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static.", "Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance.", "Extreme-RL is\nparticularly rigid, requiring a peak force of 51. 14 N, while\nVanilla-RL requires 24. 59 N.", "In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A", "C. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising", "Hugging\nin right position", "Hugging with\nmisalignment", "Sensor Pad\nReal-time Pressure\nVisualization", "Extreme-RL", "Hugging with misalignment", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "8", "0", "0", "1", "0", "0", "0", "0", "1", "3", "4", "2", "0", "0", "0", "0", "6", "16", "8", "0", "0", "0", "0", "0", "1", "0", "0", "1", "2", "0", "17", "9", "0", "0", "0", "0", "50", "6", "6", "0", "0", "0", "1", "0", "0", "0", "28", "5", "5", "0", "30", "7", "1", "1", "1", "3", "45", "9", "8", "1", "0", "0", "0", "0", "0", "1", "111", "5", "3", "0", "11", "1", "1", "1", "0", "8", "34", "41", "4", "0", "0", "0", "0", "0", "0", "19", "26", "37", "2", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "15", "10", "5", "0", "0", "6", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "7", "4", "0", "0", "0", "0", "0", "0", "0", "0", "2", "1", "1", "0", "0", "0", "0", "0", "15", "20", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "4", "4", "0", "0", "0", "0", "38", "13", "17", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "11", "0", "16", "1", "0", "0", "0", "1", "3", "11", "0", "0", "0", "0", "0", "1", "7", "12", "2", "4", "1", "2", "5", "1", "0", "0", "0", "1", "8", "1", "0", "0", "0", "0", "0", "0", "9", "8", "16", "2", "0", "0", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "208", "61", "8", "3", "1", "0", "1", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "2", "0", "10", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "7", "13", "0", "1", "4", "7", "23", "1", "0", "1", "4", "11", "12", "0", "4", "2", "2", "0", "0", "0", "2", "27", "6", "0", "0", "0", "19", "11", "0", "0", "62", "0", "2", "11", "25", "4", "1", "0", "0", "4", "2", "52", "7", "0", "0", "0", "0", "0", "0", "5", "5", "0", "2", "3", "64", "4", "0", "0", "0", "22", "6", "83", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "2", "2", "6", "15", "0", "0", "0", "2", "17", "8", "0", "0", "0", "0", "0", "0", "0", "0", "2", "0", "0", "16", "417", "10", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "2", "19", "5", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "551", "1", "0", "0", "0", "0", "0", "14", "16", "GentleHumanoid", "Vanilla-RL", "Hugging in right position", "350", "300", "250", "200", "150", "100", "50", "0", "Extreme-RL", "Hugging with misalignment\n30", "25", "25", "Peak Force (N)", "30", "20\n15\n10\n5\n0", "400", "Pressure (kPa)", "Vanilla-RL", "GentleHumanoid", "Hugging in right position", "Force (N)", "key observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit. For example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N.", "This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines. 2) Hugging a mannequin.", "We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.", "Pressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge.", "Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion. 3) Handling deformable objects.", "Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N.", "As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it. Across all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.", "20\n15\n10\n5", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "14", "16", "0", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "Fig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.", "for healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions. We also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c).", "From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner.", "Experiments", "with participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions. V. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots.", "By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction. Several limitations remain.", "First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage.", "Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation.", "Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.", "VI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset.", "We further thank the\nUnitree team for their timely and reliable hardware support. R EFERENCES\n[1] D. J.", "Agravante, A. Cherubini, A. Sherikov, P.", "-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol.", "35, no. 4, pp. 833 846, 2019.", "2\n[2] Z. Fu, Q. Zhao, Q.", "Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024.", "2, 5\n[3] M. Ji, X. Peng, F.", "Liu, J. Li, G. Yang, X.", "Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412. 13196, 2024.", "2\n[4] Z. Chen, M. Ji, X.", "Cheng, X. Peng, X. B.", "Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506. 14770, 2025.", "2", "[5] T. He, J. Gao, W.", "Xiao, Y. Zhang, Z. Wang, J.", "Wang, Z. Luo, G. He,\nN.", "Sobanbab, C. Pan et al. , Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.", "01143, 2025. 2\n[6] Q. Liao, T.", "E. Truong, X. Huang, G.", "Tevet, K. Sreenath, and C. K.", "Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z.", "Luo, X. He, W. Xiao, C.", "Zhang, W. Zhang, K. Kitani,\nC.", "Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406. 08858, 2024.", "2\n[8] Y. Ze, Z. Chen, J.", "P. Arau jo, Z. ang Cao, X.", "B. Peng, J. Wu, and\nC.", "K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505. 02833, 2025.", "2, 5, 7\n[9] T. Portela, G. B.", "Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA).", "IEEE, 2024, pp. 15 366 15 372. 2\n[10] B.", "Xu, H. Weng, Q. Lu, Y.", "Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505. 06883, 2025.", "2, 3, 5\n[11] P. Zhi, P. Li, J.", "Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.", "20829, 2025. 2\n[12] Y. Zhang, Y.", "Yuan, P. Gurunath, T. He, S.", "Omidshafiei, A. -a. Aghamohammadi, M.", "Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.", "06776, 2025. 2\n[13] M. Murooka, K.", "Chappellet, A. Tanguy, M. Benallegue, I.", "Kumagai, M. Morisawa, F. Kanehiro, and A.", "Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp.", "5597 5604, 2021. 2\n[14] E. Dantec, R.", "Budhiraja, A. Roig, T. Lembono, G.", "Saurel, O. Stasse,\nP. Fernbach, S.", "Tonneau, S. Vijayakumar, S. Calinon et al.", ", Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208.", "2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024.", "2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209. 13145, 2022. 2\n[17] A.", "Rigo, M. Hu, S. K.", "Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp.", "15 322 15 328. 2\n[18] N. Fey, G.", "B. Margolis, M. Peticco, and P.", "Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502. 10894, 2025. 2\n[19] T.", "Mukai, S. Hirano, H. Nakashima, Y.", "Kato, Y. Sakaida, S. Guo, and\nS.", "Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A.", "E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph. D.", "dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S.", "Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no.", "3, pp. 5705 5712, 2021. 3\n[22] H.", "Lefe vre, T. Chaki, T. Kawakami, A.", "Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024.", "3\n[23] Y. Sun, R. Chen, K.", "S. Yun, Y. Fang, S.", "Jung, F. Li, B. Li, W.", "Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502. 03132, 2025.", "3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016.", "5\n[25] J. Kim, A. Alspach, I.", "Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp.", "557 561. 5\n[26] Y. Nam, S.", "Yang, J. Kim, B. Koo, S.", "Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no.", "4, p. 2206, 2023. 5", "[27] J. Schulman, F. Wolski, P.", "Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol.", "abs/1707. 06347, 2017. 5\n[28] Y.", "Ze, J. P. Arau jo, J.", "Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository.", "5\n[29] N. Mahmood, N. Ghorbani, N.", "F. Troje, G. Pons-Moll, and M.", "J. Black, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp.", "5442 \n5451. 5\n[30] L. Xu, X.", "Lv, Y. Yan, Y. Jin, G.", "Wu, Y. Xu, L. Qiao, X.", "Zhu, J. Liu,\nR. Zhang et al.", ", Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312. 16051, 2023. 5\n[31] F.", "G. Harvey, M. Yurick, D.", "Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol.", "abs/2102. 04942, 2021. 5\n[32] M.", "J. Black, P. Patel, J.", "Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun.", "2023, pp. 8726 8737. 7, 10\n[33] Y.", "Wang, Y. Sun, P. Patel, K.", "Daniilidis, M. J. Black, and M.", "Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp. 1148 1159. 11", "A PPENDIX", "B. Reference Dynamics Integration", "A. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator.", "1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).", "Gains may gently increase, hold, and then decrease back to\nzero at the end of the interval. In parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping. 2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame.", "We use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change. 3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.", "e. , leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction.", "This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts. 4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.", "TABLE II: External Force Application Parameters. Parameter", "Symbol", "Typical value / range", "Max per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain", "Fmax\nτsafe (t)\nτF\nτM\nKspring (t)", "30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250", "All reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)", "The driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter", "Symbol", "Value", "Virtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip", "M\nD\nKp\nKd\n t\nNsub\n x max\n x max", "0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2", "C. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential. While our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape.", "To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point. Next, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane.", "We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection. The objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r", "( ,k) S", "2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.", "After obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.", "D. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.", "Finally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e. g.", ", with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "semantic", "num_chunks": 219, "avg_chunk_len": 230.62100456621005, "std_chunk_len": 242.62336732391128, "max_chunk_len": 2457, "min_chunk_len": 15, "total_chars": 50506, "compression_ratio": 1.0027917475151467, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid. axell.", "top\n\narXiv:2511. 04679v1 [cs. RO] 6 Nov 2025\n\n(a) Sit-to-stand Support\n\n(b) Handshaking\n\n(d) Balloon Handling\n\nGentleHumanoid\n(c) Shape-aware Hugging\n\nVanilla Tracking RL\n\nTracking RL w/ Large Perturbation\n\nFig.", "1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits. It enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail. Abstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential.", "However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance.", "At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-\n\nship at Stanford University.", "Qingzhou is now with Tsinghua University. shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds.", "We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.", "I. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive\n\nwhole-body locomotion and manipulation [1] [8].", "However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.", "In contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e. g.", ", hugging people or handling\nfragile objects) to firm, supportive assistance (e. g. , sit-tostand), while always remaining within safe force thresholds.", "This raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces. We address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.", "Since collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios.", "To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e. g.", ", shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.", "We evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted\n\npressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation.", "We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs. In summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.", "We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction. We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.", "II. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics.", "The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods.", "In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.", "B. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18].", "However, these\n\nmethods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.", "C. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20].", "More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23].", "In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes. III.", "M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces.", "Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig.", "2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,\n\n(1)\n\nwhere xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0. 1 kg\nin our reference dynamics model.", "The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component. For clarity, we introduce the index i once and omit it\nhenceforth.", "All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame. B. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory.", "The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,\n\n(2)\n\nwhere xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target. To ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp .", "All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control. C.", "Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities.", "We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object. Guiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm. Both cases are modeled using the same spring formulation\nwith a consistent anchor terminology:\n\nfinteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as\n\n xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.", "sample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration. This formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures. Posture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and\n\n(a) Reference Dynamics\n\n(c) Deployment\nHRI Motion\n\nContact\n\nHugging\nPlanner\n\ntimestep\n\nVision\n\nDriving\nForce\n\nInteraction\nForce\n\nHugging Motion\n\nMotion Target\nCurrent Pos\nAnchor Pos\n\nGentleHumanoid Policy\nProprioception\n\nPrivileged Obs\n\n(b) Training\nReward\n\nSafe & Compliant Interactions\n\nPolicy\n\nTarget Motion\nAction\n\nSimulator\n\nFig.", "2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ).", "(c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects. D. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error.", "Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the\n\nRight Shoulder Link\n\nRight Elbow Link\n\n0. 200\n\nRight Hand Link\n0.", "16\n\n0. 7\n0. 175\n0.", "6\n\n0. 14\n\n0. 150\n\n0.", "5\n\nDensity\n\ncorrespond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.", "To further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force.", "Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.", "0. 4\n\n0. 12\n\n0.", "125\n\n0. 10\n\n0. 100\n\n0.", "08\n\n0. 3\n\n0. 075\n\n0.", "06\n\n0. 2\n\n0. 050\n\n0.", "04\n\n0. 1\n\n0. 025\n\n0.", "0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0. 000\n\n0. 02\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.", "00\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\nFig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right).", "Insets (top right) illustrate the corresponding\nforce directions on a sphere. maximum allowable force applied by the robot. We define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 .", "The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits. The current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks.", "When the driving force exceeds the threshold, we\napply a scaling mechanism:\n\nτsafe\nfdrive limited = min 1. 0,\n fdrive ,\n(5)\n fdrive \n\ncompliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds.", "The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e. g.", ", balloons), we set F1 = 5 N\nand F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.", "5 0. 5 cm contact area\n(0. 25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.", "g. ,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26].", "Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band. E. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar .", "The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract . To incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0. 005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM\n\n(6)\n\nref\nref\nxref\nt+1 = xt + t x t+1 .", "(7)\n\nWhere t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.", "This impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce. We compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design). During training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel.", "In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios. 1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer. We adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27].", "The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,\n\nwhere τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history. The teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq.", "7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.", "Both policies output joint position targets at R29 which\nare tracked by low-level PD controllers. 2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.", "3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance. In GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq.", "7:\n\n x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n. rdyn = exp t\nσx\nσv\n\nExponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity. Reference Force Tracking.", "To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:\n\nsim\n finteract finteract\n 2\nrforce = exp \n. σf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds. Unsafe Force Penalty.", "To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety\n\nmargin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) . rcompliance = wdyn rdyn + wforce rforce + wpen rpen .", "The weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I. TABLE I: Reward Terms and Weights. Reward\n\nVanilla-RL\n\nExtreme-RL\n\nRight Elbow Link\n\n20\n\nRight Shoulder Link\n15\n\n15\n10\n5\n0\n\nForce (N)\n\n20\n\nForce (N)\n\nForce (N)\n\nHere, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties.", "This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations. The overall compliance reward is a weighted sum of these\nterms:\n\nGentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n10\n\n5\n\n0\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\nTime (s)\n\nFig.", "4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.", "! \"#$ =10N\n\n ! \"#$ =15N\n\nWeight\n !", "\"#$ =5N\n\nCompliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty\n\n2. 0\n2. 0\n6.", "0\n\nGentleHumanoid with different force limits\n\nMotion Tracking\nRoot Tracking\nJoint Tracking\n\n0. 5\n1. 0\nVanilla-RL\n\nLocomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit\n\n5.", "0\n10. 0\n4. 0\n2.", "0\n0. 1\n5. 0e-4\n1.", "0\n\nExtreme-RL\n\nFig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures.", "Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses. Force gauge readings (N) are highlighted for clarity. IV.", "E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods. A.", "Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks.", "At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N\n\nwith rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.", "B. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force.", "We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values.", "As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\nparticularly rigid, requiring a peak force of 51. 14 N, while\nVanilla-RL requires 24.", "59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A\n\nC.", "More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling. Demonstrations of joystick-based control are provided in the\nsupplementary video.", "While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising\n\nHugging\nin right position\n\nHugging with\nmisalignment\n\nSensor Pad\nReal-time Pressure\nVisualization\n\nExtreme-RL\n\nHugging with misalignment\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n8\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n3\n\n4\n\n2\n\n0\n\n0\n\n0\n\n0\n\n6\n\n16\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n2\n\n0\n\n17\n\n9\n\n0\n\n0\n\n0\n\n0\n\n50\n\n6\n\n6\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n28\n\n5\n\n5\n\n0\n\n30\n\n7\n\n1\n\n1\n\n1\n\n3\n\n45\n\n9\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n111\n\n5\n\n3\n\n0\n\n11\n\n1\n\n1\n\n1\n\n0\n\n8\n\n34\n\n41\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n19\n\n26\n\n37\n\n2\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n10\n\n5\n\n0\n\n0\n\n6\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n20\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n4\n\n0\n\n0\n\n0\n\n0\n\n38\n\n13\n\n17\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n11\n\n0\n\n16\n\n1\n\n0\n\n0\n\n0\n\n1\n\n3\n\n11\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n12\n\n2\n\n4\n\n1\n\n2\n\n5\n\n1\n\n0\n\n0\n\n0\n\n1\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n9\n\n8\n\n16\n\n2\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n208\n\n61\n\n8\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n2\n\n0\n\n10\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n7\n\n13\n\n0\n\n1\n\n4\n\n7\n\n23\n\n1\n\n0\n\n1\n\n4\n\n11\n\n12\n\n0\n\n4\n\n2\n\n2\n\n0\n\n0\n\n0\n\n2\n\n27\n\n6\n\n0\n\n0\n\n0\n\n19\n\n11\n\n0\n\n0\n\n62\n\n0\n\n2\n\n11\n\n25\n\n4\n\n1\n\n0\n\n0\n\n4\n\n2\n\n52\n\n7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n5\n\n5\n\n0\n\n2\n\n3\n\n64\n\n4\n\n0\n\n0\n\n0\n\n22\n\n6\n\n83\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n2\n\n6\n\n15\n\n0\n\n0\n\n0\n\n2\n\n17\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n0\n\n16\n\n417\n\n10\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n2\n\n19\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n551\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n14\n\n16\n\nGentleHumanoid\n\nVanilla-RL\n\nHugging in right position\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\nExtreme-RL\n\nHugging with misalignment\n30\n\n25\n\n25\n\nPeak Force (N)\n\n30\n\n20\n15\n10\n5\n0\n\n400\n\nPressure (kPa)\n\nVanilla-RL\n\nGentleHumanoid\n\nHugging in right position\n\nForce (N)\n\nkey observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.", "For example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.", "2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion.", "In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact. Pressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines.", "For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.", "3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse.", "For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it. Across all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.", "20\n15\n10\n5\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\n14\n\n16\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\nFig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization.", "Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.", "for healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions. We also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c).", "From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner.", "Experiments\n\nwith participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions. V. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots.", "By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction. Several limitations remain.", "First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage.", "Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation.", "Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.", "VI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset.", "We further thank the\nUnitree team for their timely and reliable hardware support. R EFERENCES\n[1] D. J.", "Agravante, A. Cherubini, A. Sherikov, P.", "-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol.", "35, no. 4, pp. 833 846, 2019.", "2\n[2] Z. Fu, Q. Zhao, Q.", "Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024.", "2, 5\n[3] M. Ji, X. Peng, F.", "Liu, J. Li, G. Yang, X.", "Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412. 13196, 2024.", "2\n[4] Z. Chen, M. Ji, X.", "Cheng, X. Peng, X. B.", "Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506. 14770, 2025.", "2\n\n[5] T. He, J. Gao, W.", "Xiao, Y. Zhang, Z. Wang, J.", "Wang, Z. Luo, G. He,\nN.", "Sobanbab, C. Pan et al. , Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.", "01143, 2025. 2\n[6] Q. Liao, T.", "E. Truong, X. Huang, G.", "Tevet, K. Sreenath, and C. K.", "Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z.", "Luo, X. He, W. Xiao, C.", "Zhang, W. Zhang, K. Kitani,\nC.", "Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406. 08858, 2024.", "2\n[8] Y. Ze, Z. Chen, J.", "P. Arau jo, Z. ang Cao, X.", "B. Peng, J. Wu, and\nC.", "K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505. 02833, 2025.", "2, 5, 7\n[9] T. Portela, G. B.", "Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA).", "IEEE, 2024, pp. 15 366 15 372. 2\n[10] B.", "Xu, H. Weng, Q. Lu, Y.", "Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505. 06883, 2025.", "2, 3, 5\n[11] P. Zhi, P. Li, J.", "Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.", "20829, 2025. 2\n[12] Y. Zhang, Y.", "Yuan, P. Gurunath, T. He, S.", "Omidshafiei, A. -a. Aghamohammadi, M.", "Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.", "06776, 2025. 2\n[13] M. Murooka, K.", "Chappellet, A. Tanguy, M. Benallegue, I.", "Kumagai, M. Morisawa, F. Kanehiro, and A.", "Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp.", "5597 5604, 2021. 2\n[14] E. Dantec, R.", "Budhiraja, A. Roig, T. Lembono, G.", "Saurel, O. Stasse,\nP. Fernbach, S.", "Tonneau, S. Vijayakumar, S. Calinon et al.", ", Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208.", "2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024.", "2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209. 13145, 2022. 2\n[17] A.", "Rigo, M. Hu, S. K.", "Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp.", "15 322 15 328. 2\n[18] N. Fey, G.", "B. Margolis, M. Peticco, and P.", "Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502. 10894, 2025. 2\n[19] T.", "Mukai, S. Hirano, H. Nakashima, Y.", "Kato, Y. Sakaida, S. Guo, and\nS.", "Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A.", "E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph. D.", "dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S.", "Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no.", "3, pp. 5705 5712, 2021. 3\n[22] H.", "Lefe vre, T. Chaki, T. Kawakami, A.", "Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024.", "3\n[23] Y. Sun, R. Chen, K.", "S. Yun, Y. Fang, S.", "Jung, F. Li, B. Li, W.", "Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502. 03132, 2025.", "3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016.", "5\n[25] J. Kim, A. Alspach, I.", "Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp.", "557 561. 5\n[26] Y. Nam, S.", "Yang, J. Kim, B. Koo, S.", "Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no.", "4, p. 2206, 2023. 5\n\n[27] J.", "Schulman, F. Wolski, P. Dhariwal, A.", "Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol. abs/1707.", "06347, 2017. 5\n[28] Y. Ze, J.", "P. Arau jo, J. Wu, and C.", "K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N.", "Mahmood, N. Ghorbani, N. F.", "Troje, G. Pons-Moll, and M. J.", "Black, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451.", "5\n[30] L. Xu, X. Lv, Y.", "Yan, Y. Jin, G. Wu, Y.", "Xu, L. Qiao, X. Zhu, J.", "Liu,\nR. Zhang et al. , Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.", "16051, 2023. 5\n[31] F. G.", "Harvey, M. Yurick, D. Nowrouzezahrai, and C.", "J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.", "04942, 2021. 5\n[32] M. J.", "Black, P. Patel, J. Tesch, and J.", "Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp.", "8726 8737. 7, 10\n[33] Y. Wang, Y.", "Sun, P. Patel, K. Daniilidis, M.", "J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.", "1148 1159. 11\n\nA PPENDIX\n\nB. Reference Dynamics Integration\n\nA.", "External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator. 1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links.", "At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps). Gains may gently increase, hold, and then decrease back to\nzero at the end of the interval.", "In parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping. 2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame. We use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point.", "In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change. 3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i. e.", ", leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.", "4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso. TABLE II: External Force Application Parameters.", "Parameter\n\nSymbol\n\nTypical value / range\n\nMax per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain\n\nFmax\nτsafe (t)\nτF\nτM\nKspring (t)\n\n30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250\n\nAll reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t .", "(8)\n\nThe driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step. TABLE III: Reference Dynamics and Integration Parameters.", "Parameter\n\nSymbol\n\nValue\n\nVirtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip\n\nM\nD\nKp\nKd\n t\nNsub\n x max\n x max\n\n0. 1 kg\n2. 0\nDerived\nfrom Kp = τsafe /0.", "05\np\n2 M Kp\nSame as simulation dt = 0. 02s\n4\n4 m/s\n1000 m/s2\n\nC. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.", "While our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.", "Next, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.", "The objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r\n\n( ,k) S\n\n2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 . Here S collects the link target pairs (e. g.", ", hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.", "After obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.", "D. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.", "Finally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e. g.", ", with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "delimiter", "num_chunks": 708, "avg_chunk_len": 69.53107344632768, "std_chunk_len": 357.37158476774937, "max_chunk_len": 4431, "min_chunk_len": 1, "total_chars": 49228, "compression_ratio": 1.0288250589095638, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top", "arXiv:2511.04679v1 [cs.RO] 6 Nov 2025", "(a) Sit-to-stand Support", "(b) Handshaking", "(d) Balloon Handling", "GentleHumanoid\n(c) Shape-aware Hugging", "Vanilla Tracking RL", "Tracking RL w/ Large Perturbation", "Fig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits.\nIt enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.\nAbstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.\nWe introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-", "ship at Stanford University. Qingzhou is now with Tsinghua University.", "shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.", "I. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive", "whole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.\nIn contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.g., hugging people or handling\nfragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds.\nThis raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces.\nWe address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.\nSince collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.g., shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.\nWe evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted", "pressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.\nIn summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.\n We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction.\n We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.\nII. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.\nB. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these", "methods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.\nC. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23]. In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes.\nIII. M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces. Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig. 2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,", "(1)", "where xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.", "For clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,", "(2)", "where xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target.\nTo ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp . All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control.\nC. Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object.\nGuiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm.\nBoth cases are modeled using the same spring formulation\nwith a consistent anchor terminology:", "finteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as", "xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and", "(a) Reference Dynamics", "(c) Deployment\nHRI Motion", "Contact", "Hugging\nPlanner", "timestep", "Vision", "Driving\nForce", "Interaction\nForce", "Hugging Motion", "Motion Target\nCurrent Pos\nAnchor Pos", "GentleHumanoid Policy\nProprioception", "Privileged Obs", "(b) Training\nReward", "Safe & Compliant Interactions", "Policy", "Target Motion\nAction", "Simulator", "Fig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects.", "D. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the", "Right Shoulder Link", "Right Elbow Link", "0.200", "Right Hand Link\n0.16", "0.7\n0.175\n0.6", "0.14", "0.150", "0.5", "Density", "correspond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.\nTo further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force. Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.", "0.4", "0.12", "0.125", "0.10", "0.100", "0.08", "0.3", "0.075", "0.06", "0.2", "0.050", "0.04", "0.1", "0.025", "0.0", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.000", "0.02\n0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.00", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "Fig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.", "maximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:", "τsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive", "compliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e.g., balloons), we set F1 = 5 N\nand F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.5 0.5 cm contact area\n(0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.g.,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.\nE. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .\nTo incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0.005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM", "(6)", "ref\nref\nxref\nt+1 = xt + t x t+1 .", "(7)", "Where t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.\nThis impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce.\nWe compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design).\nDuring training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel. In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios.\n1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer.\nWe adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27]. The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,", "where τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history.\nThe teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq. 7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.\nBoth policies output joint position targets at R29 which\nare tracked by low-level PD controllers.\n2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.\n3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance.\nIn GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq. 7:", "x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = exp t\nσx\nσv", "Exponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:", "sim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety", "margin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .", "rcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward", "Vanilla-RL", "Extreme-RL", "Right Elbow Link", "20", "Right Shoulder Link\n15", "15\n10\n5\n0", "Force (N)", "20", "Force (N)", "Force (N)", "Here, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:", "GentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0", "2", "4", "6", "8", "10", "10", "5", "0\n0", "2", "Time (s)", "4", "6", "8", "10", "0", "2", "Time (s)", "4", "6", "8", "10", "Time (s)", "Fig. 4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N", "!\"#$ =15N", "Weight\n !\"#$ =5N", "Compliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty", "2.0\n2.0\n6.0", "GentleHumanoid with different force limits", "Motion Tracking\nRoot Tracking\nJoint Tracking", "0.5\n1.0\nVanilla-RL", "Locomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit", "5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0", "Extreme-RL", "Fig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.", "IV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods.\nA. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N", "with rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.\nB. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\nparticularly rigid, requiring a peak force of 51.14 N, while\nVanilla-RL requires 24.59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A", "C. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising", "Hugging\nin right position", "Hugging with\nmisalignment", "Sensor Pad\nReal-time Pressure\nVisualization", "Extreme-RL", "Hugging with misalignment", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "8", "0", "0", "1", "0", "0", "0", "0", "1", "3", "4", "2", "0", "0", "0", "0", "6", "16", "8", "0", "0", "0", "0", "0", "1", "0", "0", "1", "2", "0", "17", "9", "0", "0", "0", "0", "50", "6", "6", "0", "0", "0", "1", "0", "0", "0", "28", "5", "5", "0", "30", "7", "1", "1", "1", "3", "45", "9", "8", "1", "0", "0", "0", "0", "0", "1", "111", "5", "3", "0", "11", "1", "1", "1", "0", "8", "34", "41", "4", "0", "0", "0", "0", "0", "0", "19", "26", "37", "2", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "15", "10", "5", "0", "0", "6", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "7", "4", "0", "0", "0", "0", "0", "0", "0", "0", "2", "1", "1", "0", "0", "0", "0", "0", "15", "20", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "4", "4", "0", "0", "0", "0", "38", "13", "17", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "11", "0", "16", "1", "0", "0", "0", "1", "3", "11", "0", "0", "0", "0", "0", "1", "7", "12", "2", "4", "1", "2", "5", "1", "0", "0", "0", "1", "8", "1", "0", "0", "0", "0", "0", "0", "9", "8", "16", "2", "0", "0", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "208", "61", "8", "3", "1", "0", "1", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "2", "0", "10", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "7", "13", "0", "1", "4", "7", "23", "1", "0", "1", "4", "11", "12", "0", "4", "2", "2", "0", "0", "0", "2", "27", "6", "0", "0", "0", "19", "11", "0", "0", "62", "0", "2", "11", "25", "4", "1", "0", "0", "4", "2", "52", "7", "0", "0", "0", "0", "0", "0", "5", "5", "0", "2", "3", "64", "4", "0", "0", "0", "22", "6", "83", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "2", "2", "6", "15", "0", "0", "0", "2", "17", "8", "0", "0", "0", "0", "0", "0", "0", "0", "2", "0", "0", "16", "417", "10", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "2", "19", "5", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "551", "1", "0", "0", "0", "0", "0", "14", "16", "GentleHumanoid", "Vanilla-RL", "Hugging in right position", "350", "300", "250", "200", "150", "100", "50", "0", "Extreme-RL", "Hugging with misalignment\n30", "25", "25", "Peak Force (N)", "30", "20\n15\n10\n5\n0", "400", "Pressure (kPa)", "Vanilla-RL", "GentleHumanoid", "Hugging in right position", "Force (N)", "key observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.\nFor example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.\n2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.\nPressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.\n3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it.\nAcross all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.", "20\n15\n10\n5", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "14", "16", "0", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "Fig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.", "for healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions.\nWe also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c). From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner. Experiments", "with participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions.\nV. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots. By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction.\nSeveral limitations remain. First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage. Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation. Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.\nVI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset. We further thank the\nUnitree team for their timely and reliable hardware support.\nR EFERENCES\n[1] D. J. Agravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2\n[2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024. 2, 5\n[3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412.13196, 2024. 2\n[4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506.14770, 2025. 2", "[5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He,\nN. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.01143, 2025. 2\n[6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\nC. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406.08858, 2024. 2\n[8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and\nC. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505.02833, 2025. 2, 5, 7\n[9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372.\n2\n[10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505.06883, 2025. 2, 3, 5\n[11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.20829, 2025. 2\n[12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.06776, 2025. 2\n[13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021.\n2\n[14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse,\nP. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208. 2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024. 2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209.13145, 2022. 2\n[17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2\n[18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502.10894, 2025. 2\n[19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and\nS. Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A. E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3\n[22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024. 3\n[23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502.03132, 2025. 3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5\n[25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5\n[26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5", "[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol. abs/1707.06347, 2017. 5\n[28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.\nBlack, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451. 5\n[30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Qiao, X. Zhu, J. Liu,\nR. Zhang et al., Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.16051, 2023. 5\n[31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5\n[32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10\n[33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.\n1148 1159. 11", "A PPENDIX", "B. Reference Dynamics Integration", "A. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator.\n1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).\nGains may gently increase, hold, and then decrease back to\nzero at the end of the interval.\nIn parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping.\n2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame.\nWe use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change.\n3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.e., leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.\n4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.\nTABLE II: External Force Application Parameters.\nParameter", "Symbol", "Typical value / range", "Max per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain", "Fmax\nτsafe (t)\nτF\nτM\nKspring (t)", "30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250", "All reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)", "The driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter", "Symbol", "Value", "Virtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip", "M\nD\nKp\nKd\n t\nNsub\n x max\n x max", "0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2", "C. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.\nWhile our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.\nNext, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.\nThe objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r", "( ,k) S", "2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.", "After obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.\nD. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.\nFinally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e.g., with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "token_limit", "num_chunks": 64, "avg_chunk_len": 790.984375, "std_chunk_len": 68.66460427949305, "max_chunk_len": 800, "min_chunk_len": 246, "total_chars": 50623, "compression_ratio": 1.0004740928036664, "avg_chunk_tokens": 197.546875, "max_chunk_tokens": 200, "min_chunk_tokens": 61, "tokenizer": "", "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top\n\narXiv:2511.04679v1 [cs.RO] 6 Nov 2025\n\n(a) Sit-to-stand Support\n\n(b) Handshaking\n\n(d) Balloon Handling\n\nGentleHumanoid\n(c) Shape-aware Hugging\n\nVanilla Tracking RL\n\nTracking RL w/ Large Perturbation\n\nFig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits.\nIt enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-", "aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.\nAbstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.\nWe introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is", "a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-\n\nship at Stanford University. Qingzhou is now with Tsinghua University.\n\nshoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our poli", "cy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.\n\nI. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive\n\nwhole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance", "or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.\nIn contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.g., hugging people or handling\nfragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds.\nThis raises two main challenges: (1) coordina", "ting force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces.\nWe address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.\nSince collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncollidin", "g surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.g., shoulder, elbow, w", "rist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.\nWe evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted\n\npressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincludi", "ng gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.\nIn summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.\n We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical", "human robot interaction.\n We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.\nII. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous", "tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.\nB. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand ad", "mittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these\n\nmethods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.", "C. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to", "consistently avoid human collisions [23]. In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes.\nIII. M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces. Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact", "feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig. 2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,\n\n(1)\n\nwhere xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactio", "ns with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.\n\nFor clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,\n\n(2)\n\nwhere xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedanc", "e stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target.\nTo ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp . All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control.\nC. Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often", "correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object.\nGuiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm.\nBoth cases are modeled using the same spring formulation\nwith a consistent anchor terminology:\n\nfinteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as\n\n xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (", "fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and\n\n(a) Reference Dynamics\n\n(c) Deployment\nHRI Motion\n\nContact\n\nHugging\nPlanner\n\ntimestep\n\nVision\n\nDriving\nForce\n\nInteraction\nForce\n\nHugging Motion\n\nMotion Target\nCurrent Pos\nAnchor Pos\n\nGentleHumanoid Policy\nProprioception\n\nPrivileged Obs\n\n(b) Training\nReward\n\nSafe & Compli", "ant Interactions\n\nPolicy\n\nTarget Motion\nAction\n\nSimulator\n\nFig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large de", "formable objects.\n\nD. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the\n\nRight Shoulder Link\n\nRight Elbow Link\n\n0.200\n\nRight Hand Link\n0.16\n\n0.7\n0.175\n0.6\n\n0.14\n\n0.150\n\n0.5\n\nDensity\n\ncorrespond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.\nTo further increase interaction div", "ersity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force. Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions", "on the sphere with magnitudes from 0 to 25 N.\n\n0.4\n\n0.12\n\n0.125\n\n0.10\n\n0.100\n\n0.08\n\n0.3\n\n0.075\n\n0.06\n\n0.2\n\n0.050\n\n0.04\n\n0.1\n\n0.025\n\n0.0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.000\n\n0.02\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.00\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\nFig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.\n\nmaximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nth", "e policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:\n\nτsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive \n\ncompliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e.g., balloons), we set F1 = 5 N\nand", "F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.5 0.5 cm contact area\n(0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.g.,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.\nE. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a", "target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .\nTo incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0.005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM\n\n(6)\n\nref\nref\nxref\nt+1 = xt + t x t+1 .\n\n(7)\n\nWhere t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velo", "cities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.\nThis impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce.\nWe compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design).\nDuring training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel. In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios.\n1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer.\nWe adopt th", "e same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27]. The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,\n\nwhere τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history.\nThe teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum )", ",\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq. 7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.\nBoth policies output joint position targets at R29 which\nare tracked by low-level PD controllers.\n2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [3", "1] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.\n3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance.\nIn GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq. 7:\n\n x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = e", "xp t\nσx\nσv\n\nExponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:\n\nsim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety\n\nmargin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .\n\nrcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe we", "ights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward\n\nVanilla-RL\n\nExtreme-RL\n\nRight Elbow Link\n\n20\n\nRight Shoulder Link\n15\n\n15\n10\n5\n0\n\nForce (N)\n\n20\n\nForce (N)\n\nForce (N)\n\nHere, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:\n\nGentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n10\n\n5\n\n0\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\nTime (s)\n\nFig. 4: Forces applied by different u", "pper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N\n\n !\"#$ =15N\n\nWeight\n !\"#$ =5N\n\nCompliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty\n\n2.0\n2.0\n6.0\n\nGentleHumanoid with different force limits\n\nMotion Tracking\nRoot Tracking\nJoint Tracking\n\n0.5\n1.0\nVanilla-RL\n\nLocomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit\n\n5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0\n\nExtreme-RL\n\nFig. 5: Comparison of interaction forces across pol", "icies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.\n\nIV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbation", "s, representative of prior force-adaptive\nmethods.\nA. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N\n\nwith rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoo", "thly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.\nB. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\npartic", "ularly rigid, requiring a peak force of 51.14 N, while\nVanilla-RL requires 24.59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A\n\nC. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent", "compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising\n\nHugging\nin right position\n\nHugging with\nmisalignment\n\nSensor Pad\nReal-time Pressure\nVisualization\n\nExtreme-RL\n\nHugging with misalignment\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n8\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n3\n\n4\n\n2\n\n0\n\n0\n\n0\n\n0\n\n6\n\n16\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n2\n\n0\n\n17\n\n9\n\n0\n\n0\n\n0\n\n0\n\n50\n\n6\n\n6\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n28\n\n5\n\n5\n\n0\n\n30\n\n7\n\n1\n\n1\n\n1\n\n3\n\n45\n\n9\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n111\n\n5\n\n3\n\n0\n\n11\n\n1\n\n1\n\n1\n\n0\n\n8\n\n34\n\n41\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n19\n\n26\n\n37\n\n2\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n10\n\n5\n\n0\n\n0\n\n6\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1", "7\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n20\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n4\n\n0\n\n0\n\n0\n\n0\n\n38\n\n13\n\n17\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n11\n\n0\n\n16\n\n1\n\n0\n\n0\n\n0\n\n1\n\n3\n\n11\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n12\n\n2\n\n4\n\n1\n\n2\n\n5\n\n1\n\n0\n\n0\n\n0\n\n1\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n9\n\n8\n\n16\n\n2\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n208\n\n61\n\n8\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n2\n\n0\n\n10\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n7\n\n13\n\n0\n\n1\n\n4\n\n7\n\n23\n\n1\n\n0\n\n1\n\n4\n\n11\n\n12\n\n0\n\n4\n\n2\n\n2\n\n0\n\n0\n\n0\n\n2\n\n27\n\n6\n\n0\n\n0\n\n0\n\n19\n\n11\n\n0\n\n0\n\n62\n\n0\n\n2\n\n11\n\n25\n\n4\n\n1\n\n0\n\n0\n\n4\n\n2\n\n52\n\n7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n5\n\n5\n\n0\n\n2\n\n3\n\n64\n\n4\n\n0\n\n0\n\n0\n\n22\n\n6\n\n83\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n2\n\n6\n\n15\n\n0\n\n0\n\n0\n\n2\n\n17\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n0\n\n16\n\n417\n\n10", "1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n2\n\n19\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n551\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n14\n\n16\n\nGentleHumanoid\n\nVanilla-RL\n\nHugging in right position\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\nExtreme-RL\n\nHugging with misalignment\n30\n\n25\n\n25\n\nPeak Force (N)\n\n30\n\n20\n15\n10\n5\n0\n\n400\n\nPressure (kPa)\n\nVanilla-RL\n\nGentleHumanoid\n\nHugging in right position\n\nForce (N)\n\nkey observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.\nFor example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges", "between 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.\n2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.\nPressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nse", "nsor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.\n3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force thre", "shold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it.\nAcross all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.\n\n20\n15\n10\n5\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\n14\n\n16\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\nFig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumano", "id maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.\n\nfor healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions.\nWe also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c). From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [3", "2] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner. Experiments\n\nwith participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions.\nV. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots. By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demons", "trations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction.\nSeveral limitations remain. First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage. Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoel", "astic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation. Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.\nVI. ACKNOWLEDGMENT\nWe would like to", "thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset. We further thank the\nUnitree team for their timely and reliable hardware support.\nR EFERENCES\n[1] D. J. Agravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2\n[2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024. 2, 5\n[3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang", ",\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412.13196, 2024. 2\n[4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506.14770, 2025. 2\n\n[5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He,\nN. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.01143, 2025. 2\n[6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\nC. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid w", "hole-body teleoperation and learning, arXiv preprint\narXiv:2406.08858, 2024. 2\n[8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and\nC. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505.02833, 2025. 2, 5, 7\n[9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372.\n2\n[10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505.06883, 2025. 2, 3, 5\n[11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.20829, 20", "25. 2\n[12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.06776, 2025. 2\n[13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021.\n2\n[14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse,\nP. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (IC", "RA). IEEE, 2021, pp. 8202 8208. 2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024. 2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209.13145, 2022. 2\n[17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2\n[18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502.10894, 2025. 2\n[19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and\nS. Hosoe, Deve", "lopment of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A. E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3\n[22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024. 3\n[23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao,\nand C. Liu, Spark: A modular benchmark for", "humanoid robot safety, \narXiv preprint arXiv:2502.03132, 2025. 3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5\n[25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5\n[26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5\n\n[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms,", "CoRR,\nvol. abs/1707.06347, 2017. 5\n[28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.\nBlack, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451. 5\n[30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Qiao, X. Zhu, J. Liu,\nR. Zhang et al., Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.16051, 2023. 5\n[31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5\n[32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motio", "n, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10\n[33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.\n1148 1159. 11\n\nA PPENDIX\n\nB. Reference Dynamics Integration\n\nA. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) appl", "ying forces/torques in the simulator.\n1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).\nGains may gently increase, hold, and then decrease back to\nzero at the end of the interval.\nIn parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping.\n2) Anchor (Interaction Spring Origin) Upda", "te: Each active link maintains an anchor o(t) in the robot root frame.\nWe use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change.\n3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.e., leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacem", "ent from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.\n4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.\nTABLE II: External Force Application Parameters.\nParameter\n\nSymbol\n\nTypical value / range\n\nMax per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain\n\nFmax\nτsafe (t)\nτF\nτM\nKspring (t)\n\n30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250\n\nAll reference quan", "tities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)\n\nThe driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter\n\nSymbol\n\nValue\n\nVirtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip\n\nM\nD\nKp\nKd\n t\nNsub", "x max\n x max\n\n0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2\n\nC. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.\nWhile our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.\nNext, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays pro", "perly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.\nThe objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r\n\n( ,k) S\n\n2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nsp", "ecific individual.\n\nAfter obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.\nD. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.\nFinally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our metho", "d\nremains robust and compliant even when the estimated\nreference motions are noisy (e.g., with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "format_aware", "num_chunks": 1, "avg_chunk_len": 50646.0, "std_chunk_len": 0.0, "max_chunk_len": 50646, "min_chunk_len": 50646, "total_chars": 50646, "compression_ratio": 1.0000197448959445, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top\n\narXiv:2511.04679v1 [cs.RO] 6 Nov 2025\n\n(a) Sit-to-stand Support\n\n(b) Handshaking\n\n(d) Balloon Handling\n\nGentleHumanoid\n(c) Shape-aware Hugging\n\nVanilla Tracking RL\n\nTracking RL w/ Large Perturbation\n\nFig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits.\nIt enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.\nAbstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.\nWe introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution. This work was done during Qingzhou Lu s intern-\n\nship at Stanford University. Qingzhou is now with Tsinghua University.\n\nshoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.\n\nI. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive\n\nwhole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12]. However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction.\nIn contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.g., hugging people or handling\nfragile objects) to firm, supportive assistance (e.g., sit-tostand), while always remaining within safe force thresholds.\nThis raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces.\nWe address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance. The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects.\nSince collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions. They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.g., shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance. To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements.\nWe evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted\n\npressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.\nIn summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios.\n We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction.\n We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation.\nII. R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability. Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6]. Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans.\nB. Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these\n\nmethods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.\nC. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20]. More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23]. In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes.\nIII. M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces. Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig. 2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,\n\n(1)\n\nwhere xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.\n\nFor clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,\n\n(2)\n\nwhere xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target.\nTo ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp . All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control.\nC. Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities. We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object.\nGuiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm.\nBoth cases are modeled using the same spring formulation\nwith a consistent anchor terminology:\n\nfinteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as\n\n xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and\n\n(a) Reference Dynamics\n\n(c) Deployment\nHRI Motion\n\nContact\n\nHugging\nPlanner\n\ntimestep\n\nVision\n\nDriving\nForce\n\nInteraction\nForce\n\nHugging Motion\n\nMotion Target\nCurrent Pos\nAnchor Pos\n\nGentleHumanoid Policy\nProprioception\n\nPrivileged Obs\n\n(b) Training\nReward\n\nSafe & Compliant Interactions\n\nPolicy\n\nTarget Motion\nAction\n\nSimulator\n\nFig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects.\n\nD. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the\n\nRight Shoulder Link\n\nRight Elbow Link\n\n0.200\n\nRight Hand Link\n0.16\n\n0.7\n0.175\n0.6\n\n0.14\n\n0.150\n\n0.5\n\nDensity\n\ncorrespond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.\nTo further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force. Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.\n\n0.4\n\n0.12\n\n0.125\n\n0.10\n\n0.100\n\n0.08\n\n0.3\n\n0.075\n\n0.06\n\n0.2\n\n0.050\n\n0.04\n\n0.1\n\n0.025\n\n0.0\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.000\n\n0.02\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\n0.00\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nForce magnitude & direction\n\nFig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.\n\nmaximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:\n\nτsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive \n\ncompliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication. Since our focus is humanoid interaction with\nhumans and fragile objects (e.g., balloons), we set F1 = 5 N\nand F2 = 15 N. These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0.5 0.5 cm contact area\n(0.25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e.g.,\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.\nE. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .\nTo incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0.005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM\n\n(6)\n\nref\nref\nxref\nt+1 = xt + t x t+1 .\n\n(7)\n\nWhere t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.\nThis impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce.\nWe compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design).\nDuring training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel. In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios.\n1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer.\nWe adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27]. The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,\n\nwhere τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history.\nThe teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq. 7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.\nBoth policies output joint position targets at R29 which\nare tracked by low-level PD controllers.\n2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.\n3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance.\nIn GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq. 7:\n\n x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = exp t\nσx\nσv\n\nExponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:\n\nsim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety\n\nmargin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .\n\nrcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward\n\nVanilla-RL\n\nExtreme-RL\n\nRight Elbow Link\n\n20\n\nRight Shoulder Link\n15\n\n15\n10\n5\n0\n\nForce (N)\n\n20\n\nForce (N)\n\nForce (N)\n\nHere, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:\n\nGentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n10\n\n5\n\n0\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\n0\n\n2\n\nTime (s)\n\n4\n\n6\n\n8\n\n10\n\nTime (s)\n\nFig. 4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N\n\n !\"#$ =15N\n\nWeight\n !\"#$ =5N\n\nCompliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty\n\n2.0\n2.0\n6.0\n\nGentleHumanoid with different force limits\n\nMotion Tracking\nRoot Tracking\nJoint Tracking\n\n0.5\n1.0\nVanilla-RL\n\nLocomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit\n\n5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0\n\nExtreme-RL\n\nFig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.\n\nIV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods.\nA. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace. As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N\n\nwith rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces.\nB. Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static. Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance. Extreme-RL is\nparticularly rigid, requiring a peak force of 51.14 N, while\nVanilla-RL requires 24.59 N. In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A\n\nC. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising\n\nHugging\nin right position\n\nHugging with\nmisalignment\n\nSensor Pad\nReal-time Pressure\nVisualization\n\nExtreme-RL\n\nHugging with misalignment\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n8\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n3\n\n4\n\n2\n\n0\n\n0\n\n0\n\n0\n\n6\n\n16\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n2\n\n0\n\n17\n\n9\n\n0\n\n0\n\n0\n\n0\n\n50\n\n6\n\n6\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n28\n\n5\n\n5\n\n0\n\n30\n\n7\n\n1\n\n1\n\n1\n\n3\n\n45\n\n9\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n111\n\n5\n\n3\n\n0\n\n11\n\n1\n\n1\n\n1\n\n0\n\n8\n\n34\n\n41\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n19\n\n26\n\n37\n\n2\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n10\n\n5\n\n0\n\n0\n\n6\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n15\n\n20\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n4\n\n0\n\n0\n\n0\n\n0\n\n38\n\n13\n\n17\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n11\n\n0\n\n16\n\n1\n\n0\n\n0\n\n0\n\n1\n\n3\n\n11\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n7\n\n12\n\n2\n\n4\n\n1\n\n2\n\n5\n\n1\n\n0\n\n0\n\n0\n\n1\n\n8\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n9\n\n8\n\n16\n\n2\n\n0\n\n0\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n208\n\n61\n\n8\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n2\n\n0\n\n10\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n7\n\n13\n\n0\n\n1\n\n4\n\n7\n\n23\n\n1\n\n0\n\n1\n\n4\n\n11\n\n12\n\n0\n\n4\n\n2\n\n2\n\n0\n\n0\n\n0\n\n2\n\n27\n\n6\n\n0\n\n0\n\n0\n\n19\n\n11\n\n0\n\n0\n\n62\n\n0\n\n2\n\n11\n\n25\n\n4\n\n1\n\n0\n\n0\n\n4\n\n2\n\n52\n\n7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n5\n\n5\n\n0\n\n2\n\n3\n\n64\n\n4\n\n0\n\n0\n\n0\n\n22\n\n6\n\n83\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n2\n\n6\n\n15\n\n0\n\n0\n\n0\n\n2\n\n17\n\n8\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n0\n\n16\n\n417\n\n10\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n2\n\n19\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n4\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n4\n\n551\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n14\n\n16\n\nGentleHumanoid\n\nVanilla-RL\n\nHugging in right position\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\nExtreme-RL\n\nHugging with misalignment\n30\n\n25\n\n25\n\nPeak Force (N)\n\n30\n\n20\n15\n10\n5\n0\n\n400\n\nPressure (kPa)\n\nVanilla-RL\n\nGentleHumanoid\n\nHugging in right position\n\nForce (N)\n\nkey observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit.\nFor example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N. This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines.\n2) Hugging a mannequin. We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.\nPressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge. Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion.\n3) Handling deformable objects. Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N. As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it.\nAcross all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.\n\n20\n15\n10\n5\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\n14\n\n16\n\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nTime (s)\n\n10\n\n12\n\nFig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.\n\nfor healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions.\nWe also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c). From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner. Experiments\n\nwith participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions.\nV. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots. By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction.\nSeveral limitations remain. First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage. Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation. Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.\nVI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset. We further thank the\nUnitree team for their timely and reliable hardware support.\nR EFERENCES\n[1] D. J. Agravante, A. Cherubini, A. Sherikov, P.-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol. 35, no. 4, pp. 833 846, 2019. 2\n[2] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024. 2, 5\n[3] M. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412.13196, 2024. 2\n[4] Z. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506.14770, 2025. 2\n\n[5] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He,\nN. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.01143, 2025. 2\n[6] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\nC. Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406.08858, 2024. 2\n[8] Y. Ze, Z. Chen, J. P. Arau jo, Z. ang Cao, X. B. Peng, J. Wu, and\nC. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505.02833, 2025. 2, 5, 7\n[9] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024, pp. 15 366 15 372.\n2\n[10] B. Xu, H. Weng, Q. Lu, Y. Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505.06883, 2025. 2, 3, 5\n[11] P. Zhi, P. Li, J. Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.20829, 2025. 2\n[12] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.06776, 2025. 2\n[13] M. Murooka, K. Chappellet, A. Tanguy, M. Benallegue, I. Kumagai, M. Morisawa, F. Kanehiro, and A. Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 5597 5604, 2021.\n2\n[14] E. Dantec, R. Budhiraja, A. Roig, T. Lembono, G. Saurel, O. Stasse,\nP. Fernbach, S. Tonneau, S. Vijayakumar, S. Calinon et al., Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208. 2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024. 2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209.13145, 2022. 2\n[17] A. Rigo, M. Hu, S. K. Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp. 15 322 15 328. 2\n[18] N. Fey, G. B. Margolis, M. Peticco, and P. Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502.10894, 2025. 2\n[19] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida, S. Guo, and\nS. Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A. E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph.D. dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S. Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no. 3, pp. 5705 5712, 2021. 3\n[22] H. Lefe vre, T. Chaki, T. Kawakami, A. Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024. 3\n[23] Y. Sun, R. Chen, K. S. Yun, Y. Fang, S. Jung, F. Li, B. Li, W. Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502.03132, 2025. 3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016. 5\n[25] J. Kim, A. Alspach, I. Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp. 557 561. 5\n[26] Y. Nam, S. Yang, J. Kim, B. Koo, S. Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no. 4, p. 2206, 2023. 5\n\n[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol. abs/1707.06347, 2017. 5\n[28] Y. Ze, J. P. Arau jo, J. Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository. 5\n[29] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.\nBlack, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp. 5442 \n5451. 5\n[30] L. Xu, X. Lv, Y. Yan, Y. Jin, G. Wu, Y. Xu, L. Qiao, X. Zhu, J. Liu,\nR. Zhang et al., Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312.16051, 2023. 5\n[31] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol. abs/2102.04942, 2021. 5\n[32] M. J. Black, P. Patel, J. Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2023, pp. 8726 8737. 7, 10\n[33] Y. Wang, Y. Sun, P. Patel, K. Daniilidis, M. J. Black, and M. Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp.\n1148 1159. 11\n\nA PPENDIX\n\nB. Reference Dynamics Integration\n\nA. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator.\n1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).\nGains may gently increase, hold, and then decrease back to\nzero at the end of the interval.\nIn parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping.\n2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame.\nWe use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change.\n3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.e., leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction. This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts.\n4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.\nTABLE II: External Force Application Parameters.\nParameter\n\nSymbol\n\nTypical value / range\n\nMax per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain\n\nFmax\nτsafe (t)\nτF\nτM\nKspring (t)\n\n30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250\n\nAll reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)\n\nThe driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter\n\nSymbol\n\nValue\n\nVirtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip\n\nM\nD\nKp\nKd\n t\nNsub\n x max\n x max\n\n0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2\n\nC. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential.\nWhile our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape. To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point.\nNext, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane. We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection.\nThe objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r\n\n( ,k) S\n\n2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.\n\nAfter obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.\nD. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.\nFinally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e.g., with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
{"method": "hybrid", "num_chunks": 873, "avg_chunk_len": 56.243986254295535, "std_chunk_len": 131.46013655829853, "max_chunk_len": 758, "min_chunk_len": 1, "total_chars": 49101, "compression_ratio": 1.031486120445612, "chunks": ["GentleHumanoid: Learning Upper-body Compliance for Contact-rich\nHuman and Object Interaction\nQingzhou Lu , Yao Feng , Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu\nStanford University\nProject Page: gentle-humanoid.axell.top", "arXiv:2511.04679v1 [cs.RO] 6 Nov 2025", "(a) Sit-to-stand Support", "(b) Handshaking", "(d) Balloon Handling", "GentleHumanoid\n(c) Shape-aware Hugging", "Vanilla Tracking RL", "Tracking RL w/ Large Perturbation", "Fig. 1: GentleHumanoid learns a universal whole-body control policy with upper-body compliance and tunable force limits. It enables: (a) sit-to-stand assistance, where the robot provides support across multiple links (hand, elbow, and shoulder);\n(b) handshaking with a 5 N force limit, allowing the robot s hand to move naturally with the human s; (c) autonomous\nshape-aware hugging, where the robot adapts its posture to the partner s body shape (estimated from camera input) for a\ncomfortable embrace; and (d) balloon handling, showing safe object manipulation where baselines fail.", "Abstract Humanoid robots are expected to operate in\nhuman-centered environments where safe and natural physical\ninteraction is essential. However, most recent reinforcement\nlearning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are\ntypically restricted to base or end-effector control and focus\non resisting extreme forces rather than enabling compliance.", "We introduce GentleHumanoid, a framework that integrates\nimpedance control into a whole-body motion tracking policy to\nachieve upper-body compliance. At its core is a unified springbased formulation that models both resistive contacts (restoring\nforces when pressing against surfaces) and guiding contacts\n(pushes or pulls sampled from human motion data). This\nformulation ensures kinematically consistent forces across the\n Equal contribution.", "This work was done during Qingzhou Lu s intern-", "ship at Stanford University. Qingzhou is now with Tsinghua University.", "shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through taskadjustable force thresholds. We evaluate our approach in both\nsimulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging,\nsit-to-stand assistance, and safe object manipulation. Compared\nto baselines, our policy consistently reduces peak contact forces\nwhile maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward\nhumanoid robots that can safely and effectively collaborate with\nhumans and handle objects in real-world environments.", "I. INTRODUCTION\nSafe and compliant physical interaction is essential for\ndeploying humanoids in human-centered environments. Reinforcement learning (RL) has recently enabled impressive", "whole-body locomotion and manipulation [1] [8]. However,\nmost policies emphasize rigid position or velocity tracking\nand treat external forces as disturbances to suppress, which\nlimits their applicability to tasks requiring adaptive compliance, such as handling objects. To address this, recent\nworks have integrated impedance or admittance control into\nRL [9] [11] or attempted to learn forceful loco-manipulation\nimplicitly [12].", "However, these approaches are restricted to\nbase or end-effector control and typically emphasize resisting\nextreme forces rather than supporting compliant interaction. In contrast, interactions such as giving a comforting hug or\nassisting with sit-to-stand support require compliance across\nthe entire upper-body kinematic chain, where multiple links\nincluding shoulders, elbows, and hands may be in contact\nsimultaneously. Depending on the scenario, compliance must\nrange from gentle yielding (e.", "g. , hugging people or handling\nfragile objects) to firm, supportive assistance (e. g.", ", sit-tostand), while always remaining within safe force thresholds. This raises two main challenges: (1) coordinating force\nresponses across multiple links of the kinematic chain, and\n(2) adapting to diverse contact scenarios, from gentle touch\nto strong supportive forces. We address these challenges with GentleHumanoid, a\nframework that integrates impedance control into a motiontracking policy to achieve whole-body humanoid control with\nupper-body compliance.", "The humanoid s action is influenced\nby two forces: a driving force for motion tracking, modeled\nas a virtual spring damper system that pulls link positions\ntoward target motions, and an interaction force that represents physical contact with humans or objects. Since collecting real interaction data is difficult, we simulate interaction forces during RL training. Physics engines\nsuch as MuJoCo and IsaacGym can generate contact forces at\ncolliding surfaces, but these are often noisy, local, and uncoordinated, unlike the smooth multi-joint compliance observed\nin human human interactions.", "They also only occur when\ncollisions arise during rollout, limiting coverage of diverse\ninteraction scenarios. To address this, we introduce a unified\nspring-based formulation with two cases: (i) resistive contact,\nwhen the humanoid presses against a surface, modeled by\nfixing the spring anchor at the initial contact point to generate\nrestoring forces; and (ii) guiding contact, when the humanoid\nis pushed or pulled by external agents, modeled by sampling\nspring anchors from upper-body postures in human motion\ndatasets. Importantly, sampling from complete postures ensures forces remain coordinated across the kinematic chain\n(e.", "g. , shoulder, elbow, wrist), rather than being applied independently to each link. This method provides kinematically\nconsistent and diverse interaction forces, enabling the policy\nto learn robust compliance.", "To further ensure safety, we apply\nforce-thresholding during training, with adjustable limits at\ndeployment based on task requirements. We evaluate GentleHumanoid against baselines, including\na vanilla whole-body RL tracking policy and an end-effectorbased force-adaptive policy, in both simulation and on the\nUnitree G1 humanoid. Quantitative tests use commercial\nforce gauges and conformable, customized waist-mounted", "pressure sensing pads with 40 calibrated capacitive taxels to\nmeasure contact forces and pressures. Qualitative demonstrations cover scenarios requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and softobject manipulation. We also show an autonomous hugging\npipeline that integrates our policy with vision-based human\nshape estimation for personalized hugs.", "In summary, the main contributions of this work are:\n We propose GentleHumanoid, a framework that integrates impedance control with motion tracking to\nachieve whole-body humanoid control with upper-body\ncompliance. Central to the framework is a unified formulation of interaction force modeling that covers both\nresistive and guiding contacts, sampling from human\nmotion datasets to ensure kinematic consistency and\ncapture diverse interaction scenarios. We develop a force-thresholding mechanism that maintains interaction forces within safe limits, enabling\ncomfortable and safer physical human robot interaction.", "We design a hugging evaluation setup with a custom\npressure-sensing pad tailored for hugging, providing\nreliable measurement of distributed contact forces. We\nvalidate our approach in both simulation and on the Unitree G1 humanoid, showing safer, smoother, and more\nadaptable performance than baselines across hugging,\nsit-to-stand assistance, and object manipulation. II.", "R ELATED W ORK\nA. Humanoid Whole Body Control\nWhole-body control for humanoid robots is a longstanding challenge in robotics. The difficulty is precipitated\nby high-dimensional dynamics and human-like morphology that introduces inherent instability.", "Traditional modelbased methods, such as model predictive control (MPC),\ncan produce stable behaviors but demand extensive expert\ndesign and meticulous tuning to balance feasibility and\ncomputational cost [13] [15]. More recently, learning-based\nmethods have alleviated many of the challenges of tedious\ndesign in model-based methods. In particular, learning from\nhuman motion data has been successful for producing highly\ndynamic motions with single-skill policies [5] and generalist\npolicies [3], [4], [6].", "Similar frameworks have also been\nused for whole-body tele-operation [2], [7], [8]. However,\nthese approaches often neglect scenarios involving complex\ncontact dynamics, which reduces their robustness to external\ndisturbances and raises safety concerns in close physical\ninteraction with humans. B.", "Force-adaptive Control\nTo address the aforementioned issue of robust and safe\ncontact, classical force-adaptive methods such as impedance\nand admittance control regulate interaction forces and have\nbeen extended to whole-body frameworks [15] [17]. More\nrecently, RL-based approaches have incorporated impedance\nor admittance control for adaptive contact behaviors [9] [11],\nwhile others aim to implicitly learn robustness to external\ndisturbances and extreme forces [12], [18]. However, these", "methods typically focus on end-effector interactions rather\nthan interactions that involve other body parts. In tasks\nsuch as carrying large objects or interacting with a human,\ncontact is not restricted to the wrists/hands but may involve\ncoordinated force distribution across multiple links, including\nelbows, and shoulders. Our work addresses this gap by\nintroducing a framework that models compliance across the\nwhole upper body kinematic chain.", "C. Human-humanoid Interaction\nAs humanoid robots move closer to deployment in humancentered environments, their ability to interact physically\nwith people becomes increasingly important. Towards this\ngoal, early works have explored using human-in-the-loop\nstrategies and haptic feedback to deliver soft and comfortable contact [19], [20].", "More recent efforts have applied\ntraditional control methods to assist humans in specific\ntasks such as sit-to-stand transitions [21], [22]. However,\nthese approaches are typically tailored to a single scenario,\nand the resulting policies do not generalize across different\ninteraction contexts such as both hugging and sit-to-stand\nassistance. Other recent works shift the focus to visionbased criteria, for example, designing policies that enable\nhumanoids to consistently avoid human collisions [23].", "In\ncontrast, our approach proposes a general motion-tracking\npolicy capable of handling multiple interaction scenarios. In\nparticular, for hugging tasks, we combine the policy with\nvisual perception to customize hugging positions for people\nof different body shapes. III.", "M ETHOD\nA. Problem Formulation\nOur goal is to achieve whole-body humanoid control that\nis both robust and safe, enabling humanoids to perform diverse motions while interacting compliantly with humans and\ndeformable objects. We frame this as learning a compliant\nmotion-tracking policy: the humanoid should follow humanlike movements while adapting its behavior in response to\ninteraction forces.", "Unlike rigid trajectory tracking, humans\nnaturally adjust their actions based on contact feedback,\nwhich motivates our use of impedance-based control. Since\nmost physical interactions occur in the upper body, we\nfocus on modeling it as a multi-link impedance system with\nkeypoints at the shoulders, elbows, and hands. As illustrated\nin Fig.", "2, the motion of each link position is influenced by\nthe combination of driving forces from target motions and\ninteraction forces from humans or objects:\nM x i = fdrive,i + finteract,i ,", "(1)", "where xi is the position of link i, x i is acceleration, and M\nis a scalar virtual mass (kg) per link. We set M as 0.1 kg\nin our reference dynamics model. The driving force fdrive,i\nis a virtual spring damper term from classical impedance\ncontrol, pulling the link position toward its target motion, and\nfinteract,i captures forces arising from interactions with the\nenvironment, including humans and objects. In the following\nsections, we detail the formulation of each force component.", "For clarity, we introduce the index i once and omit it\nhenceforth. All link positions x and velocities x are 3D\nCartesian quantities expressed in the robot s root frame.\nB. Impedance-Based Driving Force from Target Motion\nFollowing prior work [10], [15], we generate driving\nforces from the target motion to pull each link position\ntoward its target trajectory. The force is modeled as a virtual\nspring damper system:\nfdrive = Kp (xtar xcur ) + Kd (vtar vcur ) ,", "(2)", "where xcur , vcur are the current link position and velocity,\nand xtar , vtar are the corresponding target link position and\nvelocity from the target motion. The gains Kp and Kd\ndenote the impedance stiffness and damping, respectively,\ncontrolling how strongly the link position tracks its target. To ensure stable and smooth\np behavior, we set the damping to\nthe critical value, Kd = 2 M Kp .", "All x and v terms above\ndenote 3D Cartesian link states (in the root frame), while the\npolicy produces actions in joint space that are tracked by lowlevel joint PD controllers. The RL policy learns to coordinate\nthese compliant forces across multiple joints, mapping them\ninto joint-level actions that balance stability and adaptability\nin whole-body control. C.", "Interaction Force Modeling\nWhen no interaction occurs, the driving force alone\nenables the humanoid to follow target motions. In real\nscenarios, however, physical contact introduces additional\ninteraction forces across multiple links, often correlated in\ndirection and magnitude. To capture these effects, we design\na unified interaction force model that accounts for both multilink coupling and force diversities.", "We distinguish two cases:\nResistive contact: Forces generated when the humanoid\nitself presses against a human or object. Guiding contact: Forces applied by an external agent,\nsuch as a human pushing or pulling the humanoid s arm. Both cases are modeled using the same spring formulation\nwith a consistent anchor terminology:", "finteract = Kspring xanchor xcur ,\n(3)\nwhere Kspring is the stiffness, xcur is the current link position,\nand the spring anchor xanchor is defined as", "xcur (t0 ), resistive contact,\nxanchor =\n(4)\n x\nguiding contact.\nsample ,\nHere, xcur (t0 ) is the link position at the moment of initial\ncontact (fixing a virtual spring anchor), xsample is a link position sampled from a dataset posture, representing an external\nagent steering the humanoid toward a new configuration.\nThis formulation provides a unified framework: Resistive\ncontact yields restoring forces that resist deviations from the\ncontact point, while Guiding contact yields guiding forces\nthat pull the humanoid toward externally defined postures.\nPosture samples are drawn from real human motion data,\nensuring that the guiding forces are kinematically valid and", "(a) Reference Dynamics", "(c) Deployment\nHRI Motion", "Contact", "Hugging\nPlanner", "timestep", "Vision", "Driving\nForce", "Interaction\nForce", "Hugging Motion", "Motion Target\nCurrent Pos\nAnchor Pos", "GentleHumanoid Policy\nProprioception", "Privileged Obs", "(b) Training\nReward", "Safe & Compliant Interactions", "Policy", "Target Motion\nAction", "Simulator", "Fig. 2: Overview framework. (a) Reference dynamics: impedance-based dynamics integrate driving forces (for motion\ntracking) and interaction forces (for compliant contact), producing reference link (on the shoulders, elbows and hands)\npositions and velocities. (b) Training: the policy receives proprioception, privileged observations, and target motions, and\nis optimized using rewards that compare simulated states (xsim , x sim ) to reference dynamics (xref , x ref ). (c) Deployment:\nthe trained GentleHumanoid policy is applied to real-world tasks, including vision-based autonomous hugging and other\nhuman robot interaction scenarios, enabling safe and compliant behaviors such as hugging, sit-to-stand assistance, and\nhandling large deformable objects.", "D. Safety-Aware Force Thresholding\nIn Equation 2, the driving force grows proportionally with\ntracking error. Without limitation, large deviations from the\ntarget motion can result in unbounded forces, potentially\nexceeding safe interaction levels. To prevent this, we introduce an adaptive force thresholding mechanism that caps the", "Right Shoulder Link", "Right Elbow Link", "0.200", "Right Hand Link\n0.16", "0.7\n0.175\n0.6", "0.14", "0.150", "0.5", "Density", "correspond to plausible upper-body movements. Specifically,\nwe precompute posture distributions from motion dataset,\nduring training, select postures close to the current multi-link\npositions. From these, a target position is randomly sampled\nand used as the spring anchor to generate guiding forces.", "To further increase interaction diversity, we randomize\nboth stiffness and the active links. The stiffness is sampled\nas Kspring U(5, 250). Active-contact sets are chosen with\nthe following probabilities: 40% no external force; 15% both\narms (all 6 links) under force; 30% a single arm (left or\nright; its 3 links) under force (15% each arm); and 15%\nonly a single link under force.", "Anchors and selections are\nresampled every 5 seconds with a short transition window\nto ensure continuity. This exposes the policy to a broad\nrange of interaction dynamics, enabling it to learn robust\ncompliance while preserving consistency along the kinematic\nchain. As a result, the model can simulate diverse external\nforce directions and magnitudes; Figure 3 visualizes the\nresulting distribution, showing that forces span a wide range\nof directions on the sphere with magnitudes from 0 to 25 N.", "0.4", "0.12", "0.125", "0.10", "0.100", "0.08", "0.3", "0.075", "0.06", "0.2", "0.050", "0.04", "0.1", "0.025", "0.0", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.000", "0.02\n0", "5", "10", "15", "20", "25", "Force magnitude & direction", "0.00", "0", "5", "10", "15", "20", "25", "Force magnitude & direction", "Fig. 3: Interaction force distributions across upper-body\nlinks. Probability densities of force magnitudes are shown\nfor the right shoulder (left), right elbow (middle), and right\nhand (right). Insets (top right) illustrate the corresponding\nforce directions on a sphere.", "maximum allowable force applied by the robot.\nWe define a range of force thresholds and sample a\npiecewise-constant value τsafe during training: F1 τsafe \nF2 . The threshold is resampled every 5 seconds, encouraging\nthe policy to remain robust across a range of safety limits.\nThe current threshold is also provided to the policy as part\nof the observation. Here, F1 and F2 define the range for the\nmaximal allowable force the robot should apply in various\ntasks. When the driving force exceeds the threshold, we\napply a scaling mechanism:", "τsafe\nfdrive limited = min 1.0,\n fdrive ,\n(5)\n fdrive", "compliance. The threshold directly tunes compliance: lower\nvalues yield softer, safer behavior for gentle interactions\nlike hugging, while higher values allow firmer support for\ntasks such as sit-to-stand assistance, all while maintaining\nsafety bounds. The choice of exact threshold depends on the\napplication.", "Since our focus is humanoid interaction with\nhumans and fragile objects (e. g. , balloons), we set F1 = 5 N\nand F2 = 15 N.", "These values are benchmarked against both\nISO/TS 15066 [24] safety ceilings and comfort studies. In\nthe extreme case of a minimal 0. 5 0.", "5 cm contact area\n(0. 25 cm2 ), 15 N corresponds to 60 N/cm2 , still below\nISO/TS 15066 pain-onset limits for torso and arms (e. g.", ",\nback/shoulder: 160 N/cm2 , chest: 120 N/cm2 ). For more realistic hugging contacts of 16 cm2 , this range corresponds\nto 3 9 kPa, consistent with measurements of children s hugs\n(soft hugs < 7 kPa, strong hugs 18 kPa) [25] and\nrehabilitation studies recommending pressures 13 kPa for\ncomfort [26]. Thus, our thresholds remain well below ISO\nceilings while lying in a comfort-oriented band.", "E. RL-based Control Policy\nFormally, we consider a humanoid robot at time t with\nobservation ot containing its proprioception and a target\nmotion sequence mtar . The policy π(at | ot ) outputs joint\nposition targets at at 50 Hz for low-level PD tracking,\nenabling the humanoid to follow the target motion while\nexhibiting compliant responses to interaction forces finteract .", "To incorporate the impedance-based reference dynamics,\nwe simulate the model using semi-implicit Euler integration,\nwith a fixed time step of 0. 005 s:\nfdrive +finteract\nref\n,\nx ref\nt+1 = x t + t \nM", "(6)", "ref\nref\nxref\nt+1 = xt + t x t+1 .", "(7)", "Where t is the integration step size, and xref\nt denotes the\nlink position in the reference dynamics model, which we\ndistinguish from the actual robot link position xsim in the\nsimulator. The objective is to guide the robot to follow the\nimpedance rules encoded in the reference dynamics. At each\ntimestep, velocities and positions are updated according to\nthe net driving and interaction forces, with semi-implicit\nEuler ensuring numerical stability.", "This impedance-based reference dynamics system specifies the compliant behavior the policy is trained to reproduce. We compute xref via the above integration and use it in the\nlink-position tracking rewards (details in Reward Design). During training, the RL agent observes ot and outputs at\nsuch that the resulting behavior aligns with this dynamics\nmodel.", "In effect, the policy learns to track target motions\nwhile adapting to stochastic interaction forces, yielding stable, compliant whole-body control across diverse scenarios. 1) Teacher-Student Architecture: We employ a two-stage\nteacher student training framework for sim-to-real transfer. We adopt the same teacher-student architecture and training\nprocedure from prior work [10], and train both policies with\nPPO [27].", "The student policy observes only information\navailable during real-world deployment:\not = (τsafe , mtar , ω, g, qthist , at 3:t 1 ) ,", "where τsafe represents the current force-safety limit, that\ncan be changed by use during deployment; mtar contains\ntarget motion information including future root poses and\ntarget joint position; ω is the root angular velocity; and\ng is gravity expressed in the robot s root frame (projected\ngravity). qthist provide joint-position history, and at 3:t 1\ncontains the recent action history. The teacher policy additionally receives comprehensive\nprivileged information:\nref\nsim\nopriv\n= (xref\nt\nt , x t , finteract , finteract , ht , τt 1 , ecum ) ,\nref\nwhere xref\nt and x t are the integrated link positions and\nvelocities from the impedance-based reference dynamics\n(Eq.", "7); finteract denotes the interaction force predicted by\nsim\nthe reference dynamics, while finteract\nis the actual interaction\nforce measured in simulation. Ideally, finteract should closely\nsim\n. ht represents link heights relative to the\nmatch finteract\nground; τt 1 are the previous joint torques; and ecum denotes\nthe cumulative tracking error.", "Both policies output joint position targets at R29 which\nare tracked by low-level PD controllers. 2) Motion Datasets: We use diverse human motion to\ntrain our policy, covering data for both human-human\nand human-object interactions datasets. Specifically, we use\nGMR [28] to retarget the AMASS [29], InterX [30], and\nLAFAN [31] datasets, and filter out some high-dynamic motions that do not conform to interaction scenarios, ultimately\nobtaining approximately 25 hours of dataset with a sampling\nfrequency of 50Hz.", "3) Reward Design: Following prior work on whole-body\nhumanoid control [2], [8], we adapt rewards for motion\ntracking and locomotion stability, as summarized in Table I,\nto encourage accurate motion tracking and stable balance. In GentleHumanoid, we additionally design a compliance\nreward composed of three terms:\nReference Dynamics Tracking. We encourage the robot\nto follow the compliant reference dynamics by minimizing\nthe discrepancy between the actual link state in simulation\nsim\nref\nref\n(xsim\nt , x t ) and the reference state (xt , x t ) from Eq.", "7:", "x sim x ref\n xsim xref\nt 2\nt 2\n+ exp t\n.\nrdyn = exp t\nσx\nσv", "Exponential kernels provide smooth gradients, with σx and\nσv controlling sensitivity.\nReference Force Tracking. To align predicted interaction\nforces with actual forces measured in simulation, we penalize\nthe discrepancy between finteract from the reference dynamics\nsim\nand finteract\nfrom the environment:", "sim\n finteract finteract\n 2\nrforce = exp \n.\nσf\nThis term complements position tracking by explicitly\nregulating force magnitudes, which is crucial for enforcing\nsafe maximum force thresholds.\nUnsafe Force Penalty. To further discourage unsafe behaviors, we penalize interaction forces that exceed the safety", "margin τsafe , in addition to the driving force thresholding in\nEq. 5:\nrpen = I( finteract > τsafe + δtol ) .", "rcompliance = wdyn rdyn + wforce rforce + wpen rpen .\nThe weights for each term along with those for motion\ntracking and locomotion stability are provided in Table I.\nTABLE I: Reward Terms and Weights.\nReward", "Vanilla-RL", "Extreme-RL", "Right Elbow Link", "20", "Right Shoulder Link\n15", "15\n10\n5\n0", "Force (N)", "20", "Force (N)", "Force (N)", "Here, δtol is a tolerance margin that allows minor deviations beyond τsafe without triggering large penalties. This\nprevents the policy from becoming overly conservative while\nstill discouraging forces that are clearly unsafe. In practice,\nwe set δtol as 10 N based on empirical observations.\nThe overall compliance reward is a weighted sum of these\nterms:", "GentleHumanoid\nRight Hand Link\n15\n10\n5\n0\n0", "2", "4", "6", "8", "10", "10", "5", "0\n0", "2", "Time (s)", "4", "6", "8", "10", "0", "2", "Time (s)", "4", "6", "8", "10", "Time (s)", "Fig. 4: Forces applied by different upper-body links under\nexternal interaction. Force profiles over time are shown for\nthe right hand (left), right elbow (middle), and right shoulder\n(right). Compared to baselines (Vanilla-RL and ExtremeRL), GentleHumanoid maintains lower and more stable force\nlevels across all links, showing safer and more compliant\nresponses during contact.\n !\"#$ =10N", "!\"#$ =15N", "Weight\n !\"#$ =5N", "Compliance\nReference Dynamics Tracking\nReference Force Tracking\nUnsafe Force Penalty", "2.0\n2.0\n6.0", "GentleHumanoid with different force limits", "Motion Tracking\nRoot Tracking\nJoint Tracking", "0.5\n1.0\nVanilla-RL", "Locomotion Stability\nSurvival\nFeet Air Time\nImpact Force\nSlip Penalty\nAction Rate\nJoint Velocity\nJoint Limit", "5.0\n10.0\n4.0\n2.0\n0.1\n5.0e-4\n1.0", "Extreme-RL", "Fig. 5: Comparison of interaction forces across policies. Top:\nGentleHumanoid with tunable force limits, which maintains\nsafe interaction by keeping contact forces within specified\nthresholds across different postures. Bottom: baseline methods, Vanilla-RL and Extreme-RL, exhibit less consistent\ncompliance, with higher peak forces or oscillatory responses.\nForce gauge readings (N) are highlighted for clarity.", "IV. E XPERIMENTS\nWe conduct both simulation and real-world experiments to\nevaluate the effectiveness of GentleHumanoid. We compare\nagainst two baselines that adopt different training strategies:\nVanilla-RL: an RL-based motion tracking policy trained\nwithout force perturbations, representative of prior wholebody tracking approaches; Extreme-RL: an RL-based motion tracking policy trained with maximum 30 N end-effector\nforce perturbations, representative of prior force-adaptive\nmethods.", "A. Simulation Results\nWe first benchmark against baselines in simulation using\na hugging motion. To evaluate compliance, we simulate\nan external pulling force that attempts to move the robot\naway from its hugging posture, mimicking a human trying\nto break free from an embrace.", "As shown in Figure 4,\nour method consistently maintains lower and more stable\ninteraction forces across the hand, elbow, and shoulder\nlinks. At the hand, GentleHumanoid stabilizes around 10\nN, whereas Vanilla-RL settles above 20 N and ExtremeRL exceeds 13 N. Similar trends are observed at the elbow\nand shoulder: while baselines quickly saturate at 15 20 N", "with rigid responses, GentleHumanoid remains bounded near\n7 10 N. These results show that our method adapts smoothly\nto external interaction, yielding compliant motions, while\nbaselines remain overly stiff and exert higher peak forces. B.", "Real-World Experiments\nWe deploy our whole-body control policy on the Unitree\nG1 humanoid to evaluate compliance in real-world interactions. Three reference scenarios are considered:\n1) Static pose with external force. We first test compliance by applying external forces at the wrist while the robot s\nbase remains static.", "Ideally, the arm should yield softly, moving with the external force instead of resisting rigidly. Forces\nare applied using a handheld force gauge (Mark-10, M510), which also records peak values. As shown in Figure 5,\nboth baselines resist stiffly: rather than letting the arm move,\nthe torso shifts, often leading to imbalance.", "Extreme-RL is\nparticularly rigid, requiring a peak force of 51. 14 N, while\nVanilla-RL requires 24. 59 N.", "In contrast, GentleHumanoid\nresponds smoothly and consistently, requiring much lower\nforces to reposition the arm while maintaining balance. A", "C. More Applications\nGentleHumanoid enables applications where compliance\nis critical. We integrate our policy with a locomotion teleoperation framework for the Unitree G1, allowing users to\ncontrol walking and trigger pre-defined reference motions\nsuch as hugging, sit-to-stand assistance, and object handling.\nDemonstrations of joystick-based control are provided in the\nsupplementary video. While this work focuses on locomotion teleoperation, extending GentleHumanoid to full-body\nteleoperation such as TWIST [8] is an important direction\nfor future work. The inherent compliance of our method\nensures safe interactions even during teleoperation under\ndirect physical contact, making it particularly promising", "Hugging\nin right position", "Hugging with\nmisalignment", "Sensor Pad\nReal-time Pressure\nVisualization", "Extreme-RL", "Hugging with misalignment", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "8", "0", "0", "1", "0", "0", "0", "0", "1", "3", "4", "2", "0", "0", "0", "0", "6", "16", "8", "0", "0", "0", "0", "0", "1", "0", "0", "1", "2", "0", "17", "9", "0", "0", "0", "0", "50", "6", "6", "0", "0", "0", "1", "0", "0", "0", "28", "5", "5", "0", "30", "7", "1", "1", "1", "3", "45", "9", "8", "1", "0", "0", "0", "0", "0", "1", "111", "5", "3", "0", "11", "1", "1", "1", "0", "8", "34", "41", "4", "0", "0", "0", "0", "0", "0", "19", "26", "37", "2", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "15", "10", "5", "0", "0", "6", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "7", "4", "0", "0", "0", "0", "0", "0", "0", "0", "2", "1", "1", "0", "0", "0", "0", "0", "15", "20", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "4", "4", "0", "0", "0", "0", "38", "13", "17", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "11", "0", "16", "1", "0", "0", "0", "1", "3", "11", "0", "0", "0", "0", "0", "1", "7", "12", "2", "4", "1", "2", "5", "1", "0", "0", "0", "1", "8", "1", "0", "0", "0", "0", "0", "0", "9", "8", "16", "2", "0", "0", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "208", "61", "8", "3", "1", "0", "1", "0", "0", "0", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "2", "0", "10", "0", "0", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "7", "13", "0", "1", "4", "7", "23", "1", "0", "1", "4", "11", "12", "0", "4", "2", "2", "0", "0", "0", "2", "27", "6", "0", "0", "0", "19", "11", "0", "0", "62", "0", "2", "11", "25", "4", "1", "0", "0", "4", "2", "52", "7", "0", "0", "0", "0", "0", "0", "5", "5", "0", "2", "3", "64", "4", "0", "0", "0", "22", "6", "83", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "2", "2", "6", "15", "0", "0", "0", "2", "17", "8", "0", "0", "0", "0", "0", "0", "0", "0", "2", "0", "0", "16", "417", "10", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "0", "0", "2", "19", "5", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1", "4", "0", "0", "0", "0", "0", "0", "0", "0", "0", "4", "551", "1", "0", "0", "0", "0", "0", "14", "16", "GentleHumanoid", "Vanilla-RL", "Hugging in right position", "350", "300", "250", "200", "150", "100", "50", "0", "Extreme-RL", "Hugging with misalignment\n30", "25", "25", "Peak Force (N)", "30", "20\n15\n10\n5\n0", "400", "Pressure (kPa)", "Vanilla-RL", "GentleHumanoid", "Hugging in right position", "Force (N)", "key observation is that GentleHumanoid provides postureinvariant compliance: the same external force suffices to\nmodulate arm position across different configurations. Moreover, compliance level matches the user-specified force limit. For example, when set to 10 N, the robot maintains balance\naround that threshold across postures, with effective ranges\nbetween 5 15 N.", "This uniform, predictable response arises\nfrom our formulation, which regulates compliance through\nvirtual spring damper dynamics and safety thresholds rather\nthan raw joint mechanics. As a result, human interaction feels\nsafer and more consistent than with baselines. 2) Hugging a mannequin.", "We next evaluate hugging performance under two conditions. In the first, the mannequin\nis properly aligned with the robot, and the G1 executes a\nhugging motion. In the second, the mannequin is deliberately misaligned to assess safety under imperfect contact.", "Pressure-sensing pads attached to the mannequin measure\ncontact forces. We set τsafe as 10 N in GentleHumanoid to\ncompare with baselines. For sensor calibration, a motorized\nstage with a PDMS applicator was used to map normalized\nsensor values to ground-truth pressures measured by a force\ngauge.", "Under localized contact, we approximate the effective\ncontact area of each texel as 6 mm 6 mm and compute\nforces from the corresponding pressure values recorded in the\npad. The evaluation setups and results are shown in Figure 6,\nGentleHumanoid maintains bounded and stable forces even\nunder misalignment, whereas the baselines Vanilla-RL and\nExtreme-RL generate higher, less predictable forces or fail\nto sustain the motion. 3) Handling deformable objects.", "Finally, we test the\nability to handle fragile objects such as balloons. The challenge is to maintain contact forces within a safe range:\ninsufficient force fails to stabilize the object, while excessive\nforce causes deformation or collapse. For this experiment,\nthe force threshold in GentleHumanoid is set to 5 N.", "As\nshown in Figure 1(d), GentleHumanoid successfully holds\nthe balloon without damage, whereas both baselines apply\nexcessive pressure, eventually squeezing the balloon until the\nG1 loses balance and drops it. Across all scenarios, GentleHumanoid consistently reduced peak interaction forces compared to baselines, resulting in safer and smoother contact.", "20\n15\n10\n5", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "14", "16", "0", "0", "2", "4", "6", "8", "Time (s)", "10", "12", "Fig. 6: Evaluation of hugging interactions with and without\nmisalignment. Top: experimental setup with custom pressuresensing pads and real-time pressure visualization. Middle:\npressure maps of peak force frames for different controllers\nunder correct hugging alignment (left) and misalignment\n(right). GentleHumanoid maintains moderate contact pressures, while baselines produce localized high-pressure peaks,\nespecially under Vanilla-RL. Bottom: Force profiles over\ntime, where GentleHumanoid maintains bounded and stable\nforces, while baselines exhibit increasing or unstable peaks.", "for healthcare and assistive scenarios where caregivers or\noperators remotely guide humanoid motions. We also develop an autonomous, shape-aware pipeline\nfor personalized hugging. The human s location and height\nare obtained using a motion-capture system with markers\nplaced on a hat, while an additional RGB camera mounted\non the G1 s head provides input for single-image human\nshape estimation, as shown in Figure 1(c).", "From this image,\nwe reconstruct a personalized body mesh using an existing\nhuman mesh estimation method [32] and scale it to the\nsubject s true height. Waist points are then extracted from\nthe mesh to optimize the humanoid s hugging motion by\naligning its hands with these target locations. This allows the\nG1 to adapt its hugging posture to individuals of different\nbody shapes in a fully autonomous manner.", "Experiments", "with participants of varying heights and builds show that the\npipeline generates stable and comfortable hugging motions. V. D ISCUSSION AND L IMITATIONS\nOur study shows that GentleHumanoid enables upper-body\ncompliance in humanoid robots.", "By integrating impedance\ncontrol into whole-body motion tracking and training with\na unified spring-based formulation, the policy generates\ncoordinated responses across multiple links and reduces peak\ncontact forces compared to baselines. Demonstrations in\nhugging, sit-to-stand assistance, and object handling highlight its ability to adapt compliance across diverse scenarios,\nunderscoring its potential for human-centered interaction. Several limitations remain.", "First, we use human motion\ndata to maintain kinematic consistency across links, but the\ndataset itself constrains the force distribution. For instance,\nforces applied to the shoulder are relatively small due to\nlimited variation in the recorded motions. Incorporating\nmore diverse motion datasets, such as dancing, could further\nimprove coverage.", "Second, our interaction modeling relies on\nsimulated spring forces, which provide structured coverage\nand kinematic consistency but do not fully capture the\ncomplexity of real human contact, such as frictional effects or\nthe viscoelastic properties of human tissue. Third, although\nthe safety-aware policy constrains interaction forces, realworld experiments reveal occasional overshoots of 1 3 N\ndue to sim-to-real discrepancies. Additional tactile sensing\nmay be necessary for more precise force regulation.", "Finally,\nhuman localization and height are currently obtained from\na motion capture system. Replacing this with a visionbased pipeline would improve autonomy and practicality,\nparticularly in long-horizon tasks. Future work will focus\non integrating richer sensing, combining general perception\nand reasoning systems such as vision language models,\nand extending evaluations to long-horizon interactions where\nthe humanoid must adapt its motion dynamically to human\npartners behaviors.", "VI. ACKNOWLEDGMENT\nWe would like to thank Haoyang Weng, Botian Xu,\nHaochen Shi, Sirui Chen, Ken Wang, Yanjie Ze, Joao Pedro Araujo, Yufei Ye and Takara Everest Truong for their\nvaluable discussions. We are also grateful to Yu Sun for\nassistance with motion capture from video and to Jiaxin Lu\nfor support with the motion dataset.", "We further thank the\nUnitree team for their timely and reliable hardware support. R EFERENCES\n[1] D. J.", "Agravante, A. Cherubini, A. Sherikov, P.", "-B. Wieber, and\nA. Kheddar, Human-humanoid collaborative carrying, IEEE Transactions on Robotics, vol.", "35, no. 4, pp. 833 846, 2019.", "2\n[2] Z. Fu, Q. Zhao, Q.", "Wu, G. Wetzstein, and C. Finn, Humanplus:\nHumanoid shadowing and imitation from humans, in Conference on\nRobot Learning (CoRL), 2024.", "2, 5\n[3] M. Ji, X. Peng, F.", "Liu, J. Li, G. Yang, X.", "Cheng, and X. Wang,\n Exbody2: Advanced expressive humanoid whole-body control, arXiv\npreprint arXiv:2412. 13196, 2024.", "2\n[4] Z. Chen, M. Ji, X.", "Cheng, X. Peng, X. B.", "Peng, and X. Wang,\n Gmt: General motion tracking for humanoid whole-body control, \narXiv:2506. 14770, 2025.", "2", "[5] T. He, J. Gao, W.", "Xiao, Y. Zhang, Z. Wang, J.", "Wang, Z. Luo, G. He,\nN.", "Sobanbab, C. Pan et al. , Asap: Aligning simulation and real-world\nphysics for learning agile humanoid whole-body skills, arXiv preprint\narXiv:2502.", "01143, 2025. 2\n[6] Q. Liao, T.", "E. Truong, X. Huang, G.", "Tevet, K. Sreenath, and C. K.", "Liu,\n Beyondmimic: From motion tracking to versatile humanoid control\nvia guided diffusion, 2025. 2\n[7] T. He, Z.", "Luo, X. He, W. Xiao, C.", "Zhang, W. Zhang, K. Kitani,\nC.", "Liu, and G. Shi, Omnih2o: Universal and dexterous humanto-humanoid whole-body teleoperation and learning, arXiv preprint\narXiv:2406. 08858, 2024.", "2\n[8] Y. Ze, Z. Chen, J.", "P. Arau jo, Z. ang Cao, X.", "B. Peng, J. Wu, and\nC.", "K. Liu, Twist: Teleoperated whole-body imitation system, arXiv\npreprint arXiv:2505. 02833, 2025.", "2, 5, 7\n[9] T. Portela, G. B.", "Margolis, Y. Ji, and P. Agrawal, Learning force control for legged manipulation, in 2024 IEEE International Conference\non Robotics and Automation (ICRA).", "IEEE, 2024, pp. 15 366 15 372. 2\n[10] B.", "Xu, H. Weng, Q. Lu, Y.", "Gao, and H. Xu, Facet: Force-adaptive\ncontrol via impedance reference tracking for legged robots, arXiv\npreprint arXiv:2505. 06883, 2025.", "2, 3, 5\n[11] P. Zhi, P. Li, J.", "Yin, B. Jia, and S. Huang, Learning unified force\nand position control for legged loco-manipulation, arXiv preprint\narXiv:2505.", "20829, 2025. 2\n[12] Y. Zhang, Y.", "Yuan, P. Gurunath, T. He, S.", "Omidshafiei, A. -a. Aghamohammadi, M.", "Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon:\nLearning force-adaptive humanoid loco-manipulation, arXiv preprint\narXiv:2505.", "06776, 2025. 2\n[13] M. Murooka, K.", "Chappellet, A. Tanguy, M. Benallegue, I.", "Kumagai, M. Morisawa, F. Kanehiro, and A.", "Kheddar, Humanoid locomanipulations pattern generation and stabilization control, IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp.", "5597 5604, 2021. 2\n[14] E. Dantec, R.", "Budhiraja, A. Roig, T. Lembono, G.", "Saurel, O. Stasse,\nP. Fernbach, S.", "Tonneau, S. Vijayakumar, S. Calinon et al.", ", Whole\nbody model predictive control with a memory of motion: Experiments\non a torque-controlled talos, in 2021 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2021, pp. 8202 8208.", "2\n[15] M. Sombolestan and Q. Nguyen, Adaptive force-based control of\ndynamic legged locomotion over uneven terrain, IEEE Transactions\non Robotics, 2024.", "2, 3\n[16] , Hierarchical adaptive loco-manipulation control for quadruped\nrobots, arXiv preprint arXiv:2209. 13145, 2022. 2\n[17] A.", "Rigo, M. Hu, S. K.", "Gupta, and Q. Nguyen, Hierarchical\noptimization-based control for whole-body loco-manipulation of heavy\nobjects, in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024, pp.", "15 322 15 328. 2\n[18] N. Fey, G.", "B. Margolis, M. Peticco, and P.", "Agrawal, Bridging\nthe sim-to-real gap for athletic loco-manipulation, arXiv preprint\narXiv:2502. 10894, 2025. 2\n[19] T.", "Mukai, S. Hirano, H. Nakashima, Y.", "Kato, Y. Sakaida, S. Guo, and\nS.", "Hosoe, Development of a nursing-care assistant robot riba that can\nlift a human in its arms, in 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2010, pp. 5996 6001. 3\n[20] A.", "E. Block, Huggiebot: An interactive hugging robot with visual\nand haptic perception, Ph. D.", "dissertation, ETH Zurich, 2021. 3\n[21] A. Bolotnikova, S.", "Courtois, and A. Kheddar, Adaptive task-space\nforce control for humanoid-to-human assistance, IEEE Robotics and\nAutomation Letters, vol. 6, no.", "3, pp. 5705 5712, 2021. 3\n[22] H.", "Lefe vre, T. Chaki, T. Kawakami, A.", "Tanguy, T. Yoshiike, and\nA. Kheddar, Humanoid-human sit-to-stand-to-sit assistance, IEEE\nRobotics and Automation Letters, 2024.", "3\n[23] Y. Sun, R. Chen, K.", "S. Yun, Y. Fang, S.", "Jung, F. Li, B. Li, W.", "Zhao,\nand C. Liu, Spark: A modular benchmark for humanoid robot safety, \narXiv preprint arXiv:2502. 03132, 2025.", "3\n[24] International Organization for Standardization, Robots and robotic\ndevices - collaborative robots, International Organization for Standardization, Tech. Rep. ISO/TS 15066:2016(E), 2016.", "5\n[25] J. Kim, A. Alspach, I.", "Leite, and K. Yamane, Study of children s\nhugging for interactive robot design, in 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2016, pp.", "557 561. 5\n[26] Y. Nam, S.", "Yang, J. Kim, B. Koo, S.", "Song, and Y. Kim, Quantification of comfort for the development of binding parts in a standing\nrehabilitation robot, Sensors, vol. 23, no.", "4, p. 2206, 2023. 5", "[27] J. Schulman, F. Wolski, P.", "Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms, CoRR,\nvol.", "abs/1707. 06347, 2017. 5\n[28] Y.", "Ze, J. P. Arau jo, J.", "Wu, and C. K. Liu, Gmr: General motion\nretargeting, 2025, gitHub repository.", "5\n[29] N. Mahmood, N. Ghorbani, N.", "F. Troje, G. Pons-Moll, and M.", "J. Black, AMASS: Archive of motion capture as surface shapes, in\nInternational Conference on Computer Vision, Oct. 2019, pp.", "5442 \n5451. 5\n[30] L. Xu, X.", "Lv, Y. Yan, Y. Jin, G.", "Wu, Y. Xu, L. Qiao, X.", "Zhu, J. Liu,\nR. Zhang et al.", ", Inter-x: Towards versatile human-human interaction\nanalysis, arXiv preprint arXiv:2312. 16051, 2023. 5\n[31] F.", "G. Harvey, M. Yurick, D.", "Nowrouzezahrai, and C. J. Pal, Robust\nmotion in-betweening, CoRR, vol.", "abs/2102. 04942, 2021. 5\n[32] M.", "J. Black, P. Patel, J.", "Tesch, and J. Yang, BEDLAM: A synthetic\ndataset of bodies exhibiting detailed lifelike animated motion, in\nProceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), Jun.", "2023, pp. 8726 8737. 7, 10\n[33] Y.", "Wang, Y. Sun, P. Patel, K.", "Daniilidis, M. J. Black, and M.", "Kocabas,\n Prompthmr: Promptable human mesh recovery, in Proceedings of\nthe Computer Vision and Pattern Recognition Conference, 2025, pp. 1148 1159. 11", "A PPENDIX", "B. Reference Dynamics Integration", "A. External Force Application Logic\nWe apply interaction forces at a subset of upper-body\nlinks (shoulders, wrists, hands). The procedure runs every\nsimulation step and consists of: (i) selecting which links are\ncurrently active and their interaction spring gains, (ii) updating an anchor (spring origin), (iii) computing interaction\nforces in the robot root frame and integrating the compliant\nreference, and (iv) applying forces/torques in the simulator.", "1) Activation and Gain Scheduling: An active link is a\nforce-application point that is enabled in the current interval;\nwe denote the active set by a binary mask m {0, 1}M over\nthe M candidate links. At the beginning of an interval we\nsample one of five modes (no-force, all-links, left-only, rightonly, or a random partial subset) to determine m. For every\nactive link we assign an interaction spring gain Kspring (t) that\nvaries smoothly over time (piecewise-linear in discrete steps).", "Gains may gently increase, hold, and then decrease back to\nzero at the end of the interval. In parallel, a force safety threshold τsafe (t) is adjusted\nsmoothly within a bounded range and later used for clamping\nand reward shaping. 2) Anchor (Interaction Spring Origin) Update: Each active link maintains an anchor o(t) in the robot root frame.", "We use two behaviors consistent with the two interaction\ntypes introduced: (1) Resistive contact: the anchor remains\nat its previously established location (relative to the root),\nmodeling a resisting load at the current contact site; (2)\nGuiding contact: the anchor is smoothly moved toward\na newly sampled surface point. In both cases the updates\nare smooth, avoiding discontinuities when the active set or\ntargets change. 3) One-Sided Projection: We model contact as one-sided:\ninteraction forces only act when the link compresses toward\nthe anchor along the intended direction of interaction; when\nthe link moves away (i.", "e. , leaves the contact side), the\ninteraction force drops to zero. Practically, we compute the\ndisplacement from the link to the anchor, take only its\ncomponent along the intended direction.", "This prevents nonphysical pull-back in free space and emulates real unilateral\ncontacts. 4) Application in the Simulator: Forces are applied in\nworld coordinates at the active links. To prevent excessive\noverall disturbance, we bound the net wrench about the torso:\nwe sum all per-link forces/torques, and if the totals exceed\npreset limits, we inject an opposite residual on the torso.", "TABLE II: External Force Application Parameters. Parameter", "Symbol", "Typical value / range", "Max per-link force cap\nSafety threshold (per link)\nNet force limit (about torso)\nNet torque limit (about torso)\nInteraction spring gain", "Fmax\nτsafe (t)\nτF\nτM\nKspring (t)", "30 N\n5 15 N (default 10 N)\n30 N\n20 N m\n5 250", "All reference quantities are expressed in the robot root\ntar\nframe. Let xt , x t be the current link state and xtar\nt , x t the\ntarget state. The reference dynamics used in this work are\ntar\nM x t = fdrive (xtar\nt , x t , xt , x t ) + finteract ( ) D x t . (8)", "The driving and interaction forces follow the definitions in\nthe method, and D x t is an additional damping term for\nstability. We integrate this system with explicit Euler using a\nsmall fixed number of substeps per simulator step (four substeps in our implementation), and clip acceleration/velocity\nat each step.\nTABLE III: Reference Dynamics and Integration Parameters.\nParameter", "Symbol", "Value", "Virtual mass\nIntegration damping\nTracking stiffness\nTracking damping\nTime step\nSubsteps per simulator step\nVelocity clip\nAcceleration clip", "M\nD\nKp\nKd\n t\nNsub\n x max\n x max", "0.1 kg\n2.0\nDerived\nfrom Kp = τsafe /0.05\np\n2 M Kp\nSame as simulation dt = 0.02s\n4\n4 m/s\n1000 m/s2", "C. Autonomous Hugging Pipeline\nFor a comfortable hugging experience, ensuring both\nsafety and an appropriate hugging position is essential. While our compliant RL policy enforces force limits for\nsafe contact, achieving comfort requires adapting the hugging\nposture to the person s body shape.", "To accomplish this, we\nfirst estimate the human body shape using BEDLAM [32],\nand rescale it according to the subject s absolute height\nobtained from motion capture. We then extract the waist\nposition, denoted as x , as the target contact point. Next, we optimize the default upper-body motion of G1 so\nthat selected robot links reach the SMPL-derived waist targets while the torso stays properly oriented in the horizontal\nplane.", "We optimize upper-body joint angles q and a planar\nfloating base r = (x, y, ψ) with fixed height z = z0 . Let\np (q, r) be the forward-kinematics position of link , {bk }\nthe target points on the waist, and Πxy the xy-projection. The objective is\nX\n2\nmin\nw k p (q, r) bk\nq, r", "( ,k) S", "2\n+ wt Πxy ptorso (q, r) + δ f (ψ) Πxy (bfront )\n+ λreg q q0 2 .\nHere S collects the link target pairs (e.g., hands to backwaist, elbows to opposite-side waist), w k and wt weight\ntheir relative importance, δ 5 cm is a small forward offset\nfor the torso, and f (ψ) = [cos ψ, sin ψ, 0] denotes the\nheading. The regularizer q q0 2 keeps the solution close to\na neutral upper-body pose. The optimized motion sequence\nis then updated as a personalized reference motion for the\nspecific individual.", "After obtaining the target posture and contact locations,\nthe robot must first stand in the proper place. We train a\nlocomotion policy that get the robot human relative pose\nfrom motion-capture markers and directly commands joint\ntargets to walk to a stance directly in front of the person, with\na 10 cm standoff and frontal alignment. Once this condition\nis met, control switches to the GentleHumanoid policy to\nexecute the hug.", "D. Video to Humanoid\nWe use a phone to record monocular RGB videos, and\napply PromptHMR [33] to estimate the corresponding human\nmotion as an SMPL-X motion sequence. The estimated\nmotion is then retargeted to the G1 humanoid using GMR.", "Finally, we execute the retargeted motion using our trained\npolicy. As shown in the supplementary video, our method\nremains robust and compliant even when the estimated\nreference motions are noisy (e. g.", ", with foot skating). It\nsuccessfully handles interactions with various objects such\nas pillows, balloons, and baskets of different sizes and\ndeformabilities."]}
