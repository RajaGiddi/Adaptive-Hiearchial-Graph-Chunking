{"method": "fixed", "num_chunks": 85, "avg_chunk_len": 794.2588235294118, "std_chunk_len": 49.67521643664657, "max_chunk_len": 800, "min_chunk_len": 339, "total_chars": 67512, "compression_ratio": 1.0004147410830666, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions\n\nSimulation\n\nZ\nC\n\nn\n\nio\n\nat\n\nl\nre\nor\n\nReal World\n\nSuccess rate - Sim\n\narXiv:2511.04665v1 [cs.RO] 6 Nov 2025\n\nKaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2\n\nSuccess rate - Real\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nFig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T", "-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.\nAbstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments", "with photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/\n\nI. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to re", "produce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.\n\nSimulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cyc", "les. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.\nWe identify two key factors for aligning simulation with\nreality: appearance and dynamics. On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints", ". Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom. To address this challenge,\n\nwe adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.\nWe integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface", "[26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.\nIn summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,", "using policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.\nII. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16,", "33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.\nB. Physical Digital Twins\nDigit", "al twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage\n\ndata from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems d", "irectly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.\nC. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim", "evaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.\nIII. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented b", "y the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAcco", "rdingly, the fidelity of simulation can be assessed along\n\nReal World\n\nSimulation\nRendering: 3D Gaussian Splatting\n\nDynamics: PhysTwin\n\nTask and\nscene info\nPositional alignment for robot and objects\n\nDemonstrations\n\nACT\n\nScene scans\n\nDi usion\n\nSmolVLA\n\nPi-0\n\nt\nHuman-object\ninteraction video\n\nOptimized softbody digital twin\n\nColor alignment with real cameras\n\nPolicy Training\nEvaluate\npolicy in real:\n\nEvaluate\npolicy in sim:\n\n Expensive\n Slow\n\n Cheap\n Scalable\n\nPerformance\ncorrelation\n\nenv.step()\n\nEvaluation\nplatform\n\nenv.render()\n\nConstructed Simulation Env\n\nff\n\nFig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trai", "ned on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.\n\ntwo axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we ad", "dress both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2.\nB. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording t", "o Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the", "objects\nthat interact with the robot (plush toy, rope, and T-block).\n\nC. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.\n1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The re", "sulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.\n2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75].", "We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link\n\nthrough a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.\n3) Color Alignment: A major contributor to the", "visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, formin", "g paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1\n\nf = arg min\n\nwhere IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi \n\n(2)\npdi ]T ,\n\n(3)\n\nwhich reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.\n4) Physics and", "Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics eng", "ine are propagated to the\nGaussian kernels. For rigid bodies, including objects and\n\nrobot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.\nOverall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.\nD. Policy Evaluation\nTo evaluate visuomoto", "r policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starti", "ng configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.\nIV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning p", "olicies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?\nA. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and\n\nToy packing\nr = 0.944\n\nRope routing\nr = 0.901\n\nT-block pushing\nr = 0.915\n\nOurs vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268\n\nFig. 3: Correlation", "between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.\n\nToy Packing - DP\n\nToy Packing - SmolVLA\n\nRope Routing - ACT\n\nRope Routing - Pi-0\n\nT-Block Pushing - DP\n\nT-Block Pushing - Pi-0\n\nFig. 4: Per-p", "olicy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.\n\nbody are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis", "placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require\n\nthe policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation.\n2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nra", "nges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy", "success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRo", "bot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.\n\nToy packing\nRope routing\nT-block pushing\n\nReal world\n\nOurs\n\nOurs - w/o phys. opt.\n\nOurs - w/o color align\n\nIsaacLab\n\nFig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics comp", "ared to our approach.\nToy packing\n\nB. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.\nC. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We obse", "rve a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.\nD. Policy Performance Analysis\nFigure 4 further illustrates per-policy,", "per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,\n\nIsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs\n\nRope routing\n\nT-block pushing\n\nMMRV \n\nr \n\nMMRV \n\nr \n\nMMRV \n\nr \n\n0.200\n0.200\n0.087\n\n0.805\n0.694\n0.944\n\n0.022\n0.156\n0.119\n0.096\n\n0.237\n0.714\n0.832\n0.901\n\n0.031\n0.031\n0.031\n0.000\n\n0.649\n0.529\n0.905\n0.915\n\nTABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer", "errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.\n\nin the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episo", "des (16 27 per checkpoint).\nE. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage\n\nin PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mis", "match in dynamics.\nFigure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T", "task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).\nTable I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points.\nV. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through ph", "ysics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.\nWe demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.\nIn future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nt", "he key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.\nACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This\n\narticle solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of", "the RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\nC. Chi et al., Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching wi", "thout in-the-wild robots, in RSS, 2024.\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\nA. Brohan et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\nT. L. Team et al., A careful examination of large behavior\nmodels for m", "ultitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\nNVIDIA, NVIDIA Isaac Sim, 2024.\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\nR. Tedrake, Drake: Model-based design and verification for\nrobotic", "s, 2019.\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs.RO].\n\n[22]\n\n[23]\n[24]\n\n[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]\n\n[32]\n\n[33]\n\n[34]\n[35]\n[36]\n[37]\n[38]\n\n[39]\n[40]\n[41]\n[42]\n[43]\n\nM. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 20", "24. arXiv: 2411.11839 [cs.RO].\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\nA. Padalkar e", "t al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp. 36 52,\nSep. 2015.\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering", ", vol. 15, no. 1, pp. 172 188,\n2018.\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatar", "s\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.\narXiv: 2310.17596 [cs.RO].\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].\n\n[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]\n\n[56]\n\n[57]\n[58]\n[59]\n[60]\n[61]\n\n[62]\n[63]\n[64]\n[65]\n\nY. R. Wang et al., Roboeval: Where robotic manipulation meets structured an", "d scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\nOpenAI et al., Solving rubik s cube with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\nB. Chen et al., Physgen3d: Crafting a miniature interac", "tive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2", "025. arXiv: 2505.\n22882 [cs.RO].\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-", "d\nvideos, in RSS, 2025.\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\nT. Zhang et al., Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26", "545 26 555.\n\n[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]\n\n[75]\n\n[76]\n\n[77]\n\n[78]\n[79]\n[80]\n\nX. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.\nJ. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n208", "13 [cs.RO].\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\nNiantic, Scaniverse, https://scaniverse.com/.\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some r", "obust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no. 3, 80 es, Jul.\n2007.\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.\n\nA PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nP", "latform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .\n\n10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\nAppendix II: Additional Results\nII-A\nScaling up Simula", "tion Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .\n\n13\n13\n13\n14\n\nA PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.\n2) Data Collection: We use GELLO for", "data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.\n\n(a) Training initial state distributions\n\n(b) Evaluation initial state distributions\n\nFi", "g. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName\n\nDynamics Type\n\n3D Representation\n\nxArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip\n\nArticulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed\n\nGS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh\n\nTABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.\n\n3) Task Definition: To evaluate the effectiveness of our\nsi", "mulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Ro", "uting: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized a", "s a\nyellow translucent mask overlaid on the fixed-camera images.\n\nRobot pose 2\n\nRobot pose 3\n\nRobot pose 4\n\nRobot pose 5\n\nSim before\nalignment\n\nReal\n(RealSense)\n\nRobot pose 1\n\nSim after\nalignment\n\n(a) Training initial state distributions\n\nFig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.\n\nt\n\nB. Simulation\n\nAlgorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring", "-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend\n\n1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process.\n2) Positional Alignment: To align the robot-scene Gaussian Splatting sca", "n with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.\n\n3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor spac", "e, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-\n\nRope PhysTwin training video\n\nPlush toy PhysTwin training video\n\nFig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the", "deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.\n\nThe initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.\n\nModel\n\nVisual\n\nState\n\nAction\n\nRelative?\n\nACT\nDP\nSmolVLA\nPi-0\n\nmean std\nmean std\nidentity\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nFalse\nFalse\nTrue\nTrue\n\nTABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales val", "ues to [ 1, 1].\nColor Transformations\n\nSpatial Transformations\n\nType\n\nRange\n\nType\n\nRange\n\nBrightness\nContrast\nSaturation\nHue\nSharpness\n\n(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)\n\nPerspective\nRotation\nCrop\n\n0.025\n[ 5 , 5 ]\n[10, 40] px\n\nTABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.\n\nTwin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T", "-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.\n5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\nC. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.\n2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consist", "ent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a\n\nModel\nACT\nDP\nSmolVLA\nPi-0\n\nVisual Res.\n\nState Dim.\n\nAction Dim.", "Tp\n\nTe\n\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\n\n8\n8\n8\n8\n\n8\n8\n8\n8\n\n50\n64\n50\n50\n\n50\n50\n50\n50\n\nTABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone\n\n#V-Params\n\n#P-Params\n\nLR\n\nBatch Size\n\n#Iters\n\nResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)\n\n18M\n18M\n350M\n260B\n\n34M\n245M\n100M\n300M\n\n1 10 5\n1 10 4\n1 10 4\n5 10 5\n\n512\n512\n128\n8\n\n7k\n7k\n20k\n30k\n\nTABLE VI: Training configuration. Model-specific hyperparameters us", "ed in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.\n\nchunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes.\n3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.\n4) Hyperparameters: A", "complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.\nD. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the object", "s to match the\nsimulated configuration.\n2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.\n3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.897\n\nr = 0.918\n\nr = 0.950\n\nMMRV=0.092\n\nMMRV=0.077\n\nMMRV=0.000\n\nFig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields", "stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)\n\nEpisodes\n\nx (cm)\n\ny (cm)\n\nθ (deg)\n\n20\n20\n27\n16\n\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}\n\nTABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.\n\nexperiments to scale up, automat", "ed success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring", "segments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0.002, the episode\nis considered successful.\nA PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world\n\nexperiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflecte", "d by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports th", "e resulting correlations between the scaled-up simulation metrics and real-world success rates.\nWe observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.\nB. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived", "images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.880\n\nr = 0.887\n\nr = 0.944\n\nMMRV=0.050\n\nMMRV=0.093\n\nMMRV=0.000\n\nFig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights t", "he degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing\n\nReplay +\nReplay \n\nRope routing\n\nGT +\n\nGT \n\n106\n25\n\n37\n132\n\nReplay +\nReplay \n\nT-block pushing\n\nGT +\n\nGT \n\n276\n24\n\n28\n77\n\nReplay +\nReplay \n\nGT +\n\nGT \n\n63\n17\n\n1\n111\n\nTABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.\n\nFigure 10 shows the resulting correlations, and Table VIII\nreports the per-episode", "replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajecto", "ry replay.\nC. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.\n\nT-block pushing (sim)\n\nT-block pushing (real)\n\nRope routing (sim)\n\nRope routing (real)\n\nToy packing (sim)\n\nToy packing (real)\n\nt\n\nFig. 11: Sim and real rollout trajectories.", "Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "sliding", "num_chunks": 168, "avg_chunk_len": 799.3511904761905, "std_chunk_len": 4.693130594096294, "max_chunk_len": 800, "min_chunk_len": 739, "total_chars": 134291, "compression_ratio": 0.5029376503265297, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions\n\nSimulation\n\nZ\nC\n\nn\n\nio\n\nat\n\nl\nre\nor\n\nReal World\n\nSuccess rate - Sim\n\narXiv:2511.04665v1 [cs.RO] 6 Nov 2025\n\nKaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2\n\nSuccess rate - Real\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nFig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T", "ope routing\n\nT-block pushing\n\nFig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.\nAbstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to r", "-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.\nAbstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments", "eproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed re", "with photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/\n\nI. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to re", "construction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/\n\nI. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.\n\nSimulation offers a scalable and systematic alternative", "produce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.\n\nSimulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cyc", "and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.\nWe identify two key factors for aligning simulation with\nreality: appearance and dynamics.", "les. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.\nWe identify two key factors for aligning simulation with\nreality: appearance and dynamics. On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints", "On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints. Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom.", ". Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom. To address this challenge,\n\nwe adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.\nWe integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface", "To address this challenge,\n\nwe adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.\nWe integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm", "[26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.\nIn summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,", "that rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.\nIn summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.\nII. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors", "using policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.\nII. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16,", ". Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful", "33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.\nB. Physical Digital Twins\nDigit", "soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.\nB. Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage\n\ndata from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-bas", "al twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage\n\ndata from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems d", "ed approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.\nC. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatt", "irectly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.\nC. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim", "ing (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.\nIII. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor", "evaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.\nIII. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented b", "policies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environ", "y the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAcco", "ment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAccordingly, the fidelity of simulation can be assessed along\n\nReal World\n\nSimulation\nRendering: 3D Gaussian Splatting\n\nDynamics: PhysTwin\n\nTask and\nscene info\nPositional alignment for robot and objects\n\nDemonstrations\n\nACT\n\nScene scans\n\nDi usion\n\nSmolVLA\n\nPi-0\n\nt\nHuman-object\ninteraction video\n\nOptimized softbody digital twin\n\nColor alignment with real cameras\n\nPolicy Training\nEvaluate\npolicy in real", "rdingly, the fidelity of simulation can be assessed along\n\nReal World\n\nSimulation\nRendering: 3D Gaussian Splatting\n\nDynamics: PhysTwin\n\nTask and\nscene info\nPositional alignment for robot and objects\n\nDemonstrations\n\nACT\n\nScene scans\n\nDi usion\n\nSmolVLA\n\nPi-0\n\nt\nHuman-object\ninteraction video\n\nOptimized softbody digital twin\n\nColor alignment with real cameras\n\nPolicy Training\nEvaluate\npolicy in real:\n\nEvaluate\npolicy in sim:\n\n Expensive\n Slow\n\n Cheap\n Scalable\n\nPerformance\ncorrelation\n\nenv.step()\n\nEvaluation\nplatform\n\nenv.render()\n\nConstructed Simulation Env\n\nff\n\nFig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trai", ":\n\nEvaluate\npolicy in sim:\n\n Expensive\n Slow\n\n Cheap\n Scalable\n\nPerformance\ncorrelation\n\nenv.step()\n\nEvaluation\nplatform\n\nenv.render()\n\nConstructed Simulation Env\n\nff\n\nFig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world be", "ned on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.\n\ntwo axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we ad", "havior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.\n\ntwo axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull fr", "dress both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2.\nB. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording t", "amework is shown in Figure 2.\nB. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with th", "o Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the", "e object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).\n\nC. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmo", "objects\nthat interact with the robot (plush toy, rope, and T-block).\n\nC. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.\n1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The re", "deling and deformation handling.\n1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.\n2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical", "sulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.\n2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75].", "object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link\n\nthrough a link segmentation process. After ICP alignment,\neach kernel is assigned to a lin", "We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link\n\nthrough a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.\n3) Color Alignment: A major contributor to the", "k by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.\n3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense", "visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, formin", ", which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1\n\nf = arg min\n\nwhere IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polyn", "g paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1\n\nf = arg min\n\nwhere IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi \n\n(2)\npdi ]T ,\n\n(3)\n\nwhich reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.\n4) Physics and", "omial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi \n\n(2)\npdi ]T ,\n\n(3)\n\nwhich reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.\n4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but un", "Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics eng", "realistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels. For rigid bodies, including objects and\n\nrobot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.\nOverall, with GS rendering, the physics s", "ine are propagated to the\nGaussian kernels. For rigid bodies, including objects and\n\nrobot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.\nOverall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.\nD. Policy Evaluation\nTo evaluate visuomoto", "olver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.\nD. Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency", "r policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starti", "and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different tra", "ng configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.\nIV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning p", "ining stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.\nIV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?\nA. Experiment Setup\n1) Tasks: We evaluate policies o", "olicies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?\nA. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and\n\nToy packing\nr = 0.944\n\nRope routing\nr = 0.901\n\nT-block pushing\nr = 0.915\n\nOurs vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268\n\nFig. 3: Correlation", "n three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and\n\nToy packing\nr = 0.944\n\nRope routing\nr = 0.901\n\nT-block pushing\nr = 0.915\n\nOurs vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268\n\nFig. 3: Correlation between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world", "between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.\n\nToy Packing - DP\n\nToy Packing - SmolVLA\n\nRope Routing - ACT\n\nRope Routing - Pi-0\n\nT-Block Pushing - DP\n\nT-Block Pushing - Pi-0\n\nFig. 4: Per-p", "behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.\n\nToy Packing - DP\n\nToy Packing - SmolVLA\n\nRope Routing - ACT\n\nRope Routing - Pi-0\n\nT-Block Pushing - DP\n\nT-Block Pushing - Pi-0\n\nFig. 4: Per-policy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positi", "olicy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.\n\nbody are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis", "ve correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.\n\nbody are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require\n\nthe policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achie", "placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require\n\nthe policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation.\n2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nra", "ve\nthe required translation and reorientation.\n2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simu", "nges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy", "lation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policie", "success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRo", "s\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.\n\nToy packing\nRope routing\nT-block pushing\n\nReal world\n\nOurs\n\nOurs - w/o phys. opt.\n\nOurs - w/o color align\n\nIsaacLab\n\nFig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three task", "bot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.\n\nToy packing\nRope routing\nT-block pushing\n\nReal world\n\nOurs\n\nOurs - w/o phys. opt.\n\nOurs - w/o color align\n\nIsaacLab\n\nFig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics comp", "s. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.\nToy packing\n\nB. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe appr", "ared to our approach.\nToy packing\n\nB. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.\nC. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We obse", "oximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.\nC. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coe", "rve a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.\nD. Policy Performance Analysis\nFigure 4 further illustrates per-policy,", "fficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.\nD. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,\n\nIsaacLab [13]\nOurs w/o color\nOurs w/", "per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,\n\nIsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs\n\nRope routing\n\nT-block pushing\n\nMMRV \n\nr \n\nMMRV \n\nr \n\nMMRV \n\nr \n\n0.200\n0.200\n0.087\n\n0.805\n0.694\n0.944\n\n0.022\n0.156\n0.119\n0.096\n\n0.237\n0.714\n0.832\n0.901\n\n0.031\n0.031\n0.031\n0.000\n\n0.649\n0.529\n0.905\n0.915\n\nTABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer", "o phys.\nOurs\n\nRope routing\n\nT-block pushing\n\nMMRV \n\nr \n\nMMRV \n\nr \n\nMMRV \n\nr \n\n0.200\n0.200\n0.087\n\n0.805\n0.694\n0.944\n\n0.022\n0.156\n0.119\n0.096\n\n0.237\n0.714\n0.832\n0.901\n\n0.031\n0.031\n0.031\n0.000\n\n0.649\n0.529\n0.905\n0.915\n\nTABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.\n\nin the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expect", "errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.\n\nin the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episo", "ations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint).\nE. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone ca", "des (16 27 per checkpoint).\nE. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage\n\nin PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mis", "mera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage\n\nin PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics.\nFigure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics,", "match in dynamics.\nFigure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T", "while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).\nTable I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate strong", "task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).\nTable I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points.\nV. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through ph", "er and more consistent correlations without being influenced by outlier points.\nV. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.\nWe demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis acr", "ysics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.\nWe demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.\nIn future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nt", "oss highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.\nIn future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.\nACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research Amer", "he key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.\nACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This\n\narticle solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of", "ica (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This\n\narticle solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\nC. Chi et al., Diffusion p", "the RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\nC. Chi et al., Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching wi", "olicy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024.\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\nA. Brohan et al., Rt-2: Vision-language-action models transfer web k", "thout in-the-wild robots, in RSS, 2024.\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\nA. Brohan et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\nT. L. Team et al., A careful examination of large behavior\nmodels for m", "nowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\nT. L. Team et al., A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\nNVIDIA, NVIDIA Isaac Sim, 2024.\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The", "ultitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\nNVIDIA, NVIDIA Isaac Sim, 2024.\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\nR. Tedrake, Drake: Model-based design and verification for\nrobotic", "IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\nR. Tedrake, Drake: Model-based design and verification for\nrobotics, 2019.\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs", "s, 2019.\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs.RO].\n\n[22]\n\n[23]\n[24]\n\n[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]\n\n[32]\n\n[33]\n\n[34]\n[35]\n[36]\n[37]\n[38]\n\n[39]\n[40]\n[41]\n[42]\n[43]\n\nM. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 20", ".RO].\n\n[22]\n\n[23]\n[24]\n\n[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]\n\n[32]\n\n[33]\n\n[34]\n[35]\n[36]\n[37]\n[38]\n\n[39]\n[40]\n[41]\n[42]\n[43]\n\nM. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024. arXiv: 2411.11839 [cs.RO].\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, I", "24. arXiv: 2411.11839 [cs.RO].\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\nA. Padalkar e", "CCV, 2025.\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\nA. Padalkar et al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol", "t al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp. 36 52,\nSep. 2015.\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering", ". 22, no. 3, pp. 36 52,\nSep. 2015.\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol. 15, no. 1, pp. 172 188,\n2018.\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\nS. James, Z. Ma, D. R. Arrojo", ", vol. 15, no. 1, pp. 172 188,\n2018.\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatar", ", and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.", "s\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.\narXiv: 2310.17596 [cs.RO].\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].\n\n[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]\n\n[56]\n\n[57]\n[58]\n[59]\n[60]\n[61]\n\n[62]\n[63]\n[64]\n[65]\n\nY. R. Wang et al., Roboeval: Where robotic manipulation meets structured an", "arXiv: 2310.17596 [cs.RO].\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].\n\n[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]\n\n[56]\n\n[57]\n[58]\n[59]\n[60]\n[61]\n\n[62]\n[63]\n[64]\n[65]\n\nY. R. Wang et al., Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\nOpenAI et al., Solving rubik s cub", "d scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\nOpenAI et al., Solving rubik s cube with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\nB. Chen et al., Physgen3d: Crafting a miniature interac", "e with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\nB. Chen et al., Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-b", "tive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2", "ased scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.\n22882 [cs.RO].\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\nT. Pf", "025. arXiv: 2505.\n22882 [cs.RO].\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-", "aff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025.\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\nT. Zhang et al., Physdreamer: Physics-ba", "d\nvideos, in RSS, 2025.\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\nT. Zhang et al., Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26", "sed interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26 545 26 555.\n\n[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]\n\n[75]\n\n[76]\n\n[77]\n\n[78]\n[79]\n[80]\n\nX. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.", "545 26 555.\n\n[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]\n\n[75]\n\n[76]\n\n[77]\n\n[78]\n[79]\n[80]\n\nX. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.\nJ. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n208", "J. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n20813 [cs.RO].\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\nNiantic, Scaniverse, https://scaniverse.com/.\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, I", "13 [cs.RO].\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\nNiantic, Scaniverse, https://scaniverse.com/.\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some r", "EEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vo", "obust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no. 3, 80 es, Jul.\n2007.\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.\n\nA PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nP", "l. 26, no. 3, 80 es, Jul.\n2007.\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.\n\nA PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI", "latform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .\n\n10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\nAppendix II: Additional Results\nII-A\nScaling up Simula", "-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .\n\n10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\nAppendix II: Additional Results\nII-A\nScaling up Simulation Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .\n\n13\n13\n13\n14\n\nA PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed", "tion Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .\n\n13\n13\n13\n14\n\nA PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.\n2) Data Collection: We use GELLO for", "pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.\n2) Data Collection: We use GELLO for data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized s", "data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.\n\n(a) Training initial state distributions\n\n(b) Evaluation initial state distributions\n\nFi", "uch that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.\n\n(a) Training initial state distributions\n\n(b) Evaluation initial state distributions\n\nFig. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName\n\nDynamics Type\n\n3D Representation\n\nxArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip\n\nArticulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed\n\nGS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwi", "g. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName\n\nDynamics Type\n\n3D Representation\n\nxArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip\n\nArticulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed\n\nGS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh\n\nTABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.\n\n3) Task Definition: To evaluate the effectiveness of our\nsi", "n\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh\n\nTABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.\n\n3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobo", "mulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Ro", "t then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total o", "uting: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized a", "f 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images.\n\nRobot pose 2\n\nRobot pose 3\n\nRobot pose 4\n\nRobot pose 5\n\nSim before\nalignment\n\nReal\n(RealSense)\n\nRobot pose 1\n\nSim after\nalignment\n\n(a) Training initial state distributions\n\nFig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs", "s a\nyellow translucent mask overlaid on the fixed-camera images.\n\nRobot pose 2\n\nRobot pose 3\n\nRobot pose 4\n\nRobot pose 5\n\nSim before\nalignment\n\nReal\n(RealSense)\n\nRobot pose 1\n\nSim after\nalignment\n\n(a) Training initial state distributions\n\nFig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.\n\nt\n\nB. Simulation\n\nAlgorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring", "distributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.\n\nt\n\nB. Simulation\n\nAlgorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx ,", "-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend\n\n1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process.\n2) Positional Alignment: To align the robot-scene Gaussian Splatting sca", "v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend\n\n1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process.\n2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly samp", "n with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.\n\n3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor spac", "led surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.\n\n3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.", "e, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-\n\nRope PhysTwin training video\n\nPlush toy PhysTwin training video\n\nFig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the", "Figure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-\n\nRope PhysTwin training video\n\nPlush toy PhysTwin training video\n\nFig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.\n\nThe initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.\n\nModel\n\nVisual\n\nState\n\nAction\n\nRelative?\n\nACT\nDP\nSmolVLA\nPi-0\n\nmean std\nmean st", "deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.\n\nThe initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.\n\nModel\n\nVisual\n\nState\n\nAction\n\nRelative?\n\nACT\nDP\nSmolVLA\nPi-0\n\nmean std\nmean std\nidentity\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nFalse\nFalse\nTrue\nTrue\n\nTABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales val", "d\nidentity\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nFalse\nFalse\nTrue\nTrue\n\nTABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1].\nColor Transformations\n\nSpatial Transformations\n\nType\n\nRange\n\nType\n\nRange\n\nBrightness\nContrast\nSaturation\nHue\nSharpness\n\n(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)\n\nPerspective\nRotation\nCrop\n\n0.025\n[ 5 , 5 ]\n[10, 40] px\n\nTABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to ima", "ues to [ 1, 1].\nColor Transformations\n\nSpatial Transformations\n\nType\n\nRange\n\nType\n\nRange\n\nBrightness\nContrast\nSaturation\nHue\nSharpness\n\n(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)\n\nPerspective\nRotation\nCrop\n\n0.025\n[ 5 , 5 ]\n[10, 40] px\n\nTABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.\n\nTwin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T", "ge intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.\n\nTwin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.\n5) Simulation Loop: The simulation loop, includ", "-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.\n5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\nC. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.\n2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consist", "ing robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\nC. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.\n2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actio", "ent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a\n\nModel\nACT\nDP\nSmolVLA\nPi-0\n\nVisual Res.\n\nState Dim.\n\nAction Dim.", "ns to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a\n\nModel\nACT\nDP\nSmolVLA\nPi-0\n\nVisual Res.\n\nState Dim.\n\nAction Dim.\n\nTp\n\nTe\n\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\n\n8\n8\n8\n8\n\n8\n8\n8\n8\n\n50\n64\n50\n50\n\n50\n50\n50\n50\n\nTABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed i", "Tp\n\nTe\n\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\n\n8\n8\n8\n8\n\n8\n8\n8\n8\n\n50\n64\n50\n50\n\n50\n50\n50\n50\n\nTABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone\n\n#V-Params\n\n#P-Params\n\nLR\n\nBatch Size\n\n#Iters\n\nResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)\n\n18M\n18M\n350M\n260B\n\n34M\n245M\n100M\n300M\n\n1 10 5\n1 10 4\n1 10 4\n5 10 5\n\n512\n512\n128\n8\n\n7k\n7k\n20k\n30k\n\nTABLE VI: Training configuration. Model-specific hyperparameters us", "n either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone\n\n#V-Params\n\n#P-Params\n\nLR\n\nBatch Size\n\n#Iters\n\nResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)\n\n18M\n18M\n350M\n260B\n\n34M\n245M\n100M\n300M\n\n1 10 5\n1 10 4\n1 10 4\n5 10 5\n\n512\n512\n128\n8\n\n7k\n7k\n20k\n30k\n\nTABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.\n\nchunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exh", "ed in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.\n\nchunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes.\n3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.\n4) Hyperparameters: A", "ibit larger\namplitudes.\n3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.\n4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.\nD. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies fr", "complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.\nD. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the object", "om\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration.\n2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.\n3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For si", "s to match the\nsimulated configuration.\n2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.\n3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.897\n\nr = 0.918\n\nr = 0.950\n\nMMRV=0.092\n\nMMRV=0.077\n\nMMRV=0.000\n\nFig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields", "mulated\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.897\n\nr = 0.918\n\nr = 0.950\n\nMMRV=0.092\n\nMMRV=0.077\n\nMMRV=0.000\n\nFig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)\n\nEpisode", "stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)\n\nEpisodes\n\nx (cm)\n\ny (cm)\n\nθ (deg)\n\n20\n20\n27\n16\n\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}\n\nTABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.\n\nexperiments to scale up, automat", "s\n\nx (cm)\n\ny (cm)\n\nθ (deg)\n\n20\n20\n27\n16\n\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}\n\nTABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.\n\nexperiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis", "ed success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring", "considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance", "segments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0.002, the episode\nis considered successful.\nA PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world\n\nexperiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflecte", "is less than 0.002, the episode\nis considered successful.\nA PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world\n\nexperiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x", "d by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports th", ", y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates.\nWe observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.\nB. Replay", "e resulting correlations between the scaled-up simulation metrics and real-world success rates.\nWe observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.\nB. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived", "ing Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.880\n\nr =", "images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.880\n\nr = 0.887\n\nr = 0.944\n\nMMRV=0.050\n\nMMRV=0.093\n\nMMRV=0.000\n\nFig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights t", "0.887\n\nr = 0.944\n\nMMRV=0.050\n\nMMRV=0.093\n\nMMRV=0.000\n\nFig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing\n\nReplay +\nReplay \n\nRope routing\n\nGT +\n\nGT \n\n106\n25\n\n37\n132\n\nReplay +\nReplay \n\nT-block pushing\n\nGT +\n\nGT \n\n276\n24\n\n28\n77\n\nReplay +\nReplay \n\nGT +\n\nGT \n\n63\n17\n\n1\n111\n\nTABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each", "he degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing\n\nReplay +\nReplay \n\nRope routing\n\nGT +\n\nGT \n\n106\n25\n\n37\n132\n\nReplay +\nReplay \n\nT-block pushing\n\nGT +\n\nGT \n\n276\n24\n\n28\n77\n\nReplay +\nReplay \n\nGT +\n\nGT \n\n63\n17\n\n1\n111\n\nTABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.\n\nFigure 10 shows the resulting correlations, and Table VIII\nreports the per-episode", "subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.\n\nFigure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction mod", "replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajecto", "els.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay.\nC. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observati", "ry replay.\nC. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.\n\nT-block pushing (sim)\n\nT-block pushing (real)\n\nRope routing (sim)\n\nRope routing (real)\n\nToy packing (sim)\n\nToy packing (real)\n\nt\n\nFig. 11: Sim and real rollout trajectories.", "ons. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.\n\nT-block pushing (sim)\n\nT-block pushing (real)\n\nRope routing (sim)\n\nRope routing (real)\n\nToy packing (sim)\n\nToy packing (real)\n\nt\n\nFig. 11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "sentence", "num_chunks": 376, "avg_chunk_len": 179.09308510638297, "std_chunk_len": 183.16730044465973, "max_chunk_len": 1074, "min_chunk_len": 5, "total_chars": 67339, "compression_ratio": 1.0029848973106223, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions\n\nSimulation\n\nZ\nC\n\nn\n\nio\n\nat\n\nl\nre\nor\n\nReal World\n\nSuccess rate - Sim\n\narXiv:2511. 04665v1 [cs. RO] 6 Nov 2025\n\nKaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2\n\nSuccess rate - Real\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nFig.", "1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance. Right: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings.", "Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment. Abstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions.", "We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies.", "Website: https:\n//real2sim-eval. github. io/\n\nI.", "I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.", "1 Columbia University\n2 SceniX Inc. 3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.", "Simulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles.", "This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably. We identify two key factors for aligning simulation with\nreality: appearance and dynamics.", "On the appearance side,\nrendered scenes must closely match real-world observations. This is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints.", "Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap. Dynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom.", "To address this challenge,\n\nwe adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics. We integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26].", "We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation. In summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.", "(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines. II.", "R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups.", "To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments. This is widely referred to as the sim-to-real gap [45 48].", "We aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics. B.", "Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage\n\ndata from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging.", "Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency.", "Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation. C. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics.", "Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.", "III. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data?", "In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1]. The objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies. To achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world.", "Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.", "g. , RGB images). Accordingly, the fidelity of simulation can be assessed along\n\nReal World\n\nSimulation\nRendering: 3D Gaussian Splatting\n\nDynamics: PhysTwin\n\nTask and\nscene info\nPositional alignment for robot and objects\n\nDemonstrations\n\nACT\n\nScene scans\n\nDi usion\n\nSmolVLA\n\nPi-0\n\nt\nHuman-object\ninteraction video\n\nOptimized softbody digital twin\n\nColor alignment with real cameras\n\nPolicy Training\nEvaluate\npolicy in real:\n\nEvaluate\npolicy in sim:\n\n Expensive\n Slow\n\n Cheap\n Scalable\n\nPerformance\ncorrelation\n\nenv.", "step()\n\nEvaluation\nplatform\n\nenv. render()\n\nConstructed Simulation Env\n\nff\n\nFig. 2: Proposed framework for real-to-sim policy evaluation.", "We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering.", "For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.", "two axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations. In this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations.", "The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2. B. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system.", "Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics. To capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y .", "The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).", "C. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.", "1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer.", "This reconstruction step is required\nonly once per task. 2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation.", "For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry. Because the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.", "To enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link\n\nthrough a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing.", "The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models. 3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.", "g. , Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.", "We perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization.", "We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1\n\nf = arg min\n\nwhere IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi \n\n(2)\npdi ]T ,\n\n(3)\n\nwhich reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers.", "Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting. 4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment.", "For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.", "At each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels. For rigid bodies, including objects and\n\nrobot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.", "Overall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution. D.", "Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.", "e. , no co-training between simulation and reality). To improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al.", "[71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations. Policy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states.", "In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.", "IV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate?", "(2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide? A.", "Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and\n\nToy packing\nr = 0. 944\n\nRope routing\nr = 0.", "901\n\nT-block pushing\nr = 0. 915\n\nOurs vs. Isaac baseline\nr1 = 0.", "904\nr2 = 0. 268\n\nFig. 3: Correlation between simulation and real-world policy performance.", "Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels.", "Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics. Toy Packing - DP\n\nToy Packing - SmolVLA\n\nRope Routing - ACT\n\nRope Routing - Pi-0\n\nT-Block Pushing - DP\n\nT-Block Pushing - Pi-0\n\nFig. 4: Per-policy, per-task performance across training.", "xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations. Unlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair.", "Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies. body are fully contained within the box, with no parts\nprotruding. Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip.", "Success is defined\nby the rope being fully threaded into the clip. T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.", "Both the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require\n\nthe policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation. 2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.", "These initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states.", "We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively. We use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).", "The number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.", "We evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state.", "For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79]. Training is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.", "Toy packing\nRope routing\nT-block pushing\n\nReal world\n\nOurs\n\nOurs - w/o phys. opt. Ours - w/o color align\n\nIsaacLab\n\nFig.", "5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve.", "Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.", "Toy packing\n\nB. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup.", "IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.", "C. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks.", "Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0. 9 for all policies.", "By\ncontrast, the baseline yields only r = 0. 649 on push-T, and an\neven lower r = 0. 237 on rope routing as a result of the larger\ndynamics gap.", "The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations. D. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations.", "We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,\n\nIsaacLab [13]\nOurs w/o color\nOurs w/o phys.", "Ours\n\nRope routing\n\nT-block pushing\n\nMMRV \n\nr \n\nMMRV \n\nr \n\nMMRV \n\nr \n\n0. 200\n0. 200\n0.", "087\n\n0. 805\n0. 694\n0.", "944\n\n0. 022\n0. 156\n0.", "119\n0. 096\n\n0. 237\n0.", "714\n0. 832\n0. 901\n\n0.", "031\n0. 031\n0. 031\n0.", "000\n\n0. 649\n0. 529\n0.", "905\n0. 915\n\nTABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment.", "Ours w/o phys. : our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation.", "Best results are highlighted in bold. in the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.", "In cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint). E.", "Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r. We provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance. Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs.", "The global\nvalue is given by the gradient-free optimization stage\n\nin PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics. Figure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states.", "Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes. In contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches. Empirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap.", "For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment). Table I details the quantitative results.", "Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points. V.", "C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering. We demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies.", "Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0. 9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.", "We found that both physics optimization and color alignment\nare critical for closing policy performance gaps. In future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.", "ACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This\n\narticle solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors. We would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc.", "for\nhelpful discussions and assistance throughout the project. R EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n\nT. Z.", "Zhao, V. Kumar, S. Levine, and C.", "Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304. 13705 [cs.", "RO]. C. Chi et al.", ", Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023. K. Black et al.", ", π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.", "LG]. M. Shukor et al.", ", Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 . 01844 [cs.", "LG]. C. Chi et al.", ", Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024. T. Lin, K.", "Sachdev, L. Fan, J. Malik, and Y.", "Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502. 20396, 2025. B.", "Tang et al. , Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.", "17110\n[cs. RO]. A.", "Brohan et al. , Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307. 15818, 2023.", "P. Intelligence et al. , π0.", "5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504. 16054\n[cs.", "LG]. NVIDIA et al. , GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar.", "2025. arXiv: 2503. 14734.", "T. L. Team et al.", ", A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025. arXiv:\n2507. 05331 [cs.", "RO]. G. R.", "Team et al. , Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.", "20020 [cs. RO]. NVIDIA, NVIDIA Isaac Sim, 2024.", "E. Todorov, T. Erez, and Y.", "Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033. F.", "Xiang et al. , SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.", "C. Li et al. , Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024.", "arXiv: 2403. 09227 [cs. RO].", "G. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.", "R. Tedrake, Drake: Model-based design and verification for\nrobotics, 2019. X.", "Li et al. , Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024. B.", "Kerbl, G. Kopanas, T. Leimku hler, and G.", "Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul.", "2023. J. Abou-Chakra et al.", ", Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504. 03597\n[cs.", "RO]. [22]\n\n[23]\n[24]\n\n[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]\n\n[32]\n\n[33]\n\n[34]\n[35]\n[36]\n[37]\n[38]\n\n[39]\n[40]\n[41]\n[42]\n[43]\n\nM. N.", "Qureshi, S. Garg, F. Yandun, D.", "Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024.", "arXiv:\n2409. 10161 [cs. RO].", "X. Li et al. , Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024.", "arXiv: 2411. 11839 [cs. RO].", "L. Barcellona, A. Zadaianchuk, D.", "Allegro, S. Papa, S. Ghidoni, and E.", "Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412. 14957 [cs.", "RO]. H. Jiang, H.", "-Y. Hsu, K. Zhang, H.", "-N. Yu, S. Wang, and Y.", "Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025. G. Brockman et al.", ", Openai gym, 2016. arXiv: 1606 . 01540 [cs.", "LG]. Octo Model Team et al. , Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.", "J. Wang, M. Leonard, K.", "Daniilidis, D. Jayaraman, and E. S.", "Hu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025. A. Padalkar et al.", ", Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310. 08864,\n2023. A.", "Khazatsky et al. , Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024. B.", "Calli, A. Walsman, A. Singh, S.", "Srinivasa, P. Abbeel,\nand A. M.", "Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp.", "36 52,\nSep. 2015. K.", "Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp.", "190 200. N. Correll et al.", ", Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol. 15, no. 1, pp.", "172 188,\n2018. G. Zhou et al.", ", Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306. 00942 [cs.", "RO]. S. Dasari et al.", ", Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203. 08098 [cs.", "RO]. S. Tao et al.", ", Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025. S. James, Z.", "Ma, D. R. Arrojo, and A.", "J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.", "12271 [cs. RO]. S.", "Srivastava et al. , Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D.", "Hsu, and G. Neumann,\nEds. , ser.", "PMLR, vol. 164, Aug. 2022, pp.", "477 490. X. Puig et al.", ", Habitat 3. 0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310.", "13724 [cs. HC]. S.", "Nasiriany et al. , Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024. Y.", "Zhu et al. , Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .", "12293 [cs. RO]. A.", "Mandlekar et al. , Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023. arXiv: 2310.", "17596 [cs. RO]. X.", "Yang, C. Eppner, J. Tremblay, D.", "Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025.", "arXiv: 2508 . 11117\n[cs. RO].", "[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]\n\n[56]\n\n[57]\n[58]\n[59]\n[60]\n[61]\n\n[62]\n[63]\n[64]\n[65]\n\nY. R. Wang et al.", ", Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507. 00435 [cs.", "RO]. X. B.", "Peng, M. Andrychowicz, W. Zaremba, and P.", "Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810. Y.", "Chebotar et al. , Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.", "OpenAI et al. , Solving rubik s cube with a robot hand, 2019. arXiv: 1910.", "07113 [cs. LG]. D.", "Ho, K. Rao, Z. Xu, E.", "Jang, M. Khansari, and Y. Bai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021.", "arXiv: 2011. 03148 [cs. RO].", "S. Liu, Z. Ren, S.", "Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.", "B. Chen et al. , Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp.", "6178 6189. Y. Jiang et al.", ", Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1. T.", "Xie et al. , Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.", "R. -Z. Qiu, G.", "Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024.", "arXiv: 2404. 01223 [cs. CV].", "B. Bianchini, M. Zhu, M.", "Sun, B. Jiang, C. J.", "Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.", "W. Yang, Z. Xie, X.", "Zhang, H. B. Amor, S.", "Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.", "22882 [cs. RO]. J.", "Abou-Chakra, K. Rana, F. Dayoub, and N.", "Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024. K. -T.", "Yu, M. Bauza, N. Fazeli, and A.", "Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.", "T. Pfaff, M. Fortunato, A.", "Sanchez-Gonzalez, and P. W. Battaglia, Learning mesh-based simulation with graph networks, 2021.", "arXiv: 2010. 03409 [cs. LG].", "K. Zhang, B. Li, K.", "Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024. K.", "Zhang, B. Li, K. Hauser, and Y.", "Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025. T. Tian, H.", "Li, B. Ai, X. Yuan, Z.", "Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503. 11999,\n2025.", "X. Li et al. , Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.", "05512, 2023. T. Zhang et al.", ", Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406. L.", "Zhong, H. -X. Yu, J.", "Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.", "C. Chen et al. , Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp.", "26 545 26 555. [66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]\n\n[75]\n\n[76]\n\n[77]\n\n[78]\n[79]\n[80]\n\nX. Han et al.", ", Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502. 08645, 2025. A.", "Escontrela et al. , Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510. 15352, 2025.", "J. Yu et al. , Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025.", "arXiv: 2505. 09601 [cs. RO].", "S. Yang et al. , Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.", "13175, 2025. G. Jiang et al.", ", Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510. 20813 [cs.", "RO]. H. Kress-Gazit et al.", ", Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409. 09491, 2024. Niantic, Scaniverse, https://scaniverse.", "com/. PlayCanvas and Snap Inc. , Supersplat, https : / /\ngithub.", "com/playcanvas/supersplat, [Computer\nsoftware], 2025. K. S.", "Arun, T. S. Huang, and S.", "D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no.", "5,\npp. 698 700, 1987. M.", "A. Fischler and R. C.", "Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno.", "6, pp. 381 395, Jun. 1981.", "P. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol.", "46, no. 2, pp. 149 170, 1984.", "M. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github . com/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar.", "2022. R. W.", "Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol.", "26, no. 3, 80 es, Jul. 2007.", "P. Wu, Y. Shentu, Z.", "Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.", "R. Cadene et al. , Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .", "com/huggingface/lerobot, 2024. A PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . .", ". . .", ". . .", ". . I-A.", "1\nRobot Setup . . .", ". . .", ". . I-A.", "2\nData Collection . . .", ". . .", "I-A. 3\nTask Definition . .", ". . .", ". I-B\nSimulation . .", ". . .", ". . .", ". . .", ". . .", ". I-B. 1\nAssets .", ". . .", ". . .", ". . .", ". I-B. 2\nPositional Alignment .", ". . I-B.", "3\nColor Alignment . . .", ". . .", "I-B. 4\nPhysTwin Training . .", ". . .", "I-B. 5\nSimulation Loop . .", ". . .", ". I-C\nPolicy Training . .", ". . .", ". . .", ". . .", ". I-C. 1\nDatasets .", ". . .", ". . .", ". . .", "I-C. 2\nNormalizations . .", ". . .", ". . I-C.", "3\nImage Augmentations . . .", "I-C. 4\nHyperparameters . .", ". . .", ". I-D\nEvaluation . .", ". . .", ". . .", ". . .", ". . .", ". I-D. 1\nEvaluation Protocol .", ". . .", "I-D. 2\nEpisode Settings . .", ". . .", ". I-D. 3\nSuccess Criteria .", ". . .", ". . 10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\nAppendix II: Additional Results\nII-A\nScaling up Simulation Evaluation .", ". . II-B\nReplaying Real Rollouts .", ". . .", ". . .", "II-C\nAdditional Qualitative Results . . .", ". 13\n13\n13\n14\n\nA PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop.", "The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip.", "To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras. 2) Data Collection: We use GELLO for data collection. GELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking.", "At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.", "During policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments. (a) Training initial state distributions\n\n(b) Evaluation initial state distributions\n\nFig. 6: Training and evaluation data distributions.", "Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set. Name\n\nDynamics Type\n\n3D Representation\n\nxArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip\n\nArticulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed\n\nGS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh\n\nTABLE II: Simulation assets.", "Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator. 3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback.", "The formulation and\nsetup of each task are described as follows. a) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box.", "Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task. b) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot.", "Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task. c) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion.", "The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images. Robot pose 2\n\nRobot pose 3\n\nRobot pose 4\n\nRobot pose 5\n\nSim before\nalignment\n\nReal\n(RealSense)\n\nRobot pose 1\n\nSim after\nalignment\n\n(a) Training initial state distributions\n\nFig.", "7: Color alignment. Five\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras.", "Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance. t\n\nB.", "Simulation\n\nAlgorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend\n\n1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process. 2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes.", "Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.", "3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization.", "To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations. Figure 7 visualizes the input images and the resulting color\nalignment.", "4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-\n\nRope PhysTwin training video\n\nPlush toy PhysTwin training video\n\nFig. 8: PhysTwin training videos.", "A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models. The initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.", "Model\n\nVisual\n\nState\n\nAction\n\nRelative? ACT\nDP\nSmolVLA\nPi-0\n\nmean std\nmean std\nidentity\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nFalse\nFalse\nTrue\nTrue\n\nTABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace.", "Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1]. Color Transformations\n\nSpatial Transformations\n\nType\n\nRange\n\nType\n\nRange\n\nBrightness\nContrast\nSaturation\nHue\nSharpness\n\n(0. 8, 1.", "2)\n(0. 8, 1. 2)\n(0.", "5, 1. 5)\n( 0. 05, 0.", "05)\n(0. 5, 1. 5)\n\nPerspective\nRotation\nCrop\n\n0.", "025\n[ 5 , 5 ]\n[10, 40] px\n\nTABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.", "Twin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8.", "For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0. 5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.", "5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1. C. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.", "2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].", "For the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a\n\nModel\nACT\nDP\nSmolVLA\nPi-0\n\nVisual Res.", "State Dim. Action Dim. Tp\n\nTe\n\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\n\n8\n8\n8\n8\n\n8\n8\n8\n8\n\n50\n64\n50\n50\n\n50\n50\n50\n50\n\nTABLE V: Observation and action spaces.", "Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.", "Vision Backbone\n\n#V-Params\n\n#P-Params\n\nLR\n\nBatch Size\n\n#Iters\n\nResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)\n\n18M\n18M\n350M\n260B\n\n34M\n245M\n100M\n300M\n\n1 10 5\n1 10 4\n1 10 4\n5 10 5\n\n512\n512\n128\n8\n\n7k\n7k\n20k\n30k\n\nTABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively.", "LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations. chunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes. 3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.", "For every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges. 4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.", "For VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets. D. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real.", "To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration. 2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.", "3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0. 897\n\nr = 0.", "918\n\nr = 0. 950\n\nMMRV=0. 092\n\nMMRV=0.", "077\n\nMMRV=0. 000\n\nFig. 9: Sim-and-real correlations from scaled-up simulation evaluations.", "Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0. 897.", "Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations. Task\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)\n\nEpisodes\n\nx (cm)\n\ny (cm)\n\nθ (deg)\n\n20\n20\n27\n16\n\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}\n\nTABLE VII: Task randomization ranges used for evaluation. For each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.", "experiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.", "3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050. b) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip.", "Within the final 100\nframes (3. 3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful. c) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles.", "Within\nthe final 100 frames (3. 3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0. 002, the episode\nis considered successful.", "A PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world\n\nexperiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals.", "To account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.", "5, 7. 5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.", "5, 7. 5] cm. To better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution.", "Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates. We observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances. B.", "Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands. This allows us to disentangle dynamic discrepancies from\nappearance gaps, i. e.", ", the difference in policy behaviors\nintroduced by differences in perceived images is eliminated. In total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.", "Toy packing\n\nRope routing\n\nT-block pushing\n\nr = 0. 880\n\nr = 0. 887\n\nr = 0.", "944\n\nMMRV=0. 050\n\nMMRV=0. 093\n\nMMRV=0.", "000\n\nFig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator.", "The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes. Toy packing\n\nReplay +\nReplay \n\nRope routing\n\nGT +\n\nGT \n\n106\n25\n\n37\n132\n\nReplay +\nReplay \n\nT-block pushing\n\nGT +\n\nGT \n\n276\n24\n\n28\n77\n\nReplay +\nReplay \n\nGT +\n\nGT \n\n63\n17\n\n1\n111\n\nTABLE VIII: Per-episode replay result.", "We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints.", "The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories. Figure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.", "Notably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models. For T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states. Overall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay.", "C. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.", "From the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance. T-block pushing (sim)\n\nT-block pushing (real)\n\nRope routing (sim)\n\nRope routing (real)\n\nToy packing (sim)\n\nToy packing (real)\n\nt\n\nFig.", "11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.", "g. , toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "paragraph", "num_chunks": 253, "avg_chunk_len": 264.901185770751, "std_chunk_len": 646.0726917631422, "max_chunk_len": 3286, "min_chunk_len": 1, "total_chars": 67020, "compression_ratio": 1.0077588779468816, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions", "Simulation", "Z\nC", "n", "io", "at", "l\nre\nor", "Real World", "Success rate - Sim", "arXiv:2511.04665v1 [cs.RO] 6 Nov 2025", "Kaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2", "Success rate - Real", "Toy packing", "Rope routing", "T-block pushing", "Fig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.\nAbstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/", "I. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.", "Simulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.\nWe identify two key factors for aligning simulation with\nreality: appearance and dynamics. On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints. Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom. To address this challenge,", "we adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.\nWe integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.\nIn summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.\nII. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.\nB. Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage", "data from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.\nC. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.\nIII. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAccordingly, the fidelity of simulation can be assessed along", "Real World", "Simulation\nRendering: 3D Gaussian Splatting", "Dynamics: PhysTwin", "Task and\nscene info\nPositional alignment for robot and objects", "Demonstrations", "ACT", "Scene scans", "Di usion", "SmolVLA", "Pi-0", "t\nHuman-object\ninteraction video", "Optimized softbody digital twin", "Color alignment with real cameras", "Policy Training\nEvaluate\npolicy in real:", "Evaluate\npolicy in sim:", "Expensive\n Slow", "Cheap\n Scalable", "Performance\ncorrelation", "env.step()", "Evaluation\nplatform", "env.render()", "Constructed Simulation Env", "ff", "Fig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.", "two axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2.\nB. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).", "C. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.\n1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.\n2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link", "through a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.\n3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1", "f = arg min", "where IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi", "(2)\npdi ]T ,", "(3)", "which reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.\n4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels. For rigid bodies, including objects and", "robot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.\nOverall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.\nD. Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.\nIV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?\nA. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and", "Toy packing\nr = 0.944", "Rope routing\nr = 0.901", "T-block pushing\nr = 0.915", "Ours vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268", "Fig. 3: Correlation between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.", "Toy Packing - DP", "Toy Packing - SmolVLA", "Rope Routing - ACT", "Rope Routing - Pi-0", "T-Block Pushing - DP", "T-Block Pushing - Pi-0", "Fig. 4: Per-policy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.", "body are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require", "the policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation.\n2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.", "Toy packing\nRope routing\nT-block pushing", "Real world", "Ours", "Ours - w/o phys. opt.", "Ours - w/o color align", "IsaacLab", "Fig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.\nToy packing", "B. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.\nC. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.\nD. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,", "IsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs", "Rope routing", "T-block pushing", "MMRV", "r", "MMRV", "r", "MMRV", "r", "0.200\n0.200\n0.087", "0.805\n0.694\n0.944", "0.022\n0.156\n0.119\n0.096", "0.237\n0.714\n0.832\n0.901", "0.031\n0.031\n0.031\n0.000", "0.649\n0.529\n0.905\n0.915", "TABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.", "in the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint).\nE. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage", "in PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics.\nFigure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).\nTable I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points.\nV. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.\nWe demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.\nIn future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.\nACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This", "article solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]", "T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\nC. Chi et al., Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024.\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\nA. Brohan et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\nT. L. Team et al., A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\nNVIDIA, NVIDIA Isaac Sim, 2024.\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\nR. Tedrake, Drake: Model-based design and verification for\nrobotics, 2019.\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs.RO].", "[22]", "[23]\n[24]", "[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]", "[32]", "[33]", "[34]\n[35]\n[36]\n[37]\n[38]", "[39]\n[40]\n[41]\n[42]\n[43]", "M. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024. arXiv: 2411.11839 [cs.RO].\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\nA. Padalkar et al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp. 36 52,\nSep. 2015.\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol. 15, no. 1, pp. 172 188,\n2018.\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.\narXiv: 2310.17596 [cs.RO].\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].", "[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]", "[56]", "[57]\n[58]\n[59]\n[60]\n[61]", "[62]\n[63]\n[64]\n[65]", "Y. R. Wang et al., Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\nOpenAI et al., Solving rubik s cube with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\nB. Chen et al., Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.\n22882 [cs.RO].\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025.\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\nT. Zhang et al., Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26 545 26 555.", "[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]", "[75]", "[76]", "[77]", "[78]\n[79]\n[80]", "X. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.\nJ. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n20813 [cs.RO].\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\nNiantic, Scaniverse, https://scaniverse.com/.\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no. 3, 80 es, Jul.\n2007.\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.", "A PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .", "10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12", "Appendix II: Additional Results\nII-A\nScaling up Simulation Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .", "13\n13\n13\n14", "A PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.\n2) Data Collection: We use GELLO for data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.", "(a) Training initial state distributions", "(b) Evaluation initial state distributions", "Fig. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName", "Dynamics Type", "3D Representation", "xArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip", "Articulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed", "GS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh", "TABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.", "3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images.", "Robot pose 2", "Robot pose 3", "Robot pose 4", "Robot pose 5", "Sim before\nalignment", "Real\n(RealSense)", "Robot pose 1", "Sim after\nalignment", "(a) Training initial state distributions", "Fig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.", "t", "B. Simulation", "Algorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend", "1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process.\n2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.", "3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-", "Rope PhysTwin training video", "Plush toy PhysTwin training video", "Fig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.", "The initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.", "Model", "Visual", "State", "Action", "Relative?", "ACT\nDP\nSmolVLA\nPi-0", "mean std\nmean std\nidentity\nmean std", "mean std\nmin max\nmean std\nmean std", "mean std\nmin max\nmean std\nmean std", "False\nFalse\nTrue\nTrue", "TABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1].\nColor Transformations", "Spatial Transformations", "Type", "Range", "Type", "Range", "Brightness\nContrast\nSaturation\nHue\nSharpness", "(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)", "Perspective\nRotation\nCrop", "0.025\n[ 5 , 5 ]\n[10, 40] px", "TABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.", "Twin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.\n5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\nC. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.\n2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a", "Model\nACT\nDP\nSmolVLA\nPi-0", "Visual Res.", "State Dim.", "Action Dim.", "Tp", "Te", "L: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240", "8\n8\n8\n8", "8\n8\n8\n8", "50\n64\n50\n50", "50\n50\n50\n50", "TABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone", "#V-Params", "#P-Params", "LR", "Batch Size", "#Iters", "ResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)", "18M\n18M\n350M\n260B", "34M\n245M\n100M\n300M", "1 10 5\n1 10 4\n1 10 4\n5 10 5", "512\n512\n128\n8", "7k\n7k\n20k\n30k", "TABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.", "chunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes.\n3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.\n4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.\nD. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration.\n2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.\n3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated", "Toy packing", "Rope routing", "T-block pushing", "r = 0.897", "r = 0.918", "r = 0.950", "MMRV=0.092", "MMRV=0.077", "MMRV=0.000", "Fig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)", "Episodes", "x (cm)", "y (cm)", "θ (deg)", "20\n20\n27\n16", "[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}", "TABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.", "experiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0.002, the episode\nis considered successful.\nA PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world", "experiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates.\nWe observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.\nB. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.", "Toy packing", "Rope routing", "T-block pushing", "r = 0.880", "r = 0.887", "r = 0.944", "MMRV=0.050", "MMRV=0.093", "MMRV=0.000", "Fig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing", "Replay +\nReplay", "Rope routing", "GT +", "GT", "106\n25", "37\n132", "Replay +\nReplay", "T-block pushing", "GT +", "GT", "276\n24", "28\n77", "Replay +\nReplay", "GT +", "GT", "63\n17", "1\n111", "TABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.", "Figure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay.\nC. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.", "T-block pushing (sim)", "T-block pushing (real)", "Rope routing (sim)", "Rope routing (real)", "Toy packing (sim)", "Toy packing (real)", "t", "Fig. 11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "recursive", "num_chunks": 502, "avg_chunk_len": 133.27888446215138, "std_chunk_len": 171.04093634195198, "max_chunk_len": 771, "min_chunk_len": 1, "total_chars": 66906, "compression_ratio": 1.0094759812273937, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions", "Simulation", "Z\nC", "n", "io", "at", "l\nre\nor", "Real World", "Success rate - Sim", "arXiv:2511.04665v1 [cs.RO] 6 Nov 2025", "Kaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2", "Success rate - Real", "Toy packing", "Rope routing", "T-block pushing", "Fig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.", "Right: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment. Abstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects.", "Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies.", "Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval. github.", "io/", "I. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.", "Simulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles.", "This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably. We identify two key factors for aligning simulation with\nreality: appearance and dynamics.", "On the appearance side,\nrendered scenes must closely match real-world observations. This is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints.", "Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap. Dynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom.", "To address this challenge,", "we adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics. We integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26].", "We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation. In summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.", "(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines. II.", "R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups.", "To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments. This is widely referred to as the sim-to-real gap [45 48].", "We aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics. B.", "Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage", "data from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings.", "Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.", "C. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56].", "Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator. III.", "M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].", "The objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies. To achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T .", "A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e. g.", ", RGB images). Accordingly, the fidelity of simulation can be assessed along", "Real World", "Simulation\nRendering: 3D Gaussian Splatting", "Dynamics: PhysTwin", "Task and\nscene info\nPositional alignment for robot and objects", "Demonstrations", "ACT", "Scene scans", "Di usion", "SmolVLA", "Pi-0", "t\nHuman-object\ninteraction video", "Optimized softbody digital twin", "Color alignment with real cameras", "Policy Training\nEvaluate\npolicy in real:", "Evaluate\npolicy in sim:", "Expensive\n Slow", "Cheap\n Scalable", "Performance\ncorrelation", "env.step()", "Evaluation\nplatform", "env.render()", "Constructed Simulation Env", "ff", "Fig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins.", "Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior.", "The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.", "two axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations. In this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations.", "The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2. B. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system.", "Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics. To capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y .", "The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).", "C. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.", "1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer.", "This reconstruction step is required\nonly once per task. 2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation.", "For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry. Because the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.", "To enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link", "through a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing.", "The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models. 3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.", "g. , Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.", "We perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization.", "We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1", "f = arg min", "where IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi", "(2)\npdi ]T ,", "(3)", "which reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.", "4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper.", "Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps. At each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels.", "For rigid bodies, including objects and", "robot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation. Overall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states.", "By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution. D. Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training.", "Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i. e.", ", no co-training between simulation and reality). To improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world.", "In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations. Policy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network.", "Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation. IV.", "E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation?", "(3) What practical benefits can the correlation provide? A. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box.", "A trial is\nconsidered successful only if the toy s arms, legs, and", "Toy packing\nr = 0.944", "Rope routing\nr = 0.901", "T-block pushing\nr = 0.915", "Ours vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268", "Fig. 3: Correlation between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.", "Toy Packing - DP", "Toy Packing - SmolVLA", "Rope Routing - ACT", "Rope Routing - Pi-0", "T-Block Pushing - DP", "T-Block Pushing - Pi-0", "Fig. 4: Per-policy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.", "body are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require", "the policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation. 2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.", "These initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states.", "We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively. We use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).", "The number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.", "We evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state.", "For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79]. Training is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.", "Toy packing\nRope routing\nT-block pushing", "Real world", "Ours", "Ours - w/o phys. opt.", "Ours - w/o color align", "IsaacLab", "Fig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.\nToy packing", "B. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup.", "IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.", "C. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks.", "Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0. 9 for all policies.", "By\ncontrast, the baseline yields only r = 0. 649 on push-T, and an\neven lower r = 0. 237 on rope routing as a result of the larger\ndynamics gap.", "The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations. D. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations.", "We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,", "IsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs", "Rope routing", "T-block pushing", "MMRV", "r", "MMRV", "r", "MMRV", "r", "0.200\n0.200\n0.087", "0.805\n0.694\n0.944", "0.022\n0.156\n0.119\n0.096", "0.237\n0.714\n0.832\n0.901", "0.031\n0.031\n0.031\n0.000", "0.649\n0.529\n0.905\n0.915", "TABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.", "in the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance. In cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ.", "We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint). E. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.", "We provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance. Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage", "in PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics. Figure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states.", "Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes. In contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches. Empirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap.", "For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment). Table I details the quantitative results.", "Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points. V.", "C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering. We demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies.", "Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0. 9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.", "We found that both physics optimization and color alignment\nare critical for closing policy performance gaps. In future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.", "ACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This", "article solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]", "T. Z. Zhao, V.", "Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023.", "arXiv: 2304. 13705 [cs. RO].", "C. Chi et al. , Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.", "K. Black et al. , π0 : A vision-language-action flow model\nfor general robot control, 2024.", "arXiv: 2410 . 24164\n[cs. LG].", "M. Shukor et al. , Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025.", "arXiv: 2506 . 01844 [cs. LG].", "C. Chi et al. , Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024.", "T. Lin, K. Sachdev, L.", "Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.", "20396, 2025. B. Tang et al.", ", Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305. 17110\n[cs.", "RO]. A. Brohan et al.", ", Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307. 15818, 2023. P.", "Intelligence et al. , π0. 5 : A vision-language-action model\nwith open-world generalization, 2025.", "arXiv: 2504. 16054\n[cs. LG].", "NVIDIA et al. , GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.", "arXiv: 2503. 14734. T.", "L. Team et al. , A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025.", "arXiv:\n2507. 05331 [cs. RO].", "G. R. Team et al.", ", Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503. 20020 [cs.", "RO]. NVIDIA, NVIDIA Isaac Sim, 2024. E.", "Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp.", "5026 \n5033. F. Xiang et al.", ", SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020. C.", "Li et al. , Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.", "09227 [cs. RO]. G.", "Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024. R.", "Tedrake, Drake: Model-based design and verification for\nrobotics, 2019. X. Li et al.", ", Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024. B. Kerbl, G.", "Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol.", "42, no. 4, Jul. 2023.", "J. Abou-Chakra et al. , Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025.", "arXiv: 2504. 03597\n[cs. RO].", "[22]", "[23]\n[24]", "[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]", "[32]", "[33]", "[34]\n[35]\n[36]\n[37]\n[38]", "[39]\n[40]\n[41]\n[42]\n[43]", "M. N. Qureshi, S.", "Garg, F. Yandun, D. Held, G.", "Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.", "10161 [cs. RO]. X.", "Li et al. , Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024. arXiv: 2411.", "11839 [cs. RO]. L.", "Barcellona, A. Zadaianchuk, D. Allegro, S.", "Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025.", "arXiv: 2412. 14957 [cs. RO].", "H. Jiang, H. -Y.", "Hsu, K. Zhang, H. -N.", "Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.", "G. Brockman et al. , Openai gym, 2016.", "arXiv: 1606 . 01540 [cs. LG].", "Octo Model Team et al. , Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024. J.", "Wang, M. Leonard, K. Daniilidis, D.", "Jayaraman, and E. S. Hu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.", "A. Padalkar et al. , Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.", "08864,\n2023. A. Khazatsky et al.", ", Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024. B. Calli, A.", "Walsman, A. Singh, S. Srinivasa, P.", "Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol.", "22, no. 3, pp. 36 52,\nSep.", "2015. K. Van Wyk, J.", "Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.", "N. Correll et al. , Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol.", "15, no. 1, pp. 172 188,\n2018.", "G. Zhou et al. , Train offline, test online: A real robot learning\nbenchmark, 2023.", "arXiv: 2306. 00942 [cs. RO].", "S. Dasari et al. , Rb2: Robotic manipulation benchmarking\nwith a twist, 2022.", "arXiv: 2203. 08098 [cs. RO].", "S. Tao et al. , Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.", "S. James, Z. Ma, D.", "R. Arrojo, and A. J.", "Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909. 12271 [cs.", "RO]. S. Srivastava et al.", ", Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G.", "Neumann,\nEds. , ser. PMLR, vol.", "164, Aug. 2022, pp. 477 490.", "X. Puig et al. , Habitat 3.", "0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310. 13724 [cs.", "HC]. S. Nasiriany et al.", ", Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024. Y. Zhu et al.", ", Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 . 12293 [cs.", "RO]. A. Mandlekar et al.", ", Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023. arXiv: 2310. 17596 [cs.", "RO]. X. Yang, C.", "Eppner, J. Tremblay, D. Fox, S.", "Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 .", "11117\n[cs. RO].", "[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]", "[56]", "[57]\n[58]\n[59]\n[60]\n[61]", "[62]\n[63]\n[64]\n[65]", "Y. R. Wang et al.", ", Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507. 00435 [cs.", "RO]. X. B.", "Peng, M. Andrychowicz, W. Zaremba, and P.", "Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810. Y.", "Chebotar et al. , Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.", "OpenAI et al. , Solving rubik s cube with a robot hand, 2019. arXiv: 1910.", "07113 [cs. LG]. D.", "Ho, K. Rao, Z. Xu, E.", "Jang, M. Khansari, and Y. Bai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021.", "arXiv: 2011. 03148 [cs. RO].", "S. Liu, Z. Ren, S.", "Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.", "B. Chen et al. , Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp.", "6178 6189. Y. Jiang et al.", ", Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1. T.", "Xie et al. , Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.", "R. -Z. Qiu, G.", "Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024.", "arXiv: 2404. 01223 [cs. CV].", "B. Bianchini, M. Zhu, M.", "Sun, B. Jiang, C. J.", "Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.", "W. Yang, Z. Xie, X.", "Zhang, H. B. Amor, S.", "Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.", "22882 [cs. RO]. J.", "Abou-Chakra, K. Rana, F. Dayoub, and N.", "Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024. K. -T.", "Yu, M. Bauza, N. Fazeli, and A.", "Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.", "T. Pfaff, M. Fortunato, A.", "Sanchez-Gonzalez, and P. W. Battaglia, Learning mesh-based simulation with graph networks, 2021.", "arXiv: 2010. 03409 [cs. LG].", "K. Zhang, B. Li, K.", "Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024. K.", "Zhang, B. Li, K. Hauser, and Y.", "Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025. T. Tian, H.", "Li, B. Ai, X. Yuan, Z.", "Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503. 11999,\n2025.", "X. Li et al. , Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.", "05512, 2023. T. Zhang et al.", ", Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406. L.", "Zhong, H. -X. Yu, J.", "Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.", "C. Chen et al. , Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp.", "26 545 26 555.", "[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]", "[75]", "[76]", "[77]", "[78]\n[79]\n[80]", "X. Han et al. , Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.", "08645, 2025. A. Escontrela et al.", ", Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510. 15352, 2025. J.", "Yu et al. , Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.", "09601 [cs. RO]. S.", "Yang et al. , Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504. 13175, 2025.", "G. Jiang et al. , Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025.", "arXiv: 2510. 20813 [cs. RO].", "H. Kress-Gazit et al. , Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.", "09491, 2024. Niantic, Scaniverse, https://scaniverse. com/.", "PlayCanvas and Snap Inc. , Supersplat, https : / /\ngithub. com/playcanvas/supersplat, [Computer\nsoftware], 2025.", "K. S. Arun, T.", "S. Huang, and S. D.", "Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp.", "698 700, 1987. M. A.", "Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun.", "ACM, vol. 24,\nno. 6, pp.", "381 395, Jun. 1981. P.", "J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no.", "2, pp. 149 170, 1984. M.", "Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github . com/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.", "R. W. Sumner, J.", "Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no.", "3, 80 es, Jul. 2007. P.", "Wu, Y. Shentu, Z. Yi, X.", "Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024. R.", "Cadene et al. , Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github . com/huggingface/lerobot, 2024.", "A PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .", "10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12", "Appendix II: Additional Results\nII-A\nScaling up Simulation Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .", "13\n13\n13\n14", "A PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom.", "The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.", "2) Data Collection: We use GELLO for data collection. GELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta.", "To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions. During policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.", "(a) Training initial state distributions", "(b) Evaluation initial state distributions", "Fig. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName", "Dynamics Type", "3D Representation", "xArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip", "Articulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed", "GS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh", "TABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.", "3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows. a) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box.", "The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.", "b) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.", "c) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images.", "Robot pose 2", "Robot pose 3", "Robot pose 4", "Robot pose 5", "Sim before\nalignment", "Real\n(RealSense)", "Robot pose 1", "Sim after\nalignment", "(a) Training initial state distributions", "Fig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.", "t", "B. Simulation", "Algorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend", "1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process. 2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes.", "Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.", "3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-", "Rope PhysTwin training video", "Plush toy PhysTwin training video", "Fig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.", "The initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.", "Model", "Visual", "State", "Action", "Relative?", "ACT\nDP\nSmolVLA\nPi-0", "mean std\nmean std\nidentity\nmean std", "mean std\nmin max\nmean std\nmean std", "mean std\nmin max\nmean std\nmean std", "False\nFalse\nTrue\nTrue", "TABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1].\nColor Transformations", "Spatial Transformations", "Type", "Range", "Type", "Range", "Brightness\nContrast\nSaturation\nHue\nSharpness", "(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)", "Perspective\nRotation\nCrop", "0.025\n[ 5 , 5 ]\n[10, 40] px", "TABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.", "Twin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8.", "For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0. 5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.", "5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1. C. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.", "2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].", "For the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a", "Model\nACT\nDP\nSmolVLA\nPi-0", "Visual Res.", "State Dim.", "Action Dim.", "Tp", "Te", "L: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240", "8\n8\n8\n8", "8\n8\n8\n8", "50\n64\n50\n50", "50\n50\n50\n50", "TABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone", "#V-Params", "#P-Params", "LR", "Batch Size", "#Iters", "ResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)", "18M\n18M\n350M\n260B", "34M\n245M\n100M\n300M", "1 10 5\n1 10 4\n1 10 4\n5 10 5", "512\n512\n128\n8", "7k\n7k\n20k\n30k", "TABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.", "chunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes. 3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training. For every image in a training batch, three augmentation\noperations are randomly sampled and composed.", "Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges. 4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI. For VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.", "D. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup.", "Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration. 2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII. 3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias.", "For simulated", "Toy packing", "Rope routing", "T-block pushing", "r = 0.897", "r = 0.918", "r = 0.950", "MMRV=0.092", "MMRV=0.077", "MMRV=0.000", "Fig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)", "Episodes", "x (cm)", "y (cm)", "θ (deg)", "20\n20\n27\n16", "[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}", "TABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.", "experiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.", "3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050. b) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip.", "Within the final 100\nframes (3. 3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful. c) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles.", "Within\nthe final 100 frames (3. 3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0. 002, the episode\nis considered successful.", "A PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world", "experiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals. To account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper.", "In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7. 5, 7.", "5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7. 5, 7.", "5] cm. To better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates.", "We observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances. B. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.", "This allows us to disentangle dynamic discrepancies from\nappearance gaps, i. e. , the difference in policy behaviors\nintroduced by differences in perceived images is eliminated.", "In total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.", "Toy packing", "Rope routing", "T-block pushing", "r = 0.880", "r = 0.887", "r = 0.944", "MMRV=0.050", "MMRV=0.093", "MMRV=0.000", "Fig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing", "Replay +\nReplay", "Rope routing", "GT +", "GT", "106\n25", "37\n132", "Replay +\nReplay", "T-block pushing", "GT +", "GT", "276\n24", "28\n77", "Replay +\nReplay", "GT +", "GT", "63\n17", "1\n111", "TABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.", "Figure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes. Notably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.", "For T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states. Overall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay. C.", "Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views. From the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations.", "Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.", "T-block pushing (sim)", "T-block pushing (real)", "Rope routing (sim)", "Rope routing (real)", "Toy packing (sim)", "Toy packing (real)", "t", "Fig. 11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "semantic", "num_chunks": 74, "avg_chunk_len": 907.9324324324324, "std_chunk_len": 1340.8970911103272, "max_chunk_len": 11796, "min_chunk_len": 108, "total_chars": 67187, "compression_ratio": 1.0052539925878519, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions\n\nKaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2\n\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.", "Simulation\n\nZ\nC\n\nn\nio\nat\nl\nre\nor\n\nReal World\n\nSuccess rate - Sim\n\narXiv:2511.04665v1 [cs.RO] 6 Nov 2025\n\nSuccess rate - Real\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nFig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.", "Abstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/", "I. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.", "Simulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.", "We identify two key factors for aligning simulation with\nreality: appearance and dynamics. On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints. Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom. To address this challenge,", "we adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.", "We integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.", "In summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.", "II. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.", "B. Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage\ndata from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.", "C. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.", "III. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAccordingly, the fidelity of simulation can be assessed along\ntwo axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2.", "B. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).", "C. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.", "Real World\n\nSimulation\nRendering: 3D Gaussian Splatting\n\nDynamics: PhysTwin\n\nTask and\nscene info\nPositional alignment for robot and objects\n\nDemonstrations\n\nACT\n\nScene scans\n\nDi usion\n\nSmolVLA\n\nPi-0\n\nt\nHuman-object\ninteraction video\n\nOptimized softbody digital twin\n\nColor alignment with real cameras\n\nPolicy Training\nEvaluate\npolicy in real:\n\nEvaluate\npolicy in sim:\n\n Expensive\n Slow\n\n Cheap\n Scalable\n\nPerformance\ncorrelation\n\nenv.step()\n\nEvaluation\nplatform\n\nenv.render()\n\nConstructed Simulation Env\n\nff\n\nFig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.", "1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.", "2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link\nthrough a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.", "3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf = arg min\nF N i=1\nwhere IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi \n\n(2)\npdi ]T ,\n\n(3)\nwhich reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.", "4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels. For rigid bodies, including objects and\nrobot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.", "Overall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.", "D. Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.", "IV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?", "A. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and\nbody are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require\nthe policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation.", "2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.", "B. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.", "C. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.", "Toy packing\nr = 0.944\n\nRope routing\nr = 0.901\n\nT-block pushing\nr = 0.915\n\nOurs vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268\n\nFig. 3: Correlation between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.", "Toy packing\nRope routing\nT-block pushing\n\nIsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs\n\nMMRV\n\n0.200\n0.200\n0.087\n\nr\n\n0.805\n0.694\n0.944\n\nMMRV\n\n0.022\n0.156\n0.119\n0.096\n\nr\n\n0.237\n0.714\n0.832\n0.901\n\nMMRV\n\n0.031\n0.031\n0.031\n0.000\n\nr\n\n0.649\n0.529\n0.905\n0.915\n\nTABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.", "D. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,\nin the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint).", "Toy Packing - DP\n\nToy Packing - SmolVLA\n\nRope Routing - ACT\n\nRope Routing - Pi-0\n\nT-Block Pushing - DP\n\nT-Block Pushing - Pi-0\n\nFig. 4: Per-policy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.", "E. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage\nin PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics.", "Figure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).", "Real world\n\nOurs\n\nOurs - w/o phys. opt.\n\nOurs - w/o color align\n\nIsaacLab\n\nFig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.", "Table I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points.", "V. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.", "We demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.", "In future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.", "ACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This\narticle solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.", "R EFERENCES\n[1]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\n[2]\nC. Chi et al., Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\n[3]\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\n[4]\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\n[5]\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024.\n[6]\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\n[7]\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\n[8]\nA. Brohan et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\n[9]\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\n[10]\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\n[11]\nT. L. Team et al., A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\n[12]\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\n[13]\nNVIDIA, NVIDIA Isaac Sim, 2024.\n[14]\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\n[15]\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\n[16]\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\n[17]\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\n[18]\nR. Tedrake, Drake: Model-based design and verification for\nrobotics, 2019.\n[19]\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\n[20]\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngausssian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\n[21]\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs.RO].\n[22]\nM. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\n[23]\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024. arXiv: 2411.11839 [cs.RO].\n[24]\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\n[25]\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.\n[26]\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\n[27]\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\n[28]\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\n[29]\nA. Padalkar et al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\n[30]\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\n[31]\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp. 36 52,\nSep. 2015.\n[32]\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\n[33]\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol. 15, no. 1, pp. 172 188,\n2018.\n[34]\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\n[35]\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\n[36]\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\n[37]\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\n[38]\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\n[39]\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\n[40]\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\n[41]\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\n[42]\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.\narXiv: 2310.17596 [cs.RO].\n[43]\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].\n[44]\nY. R. Wang et al., Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\n[45]\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\n[46]\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\n[47]\nOpenAI et al., Solving rubik s cube with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\n[48]\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\n[49]\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\n[50]\nB. Chen et al., Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\n[51]\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\n[52]\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\n[53]\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\n[54]\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\n[55]\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.\n22882 [cs.RO].\n[56]\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\n[57]\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\n[58]\nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\n[59]\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\n[60]\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025.\n[61]\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\n[62]\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\n[63]\nT. Zhang et al., Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\n[64]\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\n[65]\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26 545 26 555.\n[66]\nX. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\n[67]\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.\n[68]\nJ. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\n[69]\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\n[70]\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n20813 [cs.RO].\n[71]\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\n[72]\nNiantic, Scaniverse, https://scaniverse.com/.\n[73]\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\n[74]\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\n[75]\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\n[76]\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\n[77]\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\n[78]\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no. 3, 80 es, Jul.\n2007.\n[79]\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\n[80]\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.", "A PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .\n\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\nAppendix II: Additional Results\nII-A\nScaling up Simulation Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . .\nII-C\nAdditional Qualitative Results . . . .\n\n13\n13\n13\n14", "A PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.", "2) Data Collection: We use GELLO for data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.", "3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images.\nThe initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.", "(a) Training initial state distributions\n\n(b) Evaluation initial state distributions\n\nFig. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.", "B. Simulation\n1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II.", "Name\n\nxArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip\n\nDynamics Type\n\nArticulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed\n\n3D Representation\n\nGS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh\n\nTABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.", "2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.", "3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.", "Robot pose 2\n\nRobot pose 3\n\nRobot pose 4\n\nRobot pose 5\n\nSim before\nalignment\n\nReal\n(RealSense)\n\nRobot pose 1\n\nSim after\nalignment\n\n(a) Training initial state distributions\n\nFig. 7: Color alignment. Five image pairs used for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation initial state distributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to the real images. Bottom: GS renderings after\napplying the optimized color transformation, showing improved consistency with real-world color appearance.", "4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. PhysTwin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.", "Rope PhysTwin training video\n\nPlush toy PhysTwin training video\n\nFig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.", "5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\n\nAlgorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend", "C. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.", "2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a\nchunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes.", "Model\n\nACT\nDP\nSmolVLA\nPi-0\n\nVisual\n\nmean std\nmean std\nidentity\nmean std\n\nState\n\nmean std\nmin max\nmean std\nmean std\n\nAction\n\nmean std\nmin max\nmean std\nmean std\n\nRelative?\n\nFalse\nFalse\nTrue\nTrue\n\nTABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1].", "3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.", "Color Transformations\n\nType\n\nBrightness\nContrast\nSaturation\nHue\nSharpness\n\nRange\n\n(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)\n\nSpatial Transformations\n\nType\n\nPerspective\nRotation\nCrop\n\nRange\n\n0.025\n[ 5 , 5 ]\n[10, 40] px\n\nTABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.", "4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.", "Model\n\nACT\nDP\nSmolVLA\nPi-0\n\nVisual Res.\n\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\n\nState Dim.\n\n8\n8\n8\n8\n\nAction Dim.\n\n8\n8\n8\n8\n\nTp\n\n50\n64\n50\n50\n\nTe\n\n50\n50\n50\n50\n\nTABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.", "Vision Backbone\n\nResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)\n\n#V-Params\n\n18M\n18M\n350M\n260B\n\n#P-Params\n\n34M\n245M\n100M\n300M\n\nLR\n\n1 10 5\n1 10 4\n1 10 4\n5 10 5\n\nBatch Size\n\n512\n512\n128\n8\n\n#Iters\n\n7k\n7k\n20k\n30k\n\nTABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.", "D. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration.", "2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.", "Task\n\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)\n\nEpisodes\n\n20\n20\n27\n16\n\nx (cm)\n\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n\ny (cm)\n\n[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]\n\nθ (deg)\n\n[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}\n\nTABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.", "3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated\nexperiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0.002, the episode\nis considered successful.", "A PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world\nexperiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates.", "Toy packing\nr = 0.897\nMMRV=0.092\n\nRope routing\nr = 0.918\nMMRV=0.077\n\nT-block pushing\nr = 0.950\nMMRV=0.000\n\nFig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.", "We observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.", "B. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.", "Toy packing\nr = 0.880\nMMRV=0.050\n\nRope routing\nr = 0.887\nMMRV=0.093\n\nT-block pushing\nr = 0.944\nMMRV=0.000\n\nFig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes.", "Toy packing\n\nReplay +\nReplay \n\nGT +\n\n106\n25\n\nGT \n\n37\n132\n\nRope routing\n\nReplay +\nReplay \n\nGT +\n\n276\n24\n\nGT \n\n28\n77\n\nT-block pushing\n\nReplay +\nReplay \n\nGT +\n\n63\n17\n\nGT \n\n1\n111\n\nTABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.", "Figure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay.", "C. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.", "T-block pushing (sim)\n\nT-block pushing (real)\n\nRope routing (sim)\n\nRope routing (real)\n\nToy packing (sim)\n\nToy packing (real)\n\nt\n\nFig. 11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "delimiter", "num_chunks": 253, "avg_chunk_len": 264.901185770751, "std_chunk_len": 646.0726917631422, "max_chunk_len": 3286, "min_chunk_len": 1, "total_chars": 67020, "compression_ratio": 1.0077588779468816, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions", "Simulation", "Z\nC", "n", "io", "at", "l\nre\nor", "Real World", "Success rate - Sim", "arXiv:2511.04665v1 [cs.RO] 6 Nov 2025", "Kaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2", "Success rate - Real", "Toy packing", "Rope routing", "T-block pushing", "Fig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.\nAbstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/", "I. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.", "Simulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.\nWe identify two key factors for aligning simulation with\nreality: appearance and dynamics. On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints. Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom. To address this challenge,", "we adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.\nWe integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.\nIn summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.\nII. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.\nB. Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage", "data from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.\nC. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.\nIII. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAccordingly, the fidelity of simulation can be assessed along", "Real World", "Simulation\nRendering: 3D Gaussian Splatting", "Dynamics: PhysTwin", "Task and\nscene info\nPositional alignment for robot and objects", "Demonstrations", "ACT", "Scene scans", "Di usion", "SmolVLA", "Pi-0", "t\nHuman-object\ninteraction video", "Optimized softbody digital twin", "Color alignment with real cameras", "Policy Training\nEvaluate\npolicy in real:", "Evaluate\npolicy in sim:", "Expensive\n Slow", "Cheap\n Scalable", "Performance\ncorrelation", "env.step()", "Evaluation\nplatform", "env.render()", "Constructed Simulation Env", "ff", "Fig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.", "two axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2.\nB. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).", "C. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.\n1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.\n2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link", "through a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.\n3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1", "f = arg min", "where IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi", "(2)\npdi ]T ,", "(3)", "which reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.\n4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels. For rigid bodies, including objects and", "robot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.\nOverall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.\nD. Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.\nIV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?\nA. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and", "Toy packing\nr = 0.944", "Rope routing\nr = 0.901", "T-block pushing\nr = 0.915", "Ours vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268", "Fig. 3: Correlation between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.", "Toy Packing - DP", "Toy Packing - SmolVLA", "Rope Routing - ACT", "Rope Routing - Pi-0", "T-Block Pushing - DP", "T-Block Pushing - Pi-0", "Fig. 4: Per-policy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.", "body are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require", "the policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation.\n2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.", "Toy packing\nRope routing\nT-block pushing", "Real world", "Ours", "Ours - w/o phys. opt.", "Ours - w/o color align", "IsaacLab", "Fig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.\nToy packing", "B. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.\nC. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.\nD. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,", "IsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs", "Rope routing", "T-block pushing", "MMRV", "r", "MMRV", "r", "MMRV", "r", "0.200\n0.200\n0.087", "0.805\n0.694\n0.944", "0.022\n0.156\n0.119\n0.096", "0.237\n0.714\n0.832\n0.901", "0.031\n0.031\n0.031\n0.000", "0.649\n0.529\n0.905\n0.915", "TABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.", "in the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint).\nE. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage", "in PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics.\nFigure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).\nTable I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points.\nV. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.\nWe demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.\nIn future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.\nACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This", "article solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]", "T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\nC. Chi et al., Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024.\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\nA. Brohan et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\nT. L. Team et al., A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\nNVIDIA, NVIDIA Isaac Sim, 2024.\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\nR. Tedrake, Drake: Model-based design and verification for\nrobotics, 2019.\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs.RO].", "[22]", "[23]\n[24]", "[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]", "[32]", "[33]", "[34]\n[35]\n[36]\n[37]\n[38]", "[39]\n[40]\n[41]\n[42]\n[43]", "M. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024. arXiv: 2411.11839 [cs.RO].\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\nA. Padalkar et al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp. 36 52,\nSep. 2015.\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol. 15, no. 1, pp. 172 188,\n2018.\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.\narXiv: 2310.17596 [cs.RO].\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].", "[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]", "[56]", "[57]\n[58]\n[59]\n[60]\n[61]", "[62]\n[63]\n[64]\n[65]", "Y. R. Wang et al., Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\nOpenAI et al., Solving rubik s cube with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\nB. Chen et al., Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.\n22882 [cs.RO].\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025.\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\nT. Zhang et al., Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26 545 26 555.", "[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]", "[75]", "[76]", "[77]", "[78]\n[79]\n[80]", "X. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.\nJ. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n20813 [cs.RO].\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\nNiantic, Scaniverse, https://scaniverse.com/.\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no. 3, 80 es, Jul.\n2007.\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.", "A PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .", "10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12", "Appendix II: Additional Results\nII-A\nScaling up Simulation Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .", "13\n13\n13\n14", "A PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.\n2) Data Collection: We use GELLO for data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.", "(a) Training initial state distributions", "(b) Evaluation initial state distributions", "Fig. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName", "Dynamics Type", "3D Representation", "xArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip", "Articulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed", "GS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh", "TABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.", "3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images.", "Robot pose 2", "Robot pose 3", "Robot pose 4", "Robot pose 5", "Sim before\nalignment", "Real\n(RealSense)", "Robot pose 1", "Sim after\nalignment", "(a) Training initial state distributions", "Fig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.", "t", "B. Simulation", "Algorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend", "1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process.\n2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.", "3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-", "Rope PhysTwin training video", "Plush toy PhysTwin training video", "Fig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.", "The initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.", "Model", "Visual", "State", "Action", "Relative?", "ACT\nDP\nSmolVLA\nPi-0", "mean std\nmean std\nidentity\nmean std", "mean std\nmin max\nmean std\nmean std", "mean std\nmin max\nmean std\nmean std", "False\nFalse\nTrue\nTrue", "TABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1].\nColor Transformations", "Spatial Transformations", "Type", "Range", "Type", "Range", "Brightness\nContrast\nSaturation\nHue\nSharpness", "(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)", "Perspective\nRotation\nCrop", "0.025\n[ 5 , 5 ]\n[10, 40] px", "TABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.", "Twin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.\n5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\nC. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.\n2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a", "Model\nACT\nDP\nSmolVLA\nPi-0", "Visual Res.", "State Dim.", "Action Dim.", "Tp", "Te", "L: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240", "8\n8\n8\n8", "8\n8\n8\n8", "50\n64\n50\n50", "50\n50\n50\n50", "TABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone", "#V-Params", "#P-Params", "LR", "Batch Size", "#Iters", "ResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)", "18M\n18M\n350M\n260B", "34M\n245M\n100M\n300M", "1 10 5\n1 10 4\n1 10 4\n5 10 5", "512\n512\n128\n8", "7k\n7k\n20k\n30k", "TABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.", "chunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes.\n3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.\n4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.\nD. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration.\n2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.\n3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated", "Toy packing", "Rope routing", "T-block pushing", "r = 0.897", "r = 0.918", "r = 0.950", "MMRV=0.092", "MMRV=0.077", "MMRV=0.000", "Fig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)", "Episodes", "x (cm)", "y (cm)", "θ (deg)", "20\n20\n27\n16", "[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}", "TABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.", "experiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0.002, the episode\nis considered successful.\nA PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world", "experiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates.\nWe observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.\nB. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.", "Toy packing", "Rope routing", "T-block pushing", "r = 0.880", "r = 0.887", "r = 0.944", "MMRV=0.050", "MMRV=0.093", "MMRV=0.000", "Fig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing", "Replay +\nReplay", "Rope routing", "GT +", "GT", "106\n25", "37\n132", "Replay +\nReplay", "T-block pushing", "GT +", "GT", "276\n24", "28\n77", "Replay +\nReplay", "GT +", "GT", "63\n17", "1\n111", "TABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.", "Figure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay.\nC. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.", "T-block pushing (sim)", "T-block pushing (real)", "Rope routing (sim)", "Rope routing (real)", "Toy packing (sim)", "Toy packing (real)", "t", "Fig. 11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "token_limit", "num_chunks": 85, "avg_chunk_len": 794.2588235294118, "std_chunk_len": 49.67521643664657, "max_chunk_len": 800, "min_chunk_len": 339, "total_chars": 67512, "compression_ratio": 1.0004147410830666, "avg_chunk_tokens": 198.3294117647059, "max_chunk_tokens": 200, "min_chunk_tokens": 84, "tokenizer": "", "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions\n\nSimulation\n\nZ\nC\n\nn\n\nio\n\nat\n\nl\nre\nor\n\nReal World\n\nSuccess rate - Sim\n\narXiv:2511.04665v1 [cs.RO] 6 Nov 2025\n\nKaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2\n\nSuccess rate - Real\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nFig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T", "-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.\nAbstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments", "with photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/\n\nI. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to re", "produce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.\n\nSimulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cyc", "les. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.\nWe identify two key factors for aligning simulation with\nreality: appearance and dynamics. On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints", ". Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom. To address this challenge,\n\nwe adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.\nWe integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface", "[26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.\nIn summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,", "using policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.\nII. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16,", "33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.\nB. Physical Digital Twins\nDigit", "al twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage\n\ndata from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems d", "irectly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.\nC. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim", "evaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.\nIII. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented b", "y the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAcco", "rdingly, the fidelity of simulation can be assessed along\n\nReal World\n\nSimulation\nRendering: 3D Gaussian Splatting\n\nDynamics: PhysTwin\n\nTask and\nscene info\nPositional alignment for robot and objects\n\nDemonstrations\n\nACT\n\nScene scans\n\nDi usion\n\nSmolVLA\n\nPi-0\n\nt\nHuman-object\ninteraction video\n\nOptimized softbody digital twin\n\nColor alignment with real cameras\n\nPolicy Training\nEvaluate\npolicy in real:\n\nEvaluate\npolicy in sim:\n\n Expensive\n Slow\n\n Cheap\n Scalable\n\nPerformance\ncorrelation\n\nenv.step()\n\nEvaluation\nplatform\n\nenv.render()\n\nConstructed Simulation Env\n\nff\n\nFig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trai", "ned on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.\n\ntwo axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we ad", "dress both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2.\nB. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording t", "o Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the", "objects\nthat interact with the robot (plush toy, rope, and T-block).\n\nC. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.\n1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The re", "sulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.\n2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75].", "We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link\n\nthrough a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.\n3) Color Alignment: A major contributor to the", "visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, formin", "g paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1\n\nf = arg min\n\nwhere IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi \n\n(2)\npdi ]T ,\n\n(3)\n\nwhich reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.\n4) Physics and", "Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics eng", "ine are propagated to the\nGaussian kernels. For rigid bodies, including objects and\n\nrobot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.\nOverall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.\nD. Policy Evaluation\nTo evaluate visuomoto", "r policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starti", "ng configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.\nIV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning p", "olicies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?\nA. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and\n\nToy packing\nr = 0.944\n\nRope routing\nr = 0.901\n\nT-block pushing\nr = 0.915\n\nOurs vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268\n\nFig. 3: Correlation", "between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.\n\nToy Packing - DP\n\nToy Packing - SmolVLA\n\nRope Routing - ACT\n\nRope Routing - Pi-0\n\nT-Block Pushing - DP\n\nT-Block Pushing - Pi-0\n\nFig. 4: Per-p", "olicy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.\n\nbody are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis", "placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require\n\nthe policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation.\n2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nra", "nges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy", "success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRo", "bot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.\n\nToy packing\nRope routing\nT-block pushing\n\nReal world\n\nOurs\n\nOurs - w/o phys. opt.\n\nOurs - w/o color align\n\nIsaacLab\n\nFig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics comp", "ared to our approach.\nToy packing\n\nB. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.\nC. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We obse", "rve a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.\nD. Policy Performance Analysis\nFigure 4 further illustrates per-policy,", "per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,\n\nIsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs\n\nRope routing\n\nT-block pushing\n\nMMRV \n\nr \n\nMMRV \n\nr \n\nMMRV \n\nr \n\n0.200\n0.200\n0.087\n\n0.805\n0.694\n0.944\n\n0.022\n0.156\n0.119\n0.096\n\n0.237\n0.714\n0.832\n0.901\n\n0.031\n0.031\n0.031\n0.000\n\n0.649\n0.529\n0.905\n0.915\n\nTABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer", "errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.\n\nin the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episo", "des (16 27 per checkpoint).\nE. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage\n\nin PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mis", "match in dynamics.\nFigure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T", "task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).\nTable I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points.\nV. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through ph", "ysics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.\nWe demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.\nIn future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nt", "he key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.\nACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This\n\narticle solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of", "the RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\nC. Chi et al., Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching wi", "thout in-the-wild robots, in RSS, 2024.\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\nA. Brohan et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\nT. L. Team et al., A careful examination of large behavior\nmodels for m", "ultitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\nNVIDIA, NVIDIA Isaac Sim, 2024.\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\nR. Tedrake, Drake: Model-based design and verification for\nrobotic", "s, 2019.\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs.RO].\n\n[22]\n\n[23]\n[24]\n\n[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]\n\n[32]\n\n[33]\n\n[34]\n[35]\n[36]\n[37]\n[38]\n\n[39]\n[40]\n[41]\n[42]\n[43]\n\nM. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 20", "24. arXiv: 2411.11839 [cs.RO].\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\nA. Padalkar e", "t al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp. 36 52,\nSep. 2015.\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering", ", vol. 15, no. 1, pp. 172 188,\n2018.\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatar", "s\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.\narXiv: 2310.17596 [cs.RO].\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].\n\n[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]\n\n[56]\n\n[57]\n[58]\n[59]\n[60]\n[61]\n\n[62]\n[63]\n[64]\n[65]\n\nY. R. Wang et al., Roboeval: Where robotic manipulation meets structured an", "d scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\nOpenAI et al., Solving rubik s cube with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\nB. Chen et al., Physgen3d: Crafting a miniature interac", "tive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2", "025. arXiv: 2505.\n22882 [cs.RO].\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-", "d\nvideos, in RSS, 2025.\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\nT. Zhang et al., Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26", "545 26 555.\n\n[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]\n\n[75]\n\n[76]\n\n[77]\n\n[78]\n[79]\n[80]\n\nX. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.\nJ. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n208", "13 [cs.RO].\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\nNiantic, Scaniverse, https://scaniverse.com/.\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some r", "obust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no. 3, 80 es, Jul.\n2007.\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.\n\nA PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nP", "latform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .\n\n10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\nAppendix II: Additional Results\nII-A\nScaling up Simula", "tion Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .\n\n13\n13\n13\n14\n\nA PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.\n2) Data Collection: We use GELLO for", "data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.\n\n(a) Training initial state distributions\n\n(b) Evaluation initial state distributions\n\nFi", "g. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName\n\nDynamics Type\n\n3D Representation\n\nxArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip\n\nArticulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed\n\nGS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh\n\nTABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.\n\n3) Task Definition: To evaluate the effectiveness of our\nsi", "mulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Ro", "uting: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized a", "s a\nyellow translucent mask overlaid on the fixed-camera images.\n\nRobot pose 2\n\nRobot pose 3\n\nRobot pose 4\n\nRobot pose 5\n\nSim before\nalignment\n\nReal\n(RealSense)\n\nRobot pose 1\n\nSim after\nalignment\n\n(a) Training initial state distributions\n\nFig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.\n\nt\n\nB. Simulation\n\nAlgorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring", "-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend\n\n1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process.\n2) Positional Alignment: To align the robot-scene Gaussian Splatting sca", "n with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.\n\n3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor spac", "e, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-\n\nRope PhysTwin training video\n\nPlush toy PhysTwin training video\n\nFig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the", "deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.\n\nThe initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.\n\nModel\n\nVisual\n\nState\n\nAction\n\nRelative?\n\nACT\nDP\nSmolVLA\nPi-0\n\nmean std\nmean std\nidentity\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nFalse\nFalse\nTrue\nTrue\n\nTABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales val", "ues to [ 1, 1].\nColor Transformations\n\nSpatial Transformations\n\nType\n\nRange\n\nType\n\nRange\n\nBrightness\nContrast\nSaturation\nHue\nSharpness\n\n(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)\n\nPerspective\nRotation\nCrop\n\n0.025\n[ 5 , 5 ]\n[10, 40] px\n\nTABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.\n\nTwin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T", "-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.\n5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\nC. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.\n2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consist", "ent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a\n\nModel\nACT\nDP\nSmolVLA\nPi-0\n\nVisual Res.\n\nState Dim.\n\nAction Dim.", "Tp\n\nTe\n\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\n\n8\n8\n8\n8\n\n8\n8\n8\n8\n\n50\n64\n50\n50\n\n50\n50\n50\n50\n\nTABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone\n\n#V-Params\n\n#P-Params\n\nLR\n\nBatch Size\n\n#Iters\n\nResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)\n\n18M\n18M\n350M\n260B\n\n34M\n245M\n100M\n300M\n\n1 10 5\n1 10 4\n1 10 4\n5 10 5\n\n512\n512\n128\n8\n\n7k\n7k\n20k\n30k\n\nTABLE VI: Training configuration. Model-specific hyperparameters us", "ed in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.\n\nchunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes.\n3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.\n4) Hyperparameters: A", "complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.\nD. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the object", "s to match the\nsimulated configuration.\n2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.\n3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.897\n\nr = 0.918\n\nr = 0.950\n\nMMRV=0.092\n\nMMRV=0.077\n\nMMRV=0.000\n\nFig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields", "stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)\n\nEpisodes\n\nx (cm)\n\ny (cm)\n\nθ (deg)\n\n20\n20\n27\n16\n\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}\n\nTABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.\n\nexperiments to scale up, automat", "ed success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring", "segments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0.002, the episode\nis considered successful.\nA PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world\n\nexperiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflecte", "d by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports th", "e resulting correlations between the scaled-up simulation metrics and real-world success rates.\nWe observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.\nB. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived", "images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.880\n\nr = 0.887\n\nr = 0.944\n\nMMRV=0.050\n\nMMRV=0.093\n\nMMRV=0.000\n\nFig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights t", "he degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing\n\nReplay +\nReplay \n\nRope routing\n\nGT +\n\nGT \n\n106\n25\n\n37\n132\n\nReplay +\nReplay \n\nT-block pushing\n\nGT +\n\nGT \n\n276\n24\n\n28\n77\n\nReplay +\nReplay \n\nGT +\n\nGT \n\n63\n17\n\n1\n111\n\nTABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.\n\nFigure 10 shows the resulting correlations, and Table VIII\nreports the per-episode", "replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajecto", "ry replay.\nC. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.\n\nT-block pushing (sim)\n\nT-block pushing (real)\n\nRope routing (sim)\n\nRope routing (real)\n\nToy packing (sim)\n\nToy packing (real)\n\nt\n\nFig. 11: Sim and real rollout trajectories.", "Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "format_aware", "num_chunks": 1, "avg_chunk_len": 67539.0, "std_chunk_len": 0.0, "max_chunk_len": 67539, "min_chunk_len": 67539, "total_chars": 67539, "compression_ratio": 1.0000148062600869, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions\n\nSimulation\n\nZ\nC\n\nn\n\nio\n\nat\n\nl\nre\nor\n\nReal World\n\nSuccess rate - Sim\n\narXiv:2511.04665v1 [cs.RO] 6 Nov 2025\n\nKaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2\n\nSuccess rate - Real\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nFig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.\nRight: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment.\nAbstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects. Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies. Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval.github.io/\n\nI. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.\n\nSimulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles. This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably.\nWe identify two key factors for aligning simulation with\nreality: appearance and dynamics. On the appearance side,\nrendered scenes must closely match real-world observations.\nThis is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints. Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap.\nDynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom. To address this challenge,\n\nwe adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics.\nWe integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26]. We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation.\nIn summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.\n(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines.\nII. R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups. To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments.\nThis is widely referred to as the sim-to-real gap [45 48].\nWe aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics.\nB. Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage\n\ndata from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings. Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.\nC. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56]. Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator.\nIII. M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].\nThe objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies.\nTo achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T . A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e.g., RGB images).\nAccordingly, the fidelity of simulation can be assessed along\n\nReal World\n\nSimulation\nRendering: 3D Gaussian Splatting\n\nDynamics: PhysTwin\n\nTask and\nscene info\nPositional alignment for robot and objects\n\nDemonstrations\n\nACT\n\nScene scans\n\nDi usion\n\nSmolVLA\n\nPi-0\n\nt\nHuman-object\ninteraction video\n\nOptimized softbody digital twin\n\nColor alignment with real cameras\n\nPolicy Training\nEvaluate\npolicy in real:\n\nEvaluate\npolicy in sim:\n\n Expensive\n Slow\n\n Cheap\n Scalable\n\nPerformance\ncorrelation\n\nenv.step()\n\nEvaluation\nplatform\n\nenv.render()\n\nConstructed Simulation Env\n\nff\n\nFig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins. Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior. The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.\n\ntwo axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations.\nIn this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations. The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2.\nB. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system. Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics.\nTo capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y . The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).\n\nC. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.\n1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer. This reconstruction step is required\nonly once per task.\n2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation. For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry.\nBecause the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.\nTo enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link\n\nthrough a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing. The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models.\n3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.g., Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.\nWe perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization. We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1\n\nf = arg min\n\nwhere IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi \n\n(2)\npdi ]T ,\n\n(3)\n\nwhich reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.\n4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper. Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps.\nAt each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels. For rigid bodies, including objects and\n\nrobot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation.\nOverall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states. By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution.\nD. Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training. Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i.e., no co-training between simulation and reality).\nTo improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world. In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations.\nPolicy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network. Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation.\nIV. E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation? (3) What practical benefits can the correlation provide?\nA. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box. A trial is\nconsidered successful only if the toy s arms, legs, and\n\nToy packing\nr = 0.944\n\nRope routing\nr = 0.901\n\nT-block pushing\nr = 0.915\n\nOurs vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268\n\nFig. 3: Correlation between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.\n\nToy Packing - DP\n\nToy Packing - SmolVLA\n\nRope Routing - ACT\n\nRope Routing - Pi-0\n\nT-Block Pushing - DP\n\nT-Block Pushing - Pi-0\n\nFig. 4: Per-policy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.\n\nbody are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require\n\nthe policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation.\n2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.\nThese initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states. We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively.\nWe use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).\nThe number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.\nWe evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state. For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79].\nTraining is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.\n\nToy packing\nRope routing\nT-block pushing\n\nReal world\n\nOurs\n\nOurs - w/o phys. opt.\n\nOurs - w/o color align\n\nIsaacLab\n\nFig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.\nToy packing\n\nB. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup. IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.\nC. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks. Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0.9 for all policies. By\ncontrast, the baseline yields only r = 0.649 on push-T, and an\neven lower r = 0.237 on rope routing as a result of the larger\ndynamics gap. The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations.\nD. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations. We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,\n\nIsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs\n\nRope routing\n\nT-block pushing\n\nMMRV \n\nr \n\nMMRV \n\nr \n\nMMRV \n\nr \n\n0.200\n0.200\n0.087\n\n0.805\n0.694\n0.944\n\n0.022\n0.156\n0.119\n0.096\n\n0.237\n0.714\n0.832\n0.901\n\n0.031\n0.031\n0.031\n0.000\n\n0.649\n0.529\n0.905\n0.915\n\nTABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.\n\nin the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance.\nIn cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ. We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint).\nE. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.\nWe provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance.\n Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage\n\nin PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics.\nFigure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states. Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes.\nIn contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches.\nEmpirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap. For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment).\nTable I details the quantitative results. Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points.\nV. C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering.\nWe demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies. Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0.9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.\nWe found that both physics optimization and color alignment\nare critical for closing policy performance gaps.\nIn future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.\nACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This\n\narticle solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023. arXiv: 2304.13705 [cs.RO].\nC. Chi et al., Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.\nK. Black et al., π0 : A vision-language-action flow model\nfor general robot control, 2024. arXiv: 2410 . 24164\n[cs.LG].\nM. Shukor et al., Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025. arXiv: 2506 .\n01844 [cs.LG].\nC. Chi et al., Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024.\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.20396, 2025.\nB. Tang et al., Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305.17110\n[cs.RO].\nA. Brohan et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307.15818, 2023.\nP. Intelligence et al., π0.5 : A vision-language-action model\nwith open-world generalization, 2025. arXiv: 2504.16054\n[cs.LG].\nNVIDIA et al., GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.\narXiv: 2503.14734.\nT. L. Team et al., A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025. arXiv:\n2507.05331 [cs.RO].\nG. R. Team et al., Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503.20020 [cs.RO].\nNVIDIA, NVIDIA Isaac Sim, 2024.\nE. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp. 5026 \n5033.\nF. Xiang et al., SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\nC. Li et al., Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.09227 [cs.RO].\nG. Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024.\nR. Tedrake, Drake: Model-based design and verification for\nrobotics, 2019.\nX. Li et al., Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024.\nB. Kerbl, G. Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol. 42, no. 4, Jul. 2023.\nJ. Abou-Chakra et al., Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025. arXiv: 2504.03597\n[cs.RO].\n\n[22]\n\n[23]\n[24]\n\n[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]\n\n[32]\n\n[33]\n\n[34]\n[35]\n[36]\n[37]\n[38]\n\n[39]\n[40]\n[41]\n[42]\n[43]\n\nM. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.10161 [cs.RO].\nX. Li et al., Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024. arXiv: 2411.11839 [cs.RO].\nL. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025. arXiv: 2412.14957 [cs.RO].\nH. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.\nG. Brockman et al., Openai gym, 2016. arXiv: 1606 .\n01540 [cs.LG].\nOcto Model Team et al., Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024.\nJ. Wang, M. Leonard, K. Daniilidis, D. Jayaraman, and E. S.\nHu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.\nA. Padalkar et al., Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.08864,\n2023.\nA. Khazatsky et al., Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024.\nB. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol. 22, no. 3, pp. 36 52,\nSep. 2015.\nK. Van Wyk, J. Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.\nN. Correll et al., Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol. 15, no. 1, pp. 172 188,\n2018.\nG. Zhou et al., Train offline, test online: A real robot learning\nbenchmark, 2023. arXiv: 2306.00942 [cs.RO].\nS. Dasari et al., Rb2: Robotic manipulation benchmarking\nwith a twist, 2022. arXiv: 2203.08098 [cs.RO].\nS. Tao et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909.12271 [cs.RO].\nS. Srivastava et al., Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G. Neumann,\nEds., ser. PMLR, vol. 164, Aug. 2022, pp. 477 490.\nX. Puig et al., Habitat 3.0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310.13724 [cs.HC].\nS. Nasiriany et al., Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024.\nY. Zhu et al., Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 .\n12293 [cs.RO].\nA. Mandlekar et al., Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023.\narXiv: 2310.17596 [cs.RO].\nX. Yang, C. Eppner, J. Tremblay, D. Fox, S. Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 . 11117\n[cs.RO].\n\n[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]\n\n[56]\n\n[57]\n[58]\n[59]\n[60]\n[61]\n\n[62]\n[63]\n[64]\n[65]\n\nY. R. Wang et al., Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507.00435 [cs.RO].\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810.\nY. Chebotar et al., Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.\nOpenAI et al., Solving rubik s cube with a robot hand, 2019.\narXiv: 1910.07113 [cs.LG].\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y.\nBai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021. arXiv: 2011.03148 [cs.RO].\nS. Liu, Z. Ren, S. Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.\nB. Chen et al., Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp. 6178 6189.\nY. Jiang et al., Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1.\nT. Xie et al., Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.\nR.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024. arXiv: 2404.01223 [cs.CV].\nB. Bianchini, M. Zhu, M. Sun, B. Jiang, C. J. Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.\n22882 [cs.RO].\nJ. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024.\nK.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.\nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W.\nBattaglia, Learning mesh-based simulation with graph networks, 2021. arXiv: 2010.03409 [cs.LG].\nK. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024.\nK. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025.\nT. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503.11999,\n2025.\nX. Li et al., Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.05512, 2023.\nT. Zhang et al., Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406.\nL. Zhong, H.-X. Yu, J. Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.\nC. Chen et al., Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp. 26 545 26 555.\n\n[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]\n\n[75]\n\n[76]\n\n[77]\n\n[78]\n[79]\n[80]\n\nX. Han et al., Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.08645, 2025.\nA. Escontrela et al., Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510.15352, 2025.\nJ. Yu et al., Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.\n09601 [cs.RO].\nS. Yang et al., Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504.13175, 2025.\nG. Jiang et al., Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025. arXiv: 2510.\n20813 [cs.RO].\nH. Kress-Gazit et al., Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.09491, 2024.\nNiantic, Scaniverse, https://scaniverse.com/.\nPlayCanvas and Snap Inc., Supersplat, https : / /\ngithub.com/playcanvas/supersplat, [Computer\nsoftware], 2025.\nK. S. Arun, T. S. Huang, and S. D. Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp. 698 700, 1987.\nM. A. Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, vol. 24,\nno. 6, pp. 381 395, Jun. 1981.\nP. J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no. 2, pp. 149 170, 1984.\nM. Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github .\ncom/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.\nR. W. Sumner, J. Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no. 3, 80 es, Jul.\n2007.\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024.\nR. Cadene et al., Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github .\ncom/huggingface/lerobot, 2024.\n\nA PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .\n\n10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12\n\nAppendix II: Additional Results\nII-A\nScaling up Simulation Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .\n\n13\n13\n13\n14\n\nA PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom. The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.\n2) Data Collection: We use GELLO for data collection.\nGELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta. To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions.\nDuring policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.\n\n(a) Training initial state distributions\n\n(b) Evaluation initial state distributions\n\nFig. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName\n\nDynamics Type\n\n3D Representation\n\nxArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip\n\nArticulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed\n\nGS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh\n\nTABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.\n\n3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows.\na) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box. The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.\nb) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.\nc) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images.\n\nRobot pose 2\n\nRobot pose 3\n\nRobot pose 4\n\nRobot pose 5\n\nSim before\nalignment\n\nReal\n(RealSense)\n\nRobot pose 1\n\nSim after\nalignment\n\n(a) Training initial state distributions\n\nFig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.\n\nt\n\nB. Simulation\n\nAlgorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend\n\n1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process.\n2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes. Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.\n\n3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-\n\nRope PhysTwin training video\n\nPlush toy PhysTwin training video\n\nFig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.\n\nThe initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.\n\nModel\n\nVisual\n\nState\n\nAction\n\nRelative?\n\nACT\nDP\nSmolVLA\nPi-0\n\nmean std\nmean std\nidentity\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nmean std\nmin max\nmean std\nmean std\n\nFalse\nFalse\nTrue\nTrue\n\nTABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1].\nColor Transformations\n\nSpatial Transformations\n\nType\n\nRange\n\nType\n\nRange\n\nBrightness\nContrast\nSaturation\nHue\nSharpness\n\n(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)\n\nPerspective\nRotation\nCrop\n\n0.025\n[ 5 , 5 ]\n[10, 40] px\n\nTABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.\n\nTwin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8. For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0.5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.\n5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1.\nC. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.\n2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].\nFor the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a\n\nModel\nACT\nDP\nSmolVLA\nPi-0\n\nVisual Res.\n\nState Dim.\n\nAction Dim.\n\nTp\n\nTe\n\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\n\n8\n8\n8\n8\n\n8\n8\n8\n8\n\n50\n64\n50\n50\n\n50\n50\n50\n50\n\nTABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone\n\n#V-Params\n\n#P-Params\n\nLR\n\nBatch Size\n\n#Iters\n\nResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)\n\n18M\n18M\n350M\n260B\n\n34M\n245M\n100M\n300M\n\n1 10 5\n1 10 4\n1 10 4\n5 10 5\n\n512\n512\n128\n8\n\n7k\n7k\n20k\n30k\n\nTABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.\n\nchunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes.\n3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training.\nFor every image in a training batch, three augmentation\noperations are randomly sampled and composed. Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges.\n4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI.\nFor VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.\nD. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup. Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration.\n2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII.\n3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias. For simulated\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.897\n\nr = 0.918\n\nr = 0.950\n\nMMRV=0.092\n\nMMRV=0.077\n\nMMRV=0.000\n\nFig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)\n\nEpisodes\n\nx (cm)\n\ny (cm)\n\nθ (deg)\n\n20\n20\n27\n16\n\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]\n\n[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}\n\nTABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.\n\nexperiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050.\nb) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip. Within the final 100\nframes (3.3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful.\nc) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles. Within\nthe final 100 frames (3.3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0.002, the episode\nis considered successful.\nA PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world\n\nexperiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals.\nTo account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper. In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7.5, 7.5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7.5, 7.5] cm.\nTo better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates.\nWe observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances.\nB. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.\nThis allows us to disentangle dynamic discrepancies from\nappearance gaps, i.e., the difference in policy behaviors\nintroduced by differences in perceived images is eliminated.\nIn total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.\n\nToy packing\n\nRope routing\n\nT-block pushing\n\nr = 0.880\n\nr = 0.887\n\nr = 0.944\n\nMMRV=0.050\n\nMMRV=0.093\n\nMMRV=0.000\n\nFig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing\n\nReplay +\nReplay \n\nRope routing\n\nGT +\n\nGT \n\n106\n25\n\n37\n132\n\nReplay +\nReplay \n\nT-block pushing\n\nGT +\n\nGT \n\n276\n24\n\n28\n77\n\nReplay +\nReplay \n\nGT +\n\nGT \n\n63\n17\n\n1\n111\n\nTABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.\n\nFigure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes.\nNotably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.\nFor T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states.\nOverall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay.\nC. Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views.\nFrom the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations. Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.\n\nT-block pushing (sim)\n\nT-block pushing (real)\n\nRope routing (sim)\n\nRope routing (real)\n\nToy packing (sim)\n\nToy packing (real)\n\nt\n\nFig. 11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
{"method": "hybrid", "num_chunks": 502, "avg_chunk_len": 133.27888446215138, "std_chunk_len": 171.04093634195198, "max_chunk_len": 771, "min_chunk_len": 1, "total_chars": 66906, "compression_ratio": 1.0094759812273937, "chunks": ["Real-to-Sim Robot Policy Evaluation with\nGaussian Splatting Simulation of Soft-Body Interactions", "Simulation", "Z\nC", "n", "io", "at", "l\nre\nor", "Real World", "Success rate - Sim", "arXiv:2511.04665v1 [cs.RO] 6 Nov 2025", "Kaifeng Zhang1,2 , Shuo Sha1,2 , Hanxiao Jiang1 , Matthew Loper2 , Hyunjong Song2 ,\nGuangyan Cai2 , Zhuo Xu3 , Xiaochen Hu2 , Changxi Zheng1,2 , Yunzhu Li1,2", "Success rate - Real", "Toy packing", "Rope routing", "T-block pushing", "Fig. 1: Real-to-sim policy evaluation with Gaussian Splatting simulation. Left: Correlation between simulated and real-world success\nrates across multiple policies (ACT [1], DP [2], Pi-0 [3], SmolVLA [4]) shows that our simulation reliably predicts real-world performance.", "Right: Representative tasks used for evaluation, including plush toy packing, rope routing, and T-block pushing, are visualized in both\nreal and simulated settings. Our framework reconstructs soft-body digital twins from real-world videos and achieves realistic appearance\nand motion, enabling scalable and reproducible policy assessment. Abstract Robotic manipulation policies are advancing\nrapidly, but their direct evaluation in the real world remains\ncostly, time-consuming, and difficult to reproduce, particularly\nfor tasks involving deformable objects.", "Simulation provides a\nscalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of\nsoft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from\nreal-world videos and renders robots, objects, and environments\nwith photorealistic fidelity using 3D Gaussian Splatting. We\nvalidate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and Tblock pushing, demonstrating that simulated rollouts correlate\nstrongly with real-world execution performance and reveal key\nbehavioral patterns of learned policies.", "Our results suggest\nthat combining physics-informed reconstruction with highquality rendering enables reproducible, scalable, and accurate\nevaluation of robotic manipulation policies. Website: https:\n//real2sim-eval. github.", "io/", "I. I NTRODUCTION\nRobotic manipulation policies have advanced rapidly\nacross a wide range of tasks [1, 2, 5 7]. However, their\nevaluation still relies heavily on real-world trials, which are\nslow, expensive, and difficult to reproduce. As the community shifts toward training foundation models for robotics [3,\n8 12], whose development depends on rapid iteration and\nlarge-scale benchmarking, this reliance has become a significant bottleneck.\n1 Columbia University\n2 SceniX Inc.\n3 Google DeepMind\n* Equal contribution. Work partially done while interning at SceniX Inc.", "Simulation offers a scalable and systematic alternative and\nis widely used for data generation and training [13 18]. Yet it\nis far less common as a tool for policy evaluation, primarily\ndue to poor sim-to-real correlation: a policy that performs\nwell in simulation often fails to translate to similar real-world\nsuccess. Narrowing this gap would allow simulation to serve\nas a trustworthy proxy for real-world testing, greatly accelerating development cycles.", "This raises the central question:\nhow can we design simulators that are sufficiently realistic\nto evaluate robot policies with confidence? To answer this\nquestion, we propose a framework for building high-fidelity\nsimulators and investigate whether they can predict realworld policy performance reliably. We identify two key factors for aligning simulation with\nreality: appearance and dynamics.", "On the appearance side,\nrendered scenes must closely match real-world observations. This is particularly challenging for policies that rely on\nwrist-mounted cameras, where simple green-screen compositing [19] is insufficient. We address this by leveraging\n3D Gaussian Splatting (3DGS) [20], which reconstructs photorealistic scenes from a single scan and supports rendering\nfrom arbitrary viewpoints.", "Beyond prior uses of 3DGS for\nsimulation [21 24], we enhance it with automatic position\nand color alignment and object deformation handling, which\nare essential for closing the appearance gap. Dynamics present another major source of sim-to-real\ndiscrepancy. Traditional simulators rely on low-dimensional\nparameter tuning, which is insufficient for deformable objects\nwith many degrees of freedom.", "To address this challenge,", "we adopt PhysTwin [25], a framework that reconstructs\ndeformable objects as dense spring-mass systems optimized\ndirectly from object interaction videos. This approach yields\nefficient system identification while closely matching realworld dynamics. We integrate these appearance and dynamics components\ninto a unified simulator and expose it through a Gym-style\ninterface [26].", "We evaluate this framework on representative\nrigid- and soft-body manipulation tasks, including plush toy\npacking, rope routing, and T-block pushing, using widely\nadopted imitation learning algorithms: ACT [1], Diffusion\nPolicy (DP) [2], SmolVLA [4], and Pi-0 [3]. By comparing\nsimulated and real-world success rates and performing ablation studies, we observe a strong correlation and confirm\nthat rendering and dynamics fidelity are both crucial to the\ntrustworthiness of simulation-based evaluation. In summary, our main contributions are: (1) A complete framework for evaluating robot policies in a Gaussian Splatting-based simulator using soft-body digital twins.", "(2) Empirical evidence that simulated rollouts strongly correlate with real-world success rates across representative tasks,\nusing policies trained exclusively on real-world data (no\nco-training). (3) A detailed analysis of design choices that\nimprove the reliability of simulation as a predictor of realworld performance, offering guidance for future simulationbased evaluation pipelines. II.", "R ELATED W ORKS\nA. Robot Policy Evaluation\nEvaluating robot policies is essential for understanding and\ncomparing policy behaviors. Most systems are still evaluated\ndirectly in the real world [11, 27 30], but such evaluations\nare costly, time-consuming, and usually tailored to specific\ntasks, embodiments, and sensor setups.", "To enable more\nsystematic study, prior works have introduced benchmarks,\neither in the real world through standardized hardware setups [31 35] or in simulation through curated assets and task\nsuites [16, 33, 36 44]. Real-world benchmarks offer high\nfidelity but lack flexibility and scalability, while simulators\noften suffer from unrealistic dynamics and rendering, which\nlimits their reliability as proxies for physical experiments. This is widely referred to as the sim-to-real gap [45 48].", "We aim to narrow this gap by building a realistic simulator\nthat combines high-quality rendering with faithful soft-body\ndynamics. Compared to SIMPLER [19], which relies on\ngreen-screen compositing, and Real-is-sim [21], which focuses on rigid-body simulation, our method integrates Gaussian Splatting-based rendering with soft-body digital twins\nderived from interaction videos, eliminating the dependence\non static cameras and providing more realistic appearance\nand dynamics. B.", "Physical Digital Twins\nDigital twins seek to realistically reconstruct and simulate\nreal-world objects. Many existing frameworks rely on prespecified physical parameters [49 53], which limits their\nability to capture complex real-world dynamics or leverage", "data from human interaction. While rigid-body twins are\nwell studied [54 57], full-order parameter identification for\ndeformable objects remains challenging. Learning-based approaches have been proposed to capture such dynamics [58 \n61], but they often sacrifice physical consistency, which\nis critical for evaluating manipulation policies in contactrich settings.", "Physics-based methods that optimize physical\nparameters from video observations [62 65] offer a more\npromising path. Among them, PhysTwin [25] reconstructs\ndeformable objects as dense spring-mass systems directly\nfrom human-object interaction videos, achieving state-of-theart realism and efficiency. Our work builds on PhysTwin\nand integrates its reconstructions with a Gaussian Splatting\nsimulator to bridge the dynamics gap in policy evaluation.", "C. Gaussian Splatting Simulators\nBuilding simulators that closely match the real world requires high-quality rendering and accurate physics. Gaussian\nSplatting (3DGS) [20] has recently emerged as a powerful\napproach for scene reconstruction, enabling photorealistic,\nreal-time rendering from arbitrary viewpoints [51, 56].", "Several studies have demonstrated its potential in robotics,\nshowing that 3DGS-based rendering can improve sim-toreal transfer for vision-based policies [22, 66, 67], augment\ntraining datasets [23, 24, 68, 69], and enable real-to-sim\nevaluation [21, 70]. We extend this line of work by supporting soft-body interactions, incorporating PhysTwin [25] for\nrealistic dynamics, and introducing automated position and\ncolor alignment, resulting in a complete and evaluation-ready\nsimulator. III.", "M ETHOD\nA. Problem Definition\nWe study the policy evaluation problem: Can a simulator\nreliably predict the real-world performance of visuomotor\npolicies trained with real data? In a typical evaluation\npipeline [11, 71], multiple policies are executed across\ncontrolled initial configurations in both simulation and the\nreal world, and performance is measured through rolloutbased metrics, typically expressed as scalar scores u [0, 1].", "The objective is to establish a strong correlation between\nsimulated and real-world outcomes, represented by the paired\nset {(ui,sim , ui,real )}Ni=1 , where ui,sim and ui,real denote the\nperformance of the i-th policy in simulation and reality,\nrespectively, and N is the number of evaluated policies. To achieve better performance correlation, one promising\nway is to build a simulator that yields consistent results\nT\nwith the real world. Formally, let {(st , ot , at )}t=1\ndenote the\nsequence of environment states st , robot observations ot ,\nand robot actions at over a time horizon T .", "A simulator\nfor policy evaluation should contain two core components:\n(1) Dynamics model: st+1 = f (st , at ), which predicts future\nstates given the current state and robot actions. (2) Appearance model: ot = g(st ), which renders observations in the\ninput modality required by the policy (e. g.", ", RGB images). Accordingly, the fidelity of simulation can be assessed along", "Real World", "Simulation\nRendering: 3D Gaussian Splatting", "Dynamics: PhysTwin", "Task and\nscene info\nPositional alignment for robot and objects", "Demonstrations", "ACT", "Scene scans", "Di usion", "SmolVLA", "Pi-0", "t\nHuman-object\ninteraction video", "Optimized softbody digital twin", "Color alignment with real cameras", "Policy Training\nEvaluate\npolicy in real:", "Evaluate\npolicy in sim:", "Expensive\n Slow", "Cheap\n Scalable", "Performance\ncorrelation", "env.step()", "Evaluation\nplatform", "env.render()", "Constructed Simulation Env", "ff", "Fig. 2: Proposed framework for real-to-sim policy evaluation. We present a pipeline that evaluates real-world robot policies in simulation\nusing Gaussian Splatting-based rendering and soft-body digital twins.", "Policies are first trained on demonstrations collected by the real\nrobot, and a phone scan of the workspace is used to reconstruct the scene via Gaussian Splatting. The reconstruction is segmented into\nrobot, objects, and background, then aligned in position and color to enable photorealistic rendering. For dynamics, we optimize soft-body\ndigital twins from object interaction videos to accurately reproduce real-world behavior.", "The resulting simulation is exposed through\na Gym-style API [26], allowing trained policies to be evaluated efficiently. Compared with real-world trials, this simulator is cheaper,\nreproducible, and scalable, while maintaining strong correlation with real-world performance.", "two axes: (i) the accuracy of simulated dynamics, and (ii) the\nrealism of rendered observations. In this work, we address both axes by jointly reducing\nthe visual gap and the dynamics gap. We employ physicsinformed reconstruction of soft-body digital twins to align\nsimulated dynamics with real-world object behavior, and use\nhigh-resolution Gaussian Splatting as the rendering engine to\ngenerate photorealistic observations.", "The following sections\ndescribe these components in detail, and an overview of the\nfull framework is shown in Figure 2. B. Preliminary: PhysTwin\nWe adopt the PhysTwin [25] digital twin framework,\nwhich reconstructs and simulates deformable and rigid objects from video using a dense spring-mass system.", "Each\nobject is represented as a set of mass nodes connected by\nsprings, with springs formed between each pair of nodes\nwithin a distance threshold d. The node positions evolve\naccording to Newtonian dynamics. To capture the behavior of diverse real-world deformable\nobjects with varying stiffness, friction, and other material\nproperties, PhysTwin employs a real-to-sim pipeline that\njointly optimizes a set of physical parameters, including the\nspring threshold d and per-spring stiffness coefficients Y .", "The\noptimization is performed from a single video of a human interacting with the object by hand: human hand keypoints are\ntracked and attached to the spring-mass system as kinematic\ncontrol points, and system parameters are tuned to minimize\nthe discrepancy between tracked object motions in the video\nand their simulated counterparts. For rigid bodies, Y is fixed\nto a large value to suppress deformation. We adopt this same\nreal-to-sim process for system identification of the objects\nthat interact with the robot (plush toy, rope, and T-block).", "C. Real-to-Sim Gaussian Splatting Simulation\nWe now describe the construction of our Gaussian\nSplatting-based simulator. Our approach addresses two complementary goals: (i) closing the visual gap through GS scene\nreconstruction, positional alignment, and color alignment,\nand (ii) closing the dynamics gap through physics-based\nmodeling and deformation handling.", "1) GS Construction: We begin by acquiring the appearance of each object of interest using Scaniverse [72], an\niPhone app that automatically generates GS reconstructions\nfrom video recordings. In a tabletop manipulation scene, we\nfirst scan the static robot workspace, including the robot,\ntable, and background, then scan each experimental object\nindividually. The resulting reconstructions are segmented\ninto robot, objects, and background using the SuperSplat [73]\ninteractive visualizer.", "This reconstruction step is required\nonly once per task. 2) Positional Alignment: After obtaining GS reconstructions of the static background, robot, PhysTwin object,\nand other static objects, we align all components to the\nreference frames: the robot base frame and canonical object\nframes. PhysTwin objects and static meshes are aligned to\ntheir corresponding PhysTwin particle sets and object 3D\nmodels by applying a relative 6-DoF transformation.", "For the\nrobot, we automatically compute the transformation between\nthe reconstructed GS model and ground truth robot points\n(generated from its URDF) using a combination of Iterative\nClosest Point (ICP) [74] and RANSAC [75]. We use 2,000\npoints per link to ensure sufficient coverage of link geometry. Because the background GS is in the same frame as the robot\nGS, we apply the same transformation estimated by ICP.", "To enable the simulation of the static robot GS, we associate each Gaussian kernel with its corresponding robot link", "through a link segmentation process. After ICP alignment,\neach kernel is assigned to a link by finding its nearest\nneighbor in the sampled robot point cloud and inheriting\nthat point s link index. This process is applied to all links,\nincluding the gripper links, allowing us to render continuous\narm motion as well as gripper opening and closing.", "The same\nprocedure generalizes naturally to other robot embodiments\nwith available URDF models. 3) Color Alignment: A major contributor to the visual gap\nin GS renderings is that reconstructed scenes often lie in a\ndifferent color space from the policy s training data, leading\nto mismatched pixel color distributions, which can affect\npolicy performance. In our setting, GS reconstructions inherit\nthe color characteristics of iPhone video captures, while\npolicies are trained in the color space of the robot s cameras\n(e.", "g. , Intel RealSense, which is known to introduce color\nshifts). To close this gap, we design a color transformation\nthat aligns GS colors to the real camera domain.", "We perform this alignment directly in RGB space. First,\nwe render images from the scene GS at the viewpoints of\nthe fixed real cameras, using the original Gaussian kernel\ncolors and opacities. Next, we capture real images from the\nsame viewpoints, forming paired data for optimization.", "We\nthen solve for a transformation function f that minimizes the\npixel-wise color discrepancy:\n1 N\n f (pi ) qi 2 , pi IGS , qi IRS , (1)\nf F N i=1", "f = arg min", "where IGS and IRS denote GS renderings and real camera captures, N is the number of pixels, pi and qi are corresponding\nRGB values, and F is the function space. We parameterize\nF as the set of degree-d polynomial transformations:\nf = { fi }di=1 , fi R3 ,\nf (pi ) = [ f0 f1 fd ] [1 pi", "(2)\npdi ]T ,", "(3)", "which reduces the problem to a standard least-squares regression. We solve it using Iteratively Reweighted Least Squares\n(IRLS) [76] to improve robustness to outliers. Empirically,\nwe find that a quadratic transform (d = 2) offers the best\ntrade-off between expressivity and overfitting.", "4) Physics and Deformation: With GS reconstruction and\nalignment mitigating the rendering gap, the physics model\nmust accurately capture real-world dynamics. We use a\ncustom physics engine built on NVIDIA Warp [77], extending the PhysTwin [25] spring-mass simulator to support\ncollisions with both robot end-effectors and objects in the\nenvironment. For grasping soft-body digital twins, we avoid\nthe common but unrealistic practice of fixing object nodes\nto the gripper.", "Instead, we model contact purely through\nfrictional interactions between gripper fingers and the object. The gripper closing motion halts automatically once a\nspecified total collision-force threshold is reached, yielding\nmore realistic and stable grasps. At each simulation step, the updated robot and environment states from the physics engine are propagated to the\nGaussian kernels.", "For rigid bodies, including objects and", "robot links, kernel positions and orientations are updated\nusing the corresponding rigid-body transformations. For deformable objects, following PhysTwin [25], we apply Linear\nBlend Skinning (LBS) [78] to transform each kernel based\non the underlying soft-body deformation. Overall, with GS rendering, the physics solver, and LBSbased deformation being the major computational steps, our\nsimulator runs at 5 to 30 FPS on a single GPU, depending on\nthe robot-object contact states.", "By eliminating the overhead\nof real-world environment resets and leveraging multi-GPU\nparallelization, we empirically achieve evaluation speeds\nseveral times faster than real-world execution. D. Policy Evaluation\nTo evaluate visuomotor policies in our simulator, we\nfirst design tasks and perform real-world data collection\nand policy training.", "Demonstrations are collected through\nhuman teleoperation using GELLO [79], after which we\nscan the scene to construct the corresponding simulation\nenvironments. All policies are trained exclusively on real\ndata (i. e.", ", no co-training between simulation and reality). To improve consistency and reduce variance, we follow the\npractice of Kress-Gazit et al. [71] by defining a fixed set\nof initial object configurations for each task and performing\nevaluations in both simulation and the real world.", "In the real\nworld, we use a real-time visualization tool that overlays\nsimulated initial states onto live camera streams, enabling\noperators to accurately and consistently reproduce the starting configurations. Policy performance u is measured in terms of binary task\nsuccess rates: in the real world, success is determined by human evaluators, while in simulation, task-specific criteria are\nautomatically computed from privileged simulation states. In\nthis work, we evaluate the performance of several state-ofthe-art imitation learning algorithms, as well as checkpoints\nfrom different training stages for each network.", "Notably,\nthe simulator is readily extensible to other policy types, as\nwe package the entire system into the widely adopted Gym\nenvironment API [26]. We are committed to open-sourcing\nour implementation to encourage community adoption and\nenable scalable, reproducible policy evaluation. IV.", "E XPERIMENTS\nIn this section, we test the performance of imitation\nlearning policies in both the real world and our simulation\nenvironment to examine the correlation. We aim to address\nthe following questions: (1) How strongly do the simulation\nand real-world performance correlate? (2) How critical are\nrendering and dynamics fidelity for improving this correlation?", "(3) What practical benefits can the correlation provide? A. Experiment Setup\n1) Tasks: We evaluate policies on three representative manipulation tasks involving both deformable and rigid objects:\n Toy packing: The robot picks up a plush sloth toy from\nthe table and packs it into a small plastic box.", "A trial is\nconsidered successful only if the toy s arms, legs, and", "Toy packing\nr = 0.944", "Rope routing\nr = 0.901", "T-block pushing\nr = 0.915", "Ours vs. Isaac baseline\nr1 = 0.904\nr2 = 0.268", "Fig. 3: Correlation between simulation and real-world policy performance. Left: Simulation success rates (y-axis) vs. real-world\nsuccess rates (x-axis) for toy packing, rope routing, and T-block pushing, across multiple state-of-the-art imitation learning policies and\ncheckpoints. The tight clustering along the diagonal indicates that, even with binary success metrics, our simulator faithfully reproduces\nreal-world behaviors across tasks and policy robustness levels. Right: Compared with IsaacLab, which models rope routing and push-T\ntasks, our approach yields substantially stronger sim-to-real correlation, highlighting the benefit of realistic rendering and dynamics.", "Toy Packing - DP", "Toy Packing - SmolVLA", "Rope Routing - ACT", "Rope Routing - Pi-0", "T-Block Pushing - DP", "T-Block Pushing - Pi-0", "Fig. 4: Per-policy, per-task performance across training. xaxis: training iterations, y-axis: success rates. Simulation (blue)\nand real-world (orange) success rates are shown across iterations.\nUnlike Figure 3, which aggregates across policies, this figure\nshows unrolled curves for each task-policy pair. Improvements in\nsimulation consistently correspond to improvements in the real\nworld, establishing a positive correlation and demonstrating that our\nsimulator can be a reliable tool for evaluating/selecting policies.", "body are fully contained within the box, with no parts\nprotruding.\n Rope routing: The robot grasps a cotton rope, lifts it, and\nroutes it through a 3D-printed clip. Success is defined\nby the rope being fully threaded into the clip.\n T-block pushing (push-T): A 3D-printed T-shaped block\nis placed on the table. Using a vertical cylindrical\npusher, the robot must contact the block and then\ntranslate and reorient it to match a specified target pose.\nBoth the toy packing and rope routing tasks are challenging because the small tolerances of the box and clip require", "the policy to leverage visual feedback. Similarly, in push-T,\nthe policy must infer the block s pose from images to achieve\nthe required translation and reorientation. 2) Evaluation: To reduce variance and ensure systematic\nevaluation, we initialize scenes from a fixed set of configurations shared between the simulation and the real world.", "These initial configurations are generated in our simulator\nby constructing a grid over the planar position (x, y) and\nrotation angle θ of objects placed on the table. The grid\nranges are chosen to ensure that the evaluation set provides\ncoverage comparable to the training distribution. In the real\nworld, objects are positioned to replicate the corresponding\ngrid states.", "We use an evaluation set size of 20, 27, and 16\nfor toy packing, rope routing, and push-T, respectively. We use binary success criteria for all tasks. Following [19],\nwe quantify the alignment between simulation and real-world\nperformance using the Mean Maximum Rank Variation\n(MMRV) and the Pearson correlation coefficient (r).", "The number of evaluation episodes plays a critical role in\nthe uncertainty of measured success rates [11]. To capture\nthis variability, we report uncertainty in our results using the\nClopper Pearson confidence interval (CI). We also visualize the Bayesian posterior of policy success rates under a\nuniform Beta prior with violin plots.", "We evaluate four state-of-the-art imitation learning policies: ACT [1], DP [2], SmolVLA [4], and Pi-0 [3]. The\nreal-world setup consists of a single UFactory xArm 7 robot\narm equipped with two calibrated Intel RealSense RGB-D\ncameras: a D405 mounted on the robot wrist and a D455\nmounted on the table as a fixed external camera. All policies\ntake as input images from both camera views, along with\nthe current end-effector state.", "For push-T, the end-effector\nstate includes only the 2D position (x, y); for the other\ntasks, it additionally includes the position, rotation, and\ngripper openness. Across all tasks, we collect 39-60 successful demonstrations via teleoperation using GELLO [79]. Training is performed using the open-source LeRobot [80]\nimplementation, except for Pi-0, where we adopt the original\nimplementation [3] for better performance.", "Toy packing\nRope routing\nT-block pushing", "Real world", "Ours", "Ours - w/o phys. opt.", "Ours - w/o color align", "IsaacLab", "Fig. 5: Comparison of rendering and dynamics quality. Real-world observations (left) compared with our method, two ablations, and the\nIsaacLab baseline across three tasks. From right to left, visual and physical fidelity progressively improve. Without physics optimization,\nobject dynamics deviate, causing failures such as the toy s limbs not fitting into the box or the rope slipping before routing. Without color\nalignment, rendered images exhibit noticeable color mismatches. The IsaacLab baseline (rightmost) shows lower realism in both rendering\nand dynamics compared to our approach.\nToy packing", "B. Baseline\nAs a baseline, we use NVIDIA IsaacLab [13] as the\nsimulation environment. Robot and environment assets are\nimported and aligned in position and color to match the\nreal-world setup.", "IsaacLab provides a general-purpose robot\nsimulation framework built on the PhysX physics engine, but\nits support for deformable objects remains limited. For ropes,\nwe approximate deformable behavior using an articulated\nchain structure. However, for the plush toy, realistic grasping\nand deformation could not be stably simulated, making task\ncompletion infeasible; we therefore excluded this task from\nour quantitative comparisons.", "C. Sim-and-Real Correlation\nFigure 3 (left) shows the performance of all policy checkpoints in both simulation and the real world. We observe a\nstrong correlation: policies that achieve higher success rates\nin reality also achieve higher success rates in our simulator,\nconsistently across architectures and tasks.", "Figure 3 (right)\nfurther highlights that our simulator achieves stronger correlation than the IsaacLab baseline [13]. This is also confirmed\nby the quantitative results in Table I, with our simulator\nachieving a Pearson coefficient r > 0. 9 for all policies.", "By\ncontrast, the baseline yields only r = 0. 649 on push-T, and an\neven lower r = 0. 237 on rope routing as a result of the larger\ndynamics gap.", "The low MMRV value for the IsaacLab rope\nrouting task arises from its consistently low success rates,\nwhich in turn produce fewer ranking violations. D. Policy Performance Analysis\nFigure 4 further illustrates per-policy, per-task performance curves across training iterations.", "We observe that\nsimulation success rates generally follow the same progression as real-world success rates, further highlighting\nthe correlation. For example, in the toy packing-DP case,\nboth simulation and real success rates peak at iteration\n5,000 and decline significantly by iteration 7,000. Similarly,", "IsaacLab [13]\nOurs w/o color\nOurs w/o phys.\nOurs", "Rope routing", "T-block pushing", "MMRV", "r", "MMRV", "r", "MMRV", "r", "0.200\n0.200\n0.087", "0.805\n0.694\n0.944", "0.022\n0.156\n0.119\n0.096", "0.237\n0.714\n0.832\n0.901", "0.031\n0.031\n0.031\n0.000", "0.649\n0.529\n0.905\n0.915", "TABLE I: Quantitative comparison of correlation. Ours w/o\ncolor: our method without color alignment. Ours w/o phys.: our\nmethod without physics optimization. Lower MMRV indicates\nfewer errors in ranking policy performance, while higher r reflects\nstronger statistical correlation. Best results are highlighted in bold.", "in the rope routing-Pi-0 case, performance peaks around\niteration 20,000. These results suggest that our simulator can\nbe used as a practical tool for monitoring policy learning\ndynamics, selecting checkpoints for real-world testing, and\nsetting approximate expectations for real-world performance. In cases where simulation and real success rates do not\noverlap, such as toy packing-SmolVLA and rope routingACT, the simulator still captures the correct performance\ntrend, even if the absolute success rates differ.", "We attribute\nthese discrepancies to residual gaps in visual appearance and\ndynamics, as well as variance from the limited number of\nevaluation episodes (16 27 per checkpoint). E. Ablation Study\nTo measure the importance of the rendering and dynamics\nrealism for our Gaussian Splatting simulator, we perform\nablation studies on the correlation metrics MMRV and r.", "We provide two ablated variants of our simulation:\n Ours w/o color alignment: we skip the color alignment\nstep in simulation construction and use the original GS\ncolors in the iPhone camera space, creating a mismatch\nin the appearance. Ours w/o physics optimization: instead of using the\nfully-optimized spring stiffness Y , we use a global\nstiffness value shared across all springs. The global\nvalue is given by the gradient-free optimization stage", "in PhysTwin [25]. For push-T, we keep its rigidity and\nchange its friction coefficients with the ground and the\nrobot to create a mismatch in dynamics. Figure 5 presents a visual comparison between our simulator, its ablated variants, and the baseline, using the same\npolicy model and identical initial states.", "Our full method\nachieves the best rendering and dynamics fidelity, resulting\nin policy rollouts that closely match real-world outcomes. In contrast, the w/o physics optimization variant produces\ninaccurate object dynamics, while the w/o color alignment\nvariant shows clear color mismatches. Empirically, both dynamics and appearance mismatches\nlead to deviations between simulated and real policy rollouts,\nthough policies exhibit different sensitivities to each type of\ngap.", "For example, in the rope routing task, the rope fails to\nenter the clip when stiffness is mis-specified (w/o physics\noptimization). In the push-T task, color discrepancies alter\nthe robot s perception, causing it to push the block differently\n(w/o color alignment). Table I details the quantitative results.", "Overall, our full\nmethod achieves the highest correlation values, outperforming the ablated variants. In particular, lower MMRV values\nreflect more accurate policy ranking, while higher Pearson\ncorrelation coefficients (r) indicate stronger and more consistent correlations without being influenced by outlier points. V.", "C ONCLUSION\nIn this work, we introduced a framework for evaluating\nrobot manipulation policies in a simulator that combines\nGaussian Splatting-based rendering with real-to-sim digital\ntwins for deformable object dynamics. By addressing both\nappearance and dynamics, our simulator narrows the sim-toreal gap through physics-informed reconstruction, positional\nand color alignment, and deformation-aware rendering. We demonstrated the framework on representative deformable and rigid body manipulation tasks, evaluating several state-of-the-art imitation learning policies.", "Our experiments show that policy success rates in simulation exhibit\nstrong correlations with real-world outcomes (r > 0. 9). Further analysis across highlights that our simulator can predict\npolicy performance trends, enabling it to serve as a practical\nproxy for checkpoint selection and performance estimation.", "We found that both physics optimization and color alignment\nare critical for closing policy performance gaps. In future work, scaling both simulation and evaluation to\nlarger task and policy sets could provide deeper insights into\nthe key design considerations for policy evaluation simulators. Moreover, our real-to-sim framework can be generalized to more diverse environments, supporting increasingly\ncomplex robot manipulation tasks.", "ACKNOWLEDGMENT\nThis work is partially supported by the DARPA TIAMAT\nprogram (HR0011-24-9-0430), NSF Award #2409661, Toyota Research Institute (TRI), Sony Group Corporation, Samsung Research America (SRA), Google, Dalus AI, Pickle\nRobot, and an Amazon Research Award (Fall 2024). This", "article solely reflects the opinions and conclusions of its\nauthors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of\nthe sponsors.\nWe would like to thank Wenhao Yu, Chuyuan Fu, Shivansh\nPatel, Ethan Lipson, Philippe Wu, and all other members of\nthe RoboPIL lab at Columbia University and SceniX Inc. for\nhelpful discussions and assistance throughout the project.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]", "T. Z. Zhao, V.", "Kumar, S. Levine, and C. Finn, Learning\nfine-grained bimanual manipulation with low-cost hardware,\n2023.", "arXiv: 2304. 13705 [cs. RO].", "C. Chi et al. , Diffusion policy: Visuomotor policy learning\nvia action diffusion, in RSS, 2023.", "K. Black et al. , π0 : A vision-language-action flow model\nfor general robot control, 2024.", "arXiv: 2410 . 24164\n[cs. LG].", "M. Shukor et al. , Smolvla: A vision-language-action model\nfor affordable and efficient robotics, 2025.", "arXiv: 2506 . 01844 [cs. LG].", "C. Chi et al. , Universal manipulation interface: In-the-wild\nrobot teaching without in-the-wild robots, in RSS, 2024.", "T. Lin, K. Sachdev, L.", "Fan, J. Malik, and Y. Zhu, Simto-real reinforcement learning for vision-based dexterous\nmanipulation on humanoids, arXiv:2502.", "20396, 2025. B. Tang et al.", ", Industreal: Transferring contact-rich assembly\ntasks from simulation to reality, 2023. arXiv: 2305. 17110\n[cs.", "RO]. A. Brohan et al.", ", Rt-2: Vision-language-action models transfer web knowledge to robotic control, in arXiv preprint\narXiv:2307. 15818, 2023. P.", "Intelligence et al. , π0. 5 : A vision-language-action model\nwith open-world generalization, 2025.", "arXiv: 2504. 16054\n[cs. LG].", "NVIDIA et al. , GR00T N1: An open foundation model for\ngeneralist humanoid robots, in ArXiv Preprint, Mar. 2025.", "arXiv: 2503. 14734. T.", "L. Team et al. , A careful examination of large behavior\nmodels for multitask dexterous manipulation, 2025.", "arXiv:\n2507. 05331 [cs. RO].", "G. R. Team et al.", ", Gemini robotics: Bringing ai into the\nphysical world, 2025. arXiv: 2503. 20020 [cs.", "RO]. NVIDIA, NVIDIA Isaac Sim, 2024. E.", "Todorov, T. Erez, and Y. Tassa, Mujoco: A physics\nengine for model-based control, in IROS, 2012, pp.", "5026 \n5033. F. Xiang et al.", ", SAPIEN: A simulated part-based interactive\nenvironment, in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020. C.", "Li et al. , Behavior-1k: A human-centered, embodied\nai benchmark with 1,000 everyday activities and realistic\nsimulation, 2024. arXiv: 2403.", "09227 [cs. RO]. G.", "Authors, Genesis: A generative and universal physics\nengine for robotics and beyond, Dec. 2024. R.", "Tedrake, Drake: Model-based design and verification for\nrobotics, 2019. X. Li et al.", ", Evaluating real-world robot manipulation\npolicies in simulation, in CoRL, 2024. B. Kerbl, G.", "Kopanas, T. Leimku hler, and G. Drettakis, 3d\ngaussian splatting for real-time radiance field rendering, \nACM Transactions on Graphics, vol.", "42, no. 4, Jul. 2023.", "J. Abou-Chakra et al. , Real-is-sim: Bridging the sim-to-real\ngap with a dynamic digital twin, 2025.", "arXiv: 2504. 03597\n[cs. RO].", "[22]", "[23]\n[24]", "[25]\n[26]\n[27]\n[28]\n[29]\n[30]\n[31]", "[32]", "[33]", "[34]\n[35]\n[36]\n[37]\n[38]", "[39]\n[40]\n[41]\n[42]\n[43]", "M. N. Qureshi, S.", "Garg, F. Yandun, D. Held, G.", "Kantor,\nand A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb\nmanipulation policies using gaussian splatting, 2024. arXiv:\n2409.", "10161 [cs. RO]. X.", "Li et al. , Robogsim: A real2sim2real robotic gaussian\nsplatting simulator, 2024. arXiv: 2411.", "11839 [cs. RO]. L.", "Barcellona, A. Zadaianchuk, D. Allegro, S.", "Papa, S. Ghidoni, and E. Gavves, Dream to manipulate: Compositional\nworld models empowering robot imitation learning with\nimagination, 2025.", "arXiv: 2412. 14957 [cs. RO].", "H. Jiang, H. -Y.", "Hsu, K. Zhang, H. -N.", "Yu, S. Wang, and Y. Li,\n Phystwin: Physics-informed reconstruction and simulation\nof deformable objects from videos, ICCV, 2025.", "G. Brockman et al. , Openai gym, 2016.", "arXiv: 1606 . 01540 [cs. LG].", "Octo Model Team et al. , Octo: An open-source generalist\nrobot policy, in Proceedings of Robotics: Science and\nSystems, Delft, Netherlands, 2024. J.", "Wang, M. Leonard, K. Daniilidis, D.", "Jayaraman, and E. S. Hu, Evaluating pi0 in the wild: Strengths, problems, and the\nfuture of generalist robot policies, 2025.", "A. Padalkar et al. , Open x-embodiment: Robotic learning\ndatasets and rt-x models, arXiv preprint arXiv:2310.", "08864,\n2023. A. Khazatsky et al.", ", Droid: A large-scale in-the-wild robot\nmanipulation dataset, 2024. B. Calli, A.", "Walsman, A. Singh, S. Srinivasa, P.", "Abbeel,\nand A. M. Dollar, Benchmarking in manipulation research:\nUsing the yale-cmu-berkeley object and model set, IEEE\nRobotics & Automation Magazine, vol.", "22, no. 3, pp. 36 52,\nSep.", "2015. K. Van Wyk, J.", "Falco, and E. Messina, Robotic grasping\nand manipulation competition: Future tasks to support the\ndevelopment of assembly robotics, in Robotic Grasping and\nManipulation Challenge, Springer, 2016, pp. 190 200.", "N. Correll et al. , Analysis and observations from the first\namazon picking challenge, IEEE Transactions on Automation Science and Engineering, vol.", "15, no. 1, pp. 172 188,\n2018.", "G. Zhou et al. , Train offline, test online: A real robot learning\nbenchmark, 2023.", "arXiv: 2306. 00942 [cs. RO].", "S. Dasari et al. , Rb2: Robotic manipulation benchmarking\nwith a twist, 2022.", "arXiv: 2203. 08098 [cs. RO].", "S. Tao et al. , Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, RSS,\n2025.", "S. James, Z. Ma, D.", "R. Arrojo, and A. J.", "Davison, Rlbench:\nThe robot learning benchmark & learning environment,\n2019. arXiv: 1909. 12271 [cs.", "RO]. S. Srivastava et al.", ", Behavior: Benchmark for everyday\nhousehold activities in virtual, interactive, and ecological\nenvironments, in CoRL, A. Faust, D. Hsu, and G.", "Neumann,\nEds. , ser. PMLR, vol.", "164, Aug. 2022, pp. 477 490.", "X. Puig et al. , Habitat 3.", "0: A co-habitat for humans, avatars\nand robots, 2023. arXiv: 2310. 13724 [cs.", "HC]. S. Nasiriany et al.", ", Robocasa: Large-scale simulation of\neveryday tasks for generalist robots, in RSS, 2024. Y. Zhu et al.", ", Robosuite: A modular simulation framework\nand benchmark for robot learning, 2025. arXiv: 2009 . 12293 [cs.", "RO]. A. Mandlekar et al.", ", Mimicgen: A data generation system for\nscalable robot learning using human demonstrations, 2023. arXiv: 2310. 17596 [cs.", "RO]. X. Yang, C.", "Eppner, J. Tremblay, D. Fox, S.", "Birchfield, and\nF. Ramos, Robot policy evaluation for sim-to-real transfer:\nA benchmarking perspective, 2025. arXiv: 2508 .", "11117\n[cs. RO].", "[44]\n[45]\n[46]\n[47]\n[48]\n[49]\n[50]\n[51]\n[52]\n[53]\n[54]\n[55]", "[56]", "[57]\n[58]\n[59]\n[60]\n[61]", "[62]\n[63]\n[64]\n[65]", "Y. R. Wang et al.", ", Roboeval: Where robotic manipulation meets structured and scalable evaluation, 2025. arXiv:\n2507. 00435 [cs.", "RO]. X. B.", "Peng, M. Andrychowicz, W. Zaremba, and P.", "Abbeel,\n Sim-to-real transfer of robotic control with dynamics randomization, in ICRA, IEEE, 2018, pp. 3803 3810. Y.", "Chebotar et al. , Closing the sim-to-real loop: Adapting\nsimulation randomization with real world experience, in\nICRA, IEEE, 2019, pp. 8973 8979.", "OpenAI et al. , Solving rubik s cube with a robot hand, 2019. arXiv: 1910.", "07113 [cs. LG]. D.", "Ho, K. Rao, Z. Xu, E.", "Jang, M. Khansari, and Y. Bai, Retinagan: An object-aware approach to sim-to-real\ntransfer, 2021.", "arXiv: 2011. 03148 [cs. RO].", "S. Liu, Z. Ren, S.", "Gupta, and S. Wang, Physgen: Rigid-body\nphysics-grounded image-to-video generation, in ECCV,\nSpringer, 2024, pp. 360 378.", "B. Chen et al. , Physgen3d: Crafting a miniature interactive\nworld from a single image, in CVPR, 2025, pp.", "6178 6189. Y. Jiang et al.", ", Vr-gs: A physical dynamics-aware interactive\ngaussian splatting system in virtual reality, in SIGGRAPH,\n2024, pp. 1 1. T.", "Xie et al. , Physgaussian: Physics-integrated 3d gaussians\nfor generative dynamics, in CVPR, 2024, pp. 4389 4398.", "R. -Z. Qiu, G.", "Yang, W. Zeng, and X. Wang, Feature splatting: Language-driven physics-based scene synthesis and\nediting, 2024.", "arXiv: 2404. 01223 [cs. CV].", "B. Bianchini, M. Zhu, M.", "Sun, B. Jiang, C. J.", "Taylor, and\nM. Posa, Vysics: Object reconstruction under occlusion by\nfusing vision and contact-rich physics, in RSS, Jun. 2025.", "W. Yang, Z. Xie, X.", "Zhang, H. B. Amor, S.", "Lin, and W. Jin,\nTwintrack: Bridging vision and contact physics for real-time\ntracking of unknown dynamic objects, 2025. arXiv: 2505.", "22882 [cs. RO]. J.", "Abou-Chakra, K. Rana, F. Dayoub, and N.", "Suenderhauf,\n Physically embodied gaussian splatting: A visually learnt\nand physically grounded 3d representation for robotics, in\nCoRL, 2024. K. -T.", "Yu, M. Bauza, N. Fazeli, and A.", "Rodriguez, More than\na million ways to be pushed. a high-fidelity experimental\ndataset of planar pushing, in IROS, IEEE, 2016, pp. 30 37.", "T. Pfaff, M. Fortunato, A.", "Sanchez-Gonzalez, and P. W. Battaglia, Learning mesh-based simulation with graph networks, 2021.", "arXiv: 2010. 03409 [cs. LG].", "K. Zhang, B. Li, K.", "Hauser, and Y. Li, Adaptigraph:\nMaterial-adaptive graph-based neural dynamics for robotic\nmanipulation, in RSS, 2024. K.", "Zhang, B. Li, K. Hauser, and Y.", "Li, Particle-grid neural\ndynamics for learning deformable object models from rgb-d\nvideos, in RSS, 2025. T. Tian, H.", "Li, B. Ai, X. Yuan, Z.", "Huang, and H. Su,\n Diffusion dynamics models with generative state estimation\nfor cloth manipulation, arXiv preprint arXiv:2503. 11999,\n2025.", "X. Li et al. , Pac-nerf: Physics augmented continuum neural\nradiance fields for geometry-agnostic system identification, \narXiv preprint arXiv:2303.", "05512, 2023. T. Zhang et al.", ", Physdreamer: Physics-based interaction\nwith 3d objects via video generation, in ECCV, Springer,\n2024, pp. 388 406. L.", "Zhong, H. -X. Yu, J.", "Wu, and Y. Li, Reconstruction and\nsimulation of elastic objects with spring-mass 3d gaussians, \nin ECCV, Springer, 2024, pp. 407 423.", "C. Chen et al. , Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free\nsimulation, in CVPR, 2025, pp.", "26 545 26 555.", "[66]\n[67]\n[68]\n[69]\n[70]\n[71]\n[72]\n[73]\n[74]", "[75]", "[76]", "[77]", "[78]\n[79]\n[80]", "X. Han et al. , Re3 sim: Generating high-fidelity simulation\ndata via 3d-photorealistic real-to-sim for robotic manipulation, arXiv preprint arXiv:2502.", "08645, 2025. A. Escontrela et al.", ", Gaussgym: An open-source real-tosim framework for learning locomotion from pixels, arXiv\npreprint arXiv:2510. 15352, 2025. J.", "Yu et al. , Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware, 2025. arXiv: 2505.", "09601 [cs. RO]. S.", "Yang et al. , Novel demonstration generation with gaussian splatting enables robust one-shot manipulation, arXiv\npreprint arXiv:2504. 13175, 2025.", "G. Jiang et al. , Gsworld: Closed-loop photo-realistic simulation suite for robotic manipulation, 2025.", "arXiv: 2510. 20813 [cs. RO].", "H. Kress-Gazit et al. , Robot learning as an empirical\nscience: Best practices for policy evaluation, arXiv preprint\narXiv:2409.", "09491, 2024. Niantic, Scaniverse, https://scaniverse. com/.", "PlayCanvas and Snap Inc. , Supersplat, https : / /\ngithub. com/playcanvas/supersplat, [Computer\nsoftware], 2025.", "K. S. Arun, T.", "S. Huang, and S. D.", "Blostein, Least-squares\nfitting of two 3-d point sets, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PAMI-9, no. 5,\npp.", "698 700, 1987. M. A.", "Fischler and R. C. Bolles, Random sample consensus:\nA paradigm for model fitting with applications to image analysis and automated cartography, Commun.", "ACM, vol. 24,\nno. 6, pp.", "381 395, Jun. 1981. P.", "J. Green, Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant\nalternatives, Journal of the Royal Statistical Society: Series\nB (Methodological), vol. 46, no.", "2, pp. 149 170, 1984. M.", "Macklin, Warp: A high-performance python framework\nfor gpu simulation and graphics, https : / / github . com/nvidia/warp, NVIDIA GPU Technology Conference (GTC), Mar. 2022.", "R. W. Sumner, J.", "Schmid, and M. Pauly, Embedded deformation for shape manipulation, vol. 26, no.", "3, 80 es, Jul. 2007. P.", "Wu, Y. Shentu, Z. Yi, X.", "Lin, and P. Abbeel, Gello: A\ngeneral, low-cost, and intuitive teleoperation framework for\nrobot manipulators, in IROS, 2024. R.", "Cadene et al. , Lerobot: State-of-the-art machine learning\nfor real-world robotics in pytorch, https : / / github . com/huggingface/lerobot, 2024.", "A PPENDIX\nContents\nAppendix I: Additional Technical Details\nI-A\nPlatform and Tasks . . . . . . . . . .\nI-A.1\nRobot Setup . . . . . . . .\nI-A.2\nData Collection . . . . . .\nI-A.3\nTask Definition . . . . . .\nI-B\nSimulation . . . . . . . . . . . . . . .\nI-B.1\nAssets . . . . . . . . . . .\nI-B.2\nPositional Alignment . . .\nI-B.3\nColor Alignment . . . . . .\nI-B.4\nPhysTwin Training . . . . .\nI-B.5\nSimulation Loop . . . . . .\nI-C\nPolicy Training . . . . . . . . . . . .\nI-C.1\nDatasets . . . . . . . . . .\nI-C.2\nNormalizations . . . . . . .\nI-C.3\nImage Augmentations . . .\nI-C.4\nHyperparameters . . . . . .\nI-D\nEvaluation . . . . . . . . . . . . . . .\nI-D.1\nEvaluation Protocol . . . .\nI-D.2\nEpisode Settings . . . . . .\nI-D.3\nSuccess Criteria . . . . . .", "10\n10\n10\n10\n10\n11\n11\n11\n11\n11\n12\n12\n12\n12\n12\n12\n12\n12\n12\n12", "Appendix II: Additional Results\nII-A\nScaling up Simulation Evaluation . . .\nII-B\nReplaying Real Rollouts . . . . . . .\nII-C\nAdditional Qualitative Results . . . .", "13\n13\n13\n14", "A PPENDIX I\nA DDITIONAL T ECHNICAL D ETAILS\nA. Platform and Tasks\n1) Robot Setup: We use a UFactory xArm 7 robot\nmounted on a tabletop. The robot arm has 7 degrees of\nfreedom.", "The robot end-effector can be interchanged between\nthe standard xArm gripper and a custom 3D-printed pusher,\ndepending on the task. Two Intel RealSense RGB-D cameras\nare connected to the robot workstation: a D455 fixed on the\ntable overlooking the workspace, and a D405 mounted on the\nrobot wrist via a custom 3D-printed clip. To ensure consistent\nappearance between real and simulated observations, we fix\nthe white balance and exposure settings of both cameras.", "2) Data Collection: We use GELLO for data collection. GELLO [79] streams high-frequency joint-angle commands\nto the robot, which we execute using joint-velocity control\nfor smooth motion tracking. At each timestep, the robot computes the difference between the commanded and measured\njoint angles, then sets each joint s target angular velocity\nproportional to this delta.", "To prevent abrupt movements, the\nvelocity vector is normalized such that its total 2 norm does\nnot exceed a predefined limit. This approach enables stable\nand continuous trajectory following without jerky motions. During policy evaluation, we apply the same control strategy,\nensuring that the policy outputs are tracked consistently in\nboth real and simulated environments.", "(a) Training initial state distributions", "(b) Evaluation initial state distributions", "Fig. 6: Training and evaluation data distributions. Top: spatial\ncoverage of initial states in the training set. Bottom: the corresponding coverage in the evaluation set.\nName", "Dynamics Type", "3D Representation", "xArm-gripper-tabletop\nxArm-pusher-tabletop\nPlush sloth\nRope\nT-block\nBox\nClip", "Articulated+Fixed\nArticulated+Fixed\nDeformable\nDeformable\nRigid\nFixed\nFixed", "GS+URDF+Mesh\nGS+URDF+Mesh\nGS+PhysTwin\nGS+PhysTwin\nGS+PhysTwin\nGS+Mesh\nGS+Mesh", "TABLE II: Simulation assets. Each row corresponds to an individual Gaussian Splatting scan, specifying its dynamics type in\nsimulation and the 3D representation used for physical simulation\nand rendering. These assets are combined to instantiate all three\nmanipulation tasks within the simulator.", "3) Task Definition: To evaluate the effectiveness of our\nsimulator, we select a set of rigid- and soft-body manipulation tasks that require the policy to leverage object dynamics\nwhile incorporating visual feedback. The formulation and\nsetup of each task are described as follows. a) Toy Packing: The robot grasps the plush toy by one\nof its limbs, lifts it above the box, and adjusts its pose such\nthat the arm and leg on one side hang into the box.", "The\nrobot then tilts the toy slightly to allow the other side s limbs\nto enter, before lowering it further to pack the toy snugly\ninside the box. Because the box is intentionally compact, the\nrobot must adapt to the toy s pose to successfully execute the\npacking motion without leaving any limbs protruding over\nthe box edges. A total of 39 human demonstration episodes\nare recorded for this task.", "b) Rope Routing: The robot grasps one end of the rope\n(marked with red rubber bands), lifts it, and positions it\nabove the cable holder before lowering it to gently place\nthe rope into the slot. Because the rope holder contact point\nis offset from the grasp location, the rope dynamics play a\ncritical role in determining the appropriate displacement and\ntrajectory required for successful placement. A total of 56\nhuman demonstration episodes are collected for this task.", "c) T-block Pushing: The robot begins with the pusher\npositioned above an orange marker on the table, while\nthe end-effector s z-coordinate remains fixed throughout the\nmotion. The robot must move to the T-block s location and\npush it toward a predefined goal region. The goal is not\nphysically marked in the workspace but is visualized as a\nyellow translucent mask overlaid on the fixed-camera images.", "Robot pose 2", "Robot pose 3", "Robot pose 4", "Robot pose 5", "Sim before\nalignment", "Real\n(RealSense)", "Robot pose 1", "Sim after\nalignment", "(a) Training initial state distributions", "Fig. 7: Color alignment.\nFive\nimage\nused for the color alignment process are shown. Top: real images captured by the RealSense\n(b) Evaluation\ninitial\nstatepairs\ndistributions\ncameras. Middle: raw Gaussian Splatting renderings with the robot posed identically to theRope\nreal PhysTwin\nimages. Bottom:\nGS renderings after\ntraining video\napplying the optimized color transformation, showing improved consistency with real-world color appearance.", "t", "B. Simulation", "Algorithm 1: Simulation Loop\nData: PhysTwin particle positions and velocities x, v,\nPhysTwin spring-mass parameters P, robot\nmesh R, robot motion a, static meshes M1:k ,\nground plane L, total timestep T , substep\ncount N, Gaussians G\nfor t 0 to T 1 do\nx , v = xt , vt\nR 1:N = interpolate robot states(Rt , at )\nfor τ 0 to N 1 do\nv = step springs(x , v , P)\nv = self collision(x , v , P)\nx , v = robot mesh collision(x , v , Rτ , aτ )\nfor i 1 to k do\nx , v = fixed mesh collision(x , v , Mi )\nend\nx , v = ground collision(x , v , L)\nend\nxt+1 , vt+1 = x , v \nRt+1 = R N\nGt+1 = renderer update(Gt , xt , xt+1 , Rt , Rt+1 )\nend", "1) Assets: A summary of the simulation assets used in our\nexperiments is provided in Table II. Each asset corresponds\nto a single Gaussian Splatting reconstruction followed by a\npose alignment process. 2) Positional Alignment: To align the robot-scene Gaussian Splatting scan with the robot s URDF model, we first\nperform a coarse manual alignment in SuperSplat [73] to\nroughly match the origins and orientations of the x, y,\nand z axes.", "Next, we manually define a bounding box to\nseparate the robot Gaussians from the scene Gaussians. We\nthen apply ICP registration between two point clouds: one\nformed by the centers of the robot Gaussians, and the other\nby uniformly sampled surface points from the robot URDF\nmesh. The resulting rigid transformation is applied to the\nentire GS, ensuring that both the robot and scene components\nare consistently aligned in the unified coordinate frame.", "3) Color Alignment: The robot scene scan has the most\nsignificant influence on the overall color profile of the\nrendered images. To align its appearance with the RealSense\ncolor space, we apply Robust IRLS with Tukey bi-weight\nto estimate the color transformation. We use five images of\nresolution 848 480 for this optimization. To mitigate the\nimbalance between the dark tabletop and the bright robot\nregions, each pixel is weighted by the norm of its RGB\nvalues, giving higher weight to high-brightness pixels in the\nleast-squares loss. The optimization is run for 50 iterations.\nFigure 7 visualizes the input images and the resulting color\nalignment.\n4) PhysTwin Training: We use the original PhysTwin [25]\ncodebase for training the rope and sloth digital twins. Phys-", "Rope PhysTwin training video", "Plush toy PhysTwin training video", "Fig. 8: PhysTwin training videos. A few representative camera\nframes are shown for each training video, where a human subject\ninteracts with the deformable object by hand. These videos are used\nby PhysTwin to reconstruct the object s geometry and estimate its\nphysical parameters for building the digital twin models.", "The initial positions and orientations of the T-block are\nrandomized, and a total of 60 human demonstration episodes\nare collected for this task.", "Model", "Visual", "State", "Action", "Relative?", "ACT\nDP\nSmolVLA\nPi-0", "mean std\nmean std\nidentity\nmean std", "mean std\nmin max\nmean std\nmean std", "mean std\nmin max\nmean std\nmean std", "False\nFalse\nTrue\nTrue", "TABLE III: Normalization schemes across models. Columns\nindicate the normalization applied to each modality (visual, state,\nand action) and whether the model operates in a relative action\nspace. Mean std denotes standardization to zero mean and unit\nvariance, while min max scales values to [ 1, 1].\nColor Transformations", "Spatial Transformations", "Type", "Range", "Type", "Range", "Brightness\nContrast\nSaturation\nHue\nSharpness", "(0.8, 1.2)\n(0.8, 1.2)\n(0.5, 1.5)\n( 0.05, 0.05)\n(0.5, 1.5)", "Perspective\nRotation\nCrop", "0.025\n[ 5 , 5 ]\n[10, 40] px", "TABLE IV: Image augmentation configuration. For color transformations, numeric ranges denote multiplicative or additive jitter\nfactors applied to image intensities. For spatial transformations,\nranges specify the perturbation magnitudes for projective distortion,\nrotation, and cropping.", "Twin requires only a single multi-view RGB-D video to\nreconstruct object geometry and optimize physical parameters. For data capture, we record using three fixed Intel\nRealSense D455 cameras. The videos for the two objects\nare visualized in Figure 8.", "For the T-block pushing task,\nsince it is a rigid object, we construct the PhysTwin object\nby uniformly sampling points within the mesh, connecting\nthem with springs using a connection radius of 0. 5 and a\nmaximum of 50 neighbors, and assigning a uniform spring\nstiffness of 3 104 to all connections. This setup ensures\nthat the object behaves like a rigid body.", "5) Simulation Loop: The simulation loop, including robot\naction processing, PhysTwin simulation, collision handling,\nand renderer updates, is summarized in Algorithm 1. C. Policy Training\n1) Datasets: To better understand the data distribution\nused for both policy training and evaluation, we visualize\nthe coverage of initial states in Figure 6.", "2) Normalizations: Normalization plays a crucial role in\nensuring stable policy learning and consistent performance\nacross models. For input and output normalization, we\nfollow the conventions defined in each algorithm s original\nimplementation (summarized in Table III). Specifically, the\nmean std scheme standardizes features to zero mean and\nunit variance, whereas the min max scheme scales each\ndimension independently to [ 1, 1].", "For the VLA (SmolVLA and Pi-0) policies, we employ\nrelative actions to encourage more corrective and stable\nbehavior, treating each action as an SE(3) transformation\nof the end-effector pose in the base frame. Inspired by\n[11], we compute both normalization statistics (mean std or\nmin max) over a rolling window corresponding to the action\nchunk size across the entire dataset. Each action within a", "Model\nACT\nDP\nSmolVLA\nPi-0", "Visual Res.", "State Dim.", "Action Dim.", "Tp", "Te", "L: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240\nL: 120 212; H: 240 240", "8\n8\n8\n8", "8\n8\n8\n8", "50\n64\n50\n50", "50\n50\n50\n50", "TABLE V: Observation and action spaces. Low-resolution inputs\nare used for the rope-routing task, while high-resolution inputs\nare used for the other tasks. State and action vectors include endeffector position, quaternion, and gripper state, expressed in either\nabsolute or relative coordinates. Tp and Te denote the prediction\nand execution horizons, respectively.\nVision Backbone", "#V-Params", "#P-Params", "LR", "Batch Size", "#Iters", "ResNet-18 (ACT)\nResNet-18 (DP)\nSmolVLM-2\nPaliGemma (Pi-0)", "18M\n18M\n350M\n260B", "34M\n245M\n100M\n300M", "1 10 5\n1 10 4\n1 10 4\n5 10 5", "512\n512\n128\n8", "7k\n7k\n20k\n30k", "TABLE VI: Training configuration. Model-specific hyperparameters used in policy training. #V-Params and #P-Params denote\nthe number of parameters in the visual encoder and policy head,\nrespectively. LR, Batch Size, and #Iters refer to the learning rate,\nbatch size, and total training iterations.", "chunk is then normalized using its own statistics to maintain\na consistent magnitude in the normalized space mitigating\nthe tendency of later actions in the chunk to exhibit larger\namplitudes. 3) Image Augmentations: To improve visual robustness\nand generalization, we apply a combination of color and\nspatial augmentations to each input image during training. For every image in a training batch, three augmentation\noperations are randomly sampled and composed.", "Table IV\nsummarizes the augmentation types and their corresponding\nparameter ranges. 4) Hyperparameters: A complete overview of the observation and action spaces, as well as the training configurations for each model, is presented in Tables V and VI. For VLA-based policies, we finetune only the action head\n(keeping the pretrained vision-language encoder frozen) on\nour datasets.", "D. Evaluation\n1) Evaluation Protocol: During evaluation, we sample\na fixed set of initial states, and rollout the policies from\nboth sim and real. To ensure that sim and real align with\neach other, we first sample object initial states in simulation\nand render them from the same camera viewpoint as the\nreal-world physical setup.", "Then, we save the set of initial\nframe renderings, and a real-time visualizer overlays these\nsimulated states onto the live camera stream, enabling a\nhuman operator to manually adjust the objects to match the\nsimulated configuration. 2) Episode Settings: In all evaluation experiments in the\nmain paper, the number of episodes for each task and the\ngrid-based initial configuration randomization ranges are set\nas in Table VII. 3) Success Criteria: Real robot experiments typically rely\non human operators to record success and failure counts,\nwhich is tedious and introduces human bias.", "For simulated", "Toy packing", "Rope routing", "T-block pushing", "r = 0.897", "r = 0.918", "r = 0.950", "MMRV=0.092", "MMRV=0.077", "MMRV=0.000", "Fig. 9: Sim-and-real correlations from scaled-up simulation evaluations. Each point represents a policy evaluated on both domains, and\nthe shaded region indicates the 95% confidence interval. Increasing the number of simulated episodes reduces statistical uncertainty and\nyields stable correlation estimates with real-world success rates, with the minimum observed correlation coefficient of 0.897. Compared to\nthe main-paper experiments, the relative ordering of policy checkpoints remains consistent, demonstrating the robustness of the evaluation\nacross larger-scale simulations.\nTask\nToy packing (toy)\nToy packing (box)\nRope routing (rope)\nT-block pushing (T-block)", "Episodes", "x (cm)", "y (cm)", "θ (deg)", "20\n20\n27\n16", "[ 5, 5]\n[ 5, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 3]\n[0, 5]\n[ 5, 5]\n[ 5, 5]", "[ 5, 5]\n[ 5, 5]\n[ 10, 10]\n{ 45, 135}", "TABLE VII: Task randomization ranges used for evaluation.\nFor each task, the initial object configurations are randomized: the\nplush toy and box in toy packing, the rope in rope routing, and the\nT-block in T-block pushing.", "experiments to scale up, automated success criteria are\nnecessary. For all three tasks, we design metrics based on\nsimulation states as follows:\na) Toy Packing: For each frame, we calculate the number of PhysTwin mass particles that fall within an oriented\nbounding box of the box s mesh. Within the final 100\nframes (3.", "3 seconds) of a 15-second episode, if the number\nexceeds a certain threshold for over 30 frames, the episode\nis considered successful. Empirically, the total number of\nPhysTwin points is 3095, and we use a threshold number of\n3050. b) Rope Rouing: For each frame, we calculate the\nnumber of PhysTwin spring segments that pass through the\nopenings of the channel of the clip.", "Within the final 100\nframes (3. 3 seconds) of a 30-second episode, if for both\nopenings and more than 30 frames, the number of the spring\nsegments that cross the opening is over 100, that indicates\na sufficient routing through the clip and the episode is\nconsidered successful. c) T-block Pushing: For each frame, we calculate the\nmean squared Euclidean distance between the current PhysTwin particles and the target-state PhysTwin particles.", "Within\nthe final 100 frames (3. 3 seconds) of a 60-second episode,\nif the mean squared distance is less than 0. 002, the episode\nis considered successful.", "A PPENDIX II\nA DDITIONAL R ESULTS\nA. Scaling up Simulation Evaluation\nIn the main paper, we evaluate each policy in simulation\nusing an identical set of initial states as in the real-world", "experiments. This design controls for randomness but limits\nthe number of available trials and thus results in high statistical uncertainty, as reflected by the wide Clopper-Pearson\nconfidence intervals. To account for the distributional differences introduced\nby uniformly sampling within the randomization range, we\nadopt slightly modified randomization settings compared\nto the grid-range experiments in the main paper.", "In the\ntoy packing task, we use the same randomization range\nas described previously. For the rope routing task, we enlarge the x, y, θ randomization ranges to [ 7. 5, 7.", "5] cm and\n[ 15, 15] degrees, respectively. For the T-block pushing task,\nwe enlarge the x and y range to [ 7. 5, 7.", "5] cm. To better estimate the asymptotic correlation between\nsimulation and real-world performance, we further scale\nup the number of simulation evaluations by sampling 200\nrandomized initial states from the task distribution. Figure 9\nreports the resulting correlations between the scaled-up simulation metrics and real-world success rates.", "We observe that the confidence intervals are significantly\nnarrowed down, and the correlation estimates stabilize as\nthe number of simulation episodes increases, suggesting that\nsimulation fidelity becomes a reliable predictor of real-world\noutcomes when averaged across diverse task instances. B. Replaying Real Rollouts\nTo further assess correspondence between our simulation\nand the real world, we perform replay-based evaluations,\nwhere real-world rollouts during policy inference are reexecuted in the simulator using the same control commands.", "This allows us to disentangle dynamic discrepancies from\nappearance gaps, i. e. , the difference in policy behaviors\nintroduced by differences in perceived images is eliminated.", "In total, we replay the real-world rollouts of 16 checkpoints each with 20 episodes for toy packing, 15 checkpoints\neach with 27 episodes for rope routing, and 12 checkpoints\neach with 16 episodes for T-block pushing. The object\nstates in simulation are initialized to be identical to the\ncorresponding real episodes.", "Toy packing", "Rope routing", "T-block pushing", "r = 0.880", "r = 0.887", "r = 0.944", "MMRV=0.050", "MMRV=0.093", "MMRV=0.000", "Fig. 10: Sim-and-real correlations from replaying real-world rollouts. Each point corresponds to a replay of a real-world policy\ncheckpoint s evaluation results using identical control commands and camera trajectories within the simulator. The success rates are\naveraged over all episodes for each checkpoint. The resulting alignment highlights the degree to which our simulator reproduces the\nobserved real-world outcomes.\nToy packing", "Replay +\nReplay", "Rope routing", "GT +", "GT", "106\n25", "37\n132", "Replay +\nReplay", "T-block pushing", "GT +", "GT", "276\n24", "28\n77", "Replay +\nReplay", "GT +", "GT", "63\n17", "1\n111", "TABLE VIII: Per-episode replay result. We calculate the per-episode correlation between the replayed result and the real-world ground\ntruth. Each subtable shows a 2 2 confusion matrix for each task (TP, FP, FN, TN), where rows indicate replay outcomes and columns\nindicate ground truth. Each entry records the total number of episodes, summed across all policy checkpoints. The strong diagonal\ndominance reflects high sim real agreement in replayed trajectories.", "Figure 10 shows the resulting correlations, and Table VIII\nreports the per-episode replay statistics. Across all three\ntasks, the confusion matrices exhibit strong diagonal dominance, indicating high agreement between replayed and real\noutcomes. Notably, for toy packing, false positives (replayed success\nbut real failure) are more frequent than false negatives,\nreflecting that the simulator tends to slightly overestimate\nsuccess, likely due to simplified contact or friction models.", "For T-block pushing, false negatives are more frequent than\nfalse positives, indicating that some real success trajectories\ncannot be reproduced in the simulation, potentially due to a\nslight mismatch in friction coefficient and initial states. Overall, the high diagonal values highlight that the simulator can reproduce real rollout outcomes most of the time,\neven with pure open-loop trajectory replay. C.", "Additional Qualitative Results\nWe include further visualizations in Figure 11, which compares synchronized simulation and real-world trajectories\nacross representative timesteps. For each task, we display\nboth front and wrist camera views. From the figure, we observe that the simulated trajectories closely reproduce the real-world sequences in both\nfront-view and wrist-view observations.", "Object poses, contact transitions, and end-effector motions remain consistent\nacross corresponding timesteps, indicating that the simulator\neffectively captures the underlying task dynamics as well as\nvisual appearance.", "T-block pushing (sim)", "T-block pushing (real)", "Rope routing (sim)", "Rope routing (real)", "Toy packing (sim)", "Toy packing (real)", "t", "Fig. 11: Sim and real rollout trajectories. Columns correspond to synchronized timesteps along each rollout, with identical timestamps\nselected for simulation and real-world policy rollouts to illustrate correspondence. Each panel (e.g., toy packing (real)) shows front-view\n(top) and wrist-view (bottom) observations, with panels alternating between real and simulated trajectories."]}
